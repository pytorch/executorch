[INFO 2026-02-11 05:41:44,817 utils.py:246] 
+-------------------------------------------------------+---------------+-----------------+
| Final Operators                                       |   Final Graph |   To_edge Graph |
+=======================================================+===============+=================+
| cadence::quantize_per_tensor                          |             1 |               0 |
| cadence::quantized_conv2d_nchw.per_tensor             |             1 |               1 |
| cadence::dequantize_per_tensor                        |             1 |               0 |
+-------------------------------------------------------+---------------+-----------------+
[INFO 2026-02-11 05:41:44,818 utils.py:262] +-------------------------------------------------------+---------------+-----------------+
| Deleted Operators                                     |   Final Graph |   To_edge Graph |
+=======================================================+===============+=================+
| quantized_decomposed::quantize_per_tensor             |             0 |               1 |
| quantized_decomposed::dequantize_per_tensor           |             0 |               1 |
+-------------------------------------------------------+---------------+-----------------+
[INFO 2026-02-11 05:41:45,196 memory_planning.py:322] 
+----------------+----------------+-----------------------+-----------------------------+
|   Memory Space | Base Address   |   Memory Size (Bytes) |   Peak Memory Usage (Bytes) |
+================+================+=======================+=============================+
|              1 |                |           68719476736 |                     4014080 |
+----------------+----------------+-----------------------+-----------------------------+
[INFO 2026-02-11 05:41:45,197 memory_planning.py:355] 
+-------------------------------------+---------------+--------+
| Peak memory usage across all spaces | 4014080 bytes | Node 7 |
+-------------------------------------+---------------+--------+
[INFO 2026-02-11 05:41:45,226 compiler.py:478] Generated ETRecord at /tmp/tmp4qvkpc_d/etrecord.bin
[INFO 2026-02-11 05:41:45,229 export_example.py:76] Final exported graph:

[INFO 2026-02-11 05:42:01,766 utils.py:287] Saved exported program to /tmp/tmp4qvkpc_d/CadenceDemoModel.pte
[INFO 2026-02-11 05:42:01,769 utils.py:304] Saved exported program to /tmp/tmp4qvkpc_d/CadenceDemoModel.bpte
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/sraut/ext_main/cad_rlc/executorch/examples/cadence/operators/test_quantized_conv2d_op.py", line 62, in <module>
    export_and_run_model(model, example_inputs)
  File "/home/sraut/ext_main/cad_rlc/executorch/.venv/lib/python3.11/site-packages/executorch/backends/cadence/aot/export_example.py", line 110, in export_and_run_model
    runtime.run_and_compare(
  File "/home/sraut/ext_main/cad_rlc/executorch/.venv/lib/python3.11/site-packages/executorch/backends/cadence/runtime/runtime.py", line 140, in run_and_compare
    outputs = run(executorch_prog, inputs, ref_outputs, working_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sraut/ext_main/cad_rlc/executorch/.venv/lib/python3.11/site-packages/executorch/backends/cadence/runtime/runtime.py", line 61, in run
    program = executorch_prog.executorch_program
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'executorch_program'
opcode         name                                      target                                        args                                                                                                                                                                        kwargs
-------------  ----------------------------------------  --------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------
placeholder    b__frozen_param0                          b__frozen_param0                              ()                                                                                                                                                                          {}
placeholder    b__frozen_param1                          b__frozen_param1                              ()                                                                                                                                                                          {}
placeholder    x                                         x                                             ()                                                                                                                                                                          {}
call_function  alloc                                     <function alloc at 0x7f1084803920>            (((1, 3, 224, 224), torch.int8),)                                                                                                                                           {}
call_function  cadence_quantize_per_tensor_default       cadence.quantize_per_tensor.out               (x, 0.030731072649359703, 2, -128, 127, torch.int8)                                                                                                                         {'out': alloc}
call_function  alloc_1                                   <function alloc at 0x7f1084803920>            (((1, 64, 112, 112), torch.int8),)                                                                                                                                          {}
call_function  cadence_quantized_conv2d_nchw_per_tensor  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantize_per_tensor_default, b__frozen_param0, b__frozen_param1, [2, 2], [3, 3], [1, 1], 1, 2, 0, 1.987867986592948e-05, 0.017547184601426125, 0, 1245603840, -9)  {'out': alloc_1}
call_function  alloc_2                                   <function alloc at 0x7f1084803920>            (((1, 64, 112, 112), torch.float32),)                                                                                                                                       {}
call_function  cadence_dequantize_per_tensor_default     cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor, 0.017547184601426125, 0, -128, 127, torch.int8)                                                                                  {'out': alloc_2}
output         output_1                                  output                                        ((cadence_dequantize_per_tensor_default,),)                                                                                                                                 {}
