
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Qualcomm AI Engine Backend &#8212; ExecuTorch 1.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=56dcb7b8"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'backends-qualcomm';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.pytorch.org/executorch/executorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.1';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://docs.pytorch.org/executorch/backends-qualcomm.html" />
    <link rel="icon" href="_static/executorch-chip-logo.svg"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="MediaTek Backend" href="backends-mediatek.html" />
    <link rel="prev" title="Exporting Llama 3.2 1B/3B Instruct to ExecuTorch Vulkan and running on device" href="backends/vulkan/tutorials/etvk-llama-tutorial.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', '1.1');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/executorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/et-logo.png" class="logo__image only-light" alt="ExecuTorch 1.1 documentation - Home"/>
    <script>document.write(`<img src="_static/et-logo.png" class="logo__image only-dark" alt="ExecuTorch 1.1 documentation - Home"/>`);</script>
  
  
</a></div>
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="intro-section.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="quick-start-section.html">
    Quick Start
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="edge-platforms-section.html">
    Edge
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="backends-section.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="llm/working-with-llms.html">
    LLMs
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="advanced-topics-section.html">
    Advanced
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tools-section.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api-section.html">
    API
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="support-section.html">
    Support
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="intro-section.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="quick-start-section.html">
    Quick Start
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="edge-platforms-section.html">
    Edge
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="backends-section.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="llm/working-with-llms.html">
    LLMs
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="advanced-topics-section.html">
    Advanced
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tools-section.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api-section.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="support-section.html">
    Support
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Backend Overview</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="backends/xnnpack/xnnpack-overview.html">XNNPACK Backend</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="backends/xnnpack/xnnpack-partitioner.html">Partitioner API</a></li>

<li class="toctree-l2"><a class="reference internal" href="backends/xnnpack/xnnpack-quantization.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/xnnpack/xnnpack-troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/xnnpack/xnnpack-arch-internals.html">Architecture and Internals</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="backends/cuda/cuda-overview.html">CUDA Backend</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="backends/coreml/coreml-overview.html">Core ML Backend</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="backends/coreml/coreml-troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/coreml/coreml-partitioner.html">Partitioner API</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/coreml/coreml-quantization.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/coreml/coreml-op-support.html">Op support</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="backends/mps/mps-overview.html">MPS Backend</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="backends/vulkan/vulkan-overview.html">Vulkan Backend</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="backends/vulkan/vulkan-partitioner.html">Partitioner API</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/vulkan/vulkan-quantization.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/vulkan/vulkan-op-support.html">Operator Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/vulkan/vulkan-troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="backends/vulkan/tutorials/vulkan-tutorials.html">Vulkan Backend Tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="backends/vulkan/tutorials/etvk-profiling-tutorial.html">Executing and profiling an ExecuTorch Vulkan model on device</a></li>
<li class="toctree-l3"><a class="reference internal" href="backends/vulkan/tutorials/etvk-llama-tutorial.html">Exporting Llama 3.2 1B/3B Instruct to ExecuTorch Vulkan and running on device</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Qualcomm AI Engine Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends-mediatek.html">MediaTek Backend</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="backends/arm-ethos-u/arm-ethos-u-overview.html">Arm Ethos-U Backend</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="backends/arm-ethos-u/arm-ethos-u-partitioner.html">Partitioner API</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/arm-ethos-u/arm-ethos-u-quantization.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/arm-ethos-u/arm-ethos-u-troubleshooting.html">Arm Ethos-U Troubleshooting</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="backends/arm-ethos-u/tutorials/arm-ethos-u-tutorials.html">Arm Ethos-U Backend Tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="backends/arm-ethos-u/tutorials/ethos-u-getting-started.html">Getting Started Tutorial</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="backends/arm-vgf/arm-vgf-overview.html">Arm VGF Backend</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="backends/arm-vgf/arm-vgf-partitioner.html">Partitioner API</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/arm-vgf/arm-vgf-quantization.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/arm-vgf/arm-vgf-troubleshooting.html">Arm VGF Troubleshooting</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="backends/arm-vgf/tutorials/arm-vgf-tutorials.html">Arm VGF Backend Tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="backends/arm-vgf/tutorials/vgf-getting-started.html">Getting Started Tutorial</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="build-run-openvino.html">Building and Running ExecuTorch with OpenVINO Backend</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="backends/nxp/nxp-overview.html">NXP eIQ Neutron Backend</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="backends/nxp/nxp-partitioner.html">Partitioner API</a></li>

<li class="toctree-l2"><a class="reference internal" href="backends/nxp/nxp-quantization.html">NXP eIQ Neutron Quantization</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="backends/nxp/tutorials/nxp-tutorials.html">NXP Tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="backends/nxp/tutorials/nxp-basic-tutorial.html">Preparing a Model for NXP eIQ Neutron Backend</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="backends/nxp/nxp-dim-order.html">NXP eIQ Dim Order Support</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="backends-cadence.html">Cadence Xtensa Backend</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="backends/samsung/samsung-overview.html">Samsung Exynos Backend</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="backends/samsung/samsung-partitioner.html">Partitioner API</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/samsung/samsung-quantization.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends/samsung/samsung-op-support.html">Operator Support</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="backends-section.html" class="nav-link">Backends</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Qualcomm AI...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="backends-section.html">
        <meta itemprop="name" content="Backends">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Qualcomm AI Engine Backend">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="qualcomm-ai-engine-backend">
<h1>Qualcomm AI Engine Backend<a class="headerlink" href="#qualcomm-ai-engine-backend" title="Link to this heading">#</a></h1>
<p>In this tutorial we will walk you through the process of getting started to
build ExecuTorch for Qualcomm AI Engine Direct and running a model on it.</p>
<p>Qualcomm AI Engine Direct is also referred to as QNN in the source and documentation.</p>
<!----This will show a grid card on the page----->
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
What you will learn in this tutorial:</div>
<ul class="simple">
<li><p class="sd-card-text">In this tutorial you will learn how to lower and deploy a model for Qualcomm AI Engine Direct.</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
Tutorials we recommend you complete before this:</div>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="intro-how-it-works.html"><span class="std std-doc">Introduction to ExecuTorch</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="getting-started.html"><span class="std std-doc">Getting Started</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="using-executorch-building-from-source.html"><span class="std std-doc">Building ExecuTorch with CMake</span></a></p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="what-s-qualcomm-ai-engine-direct">
<h2>What’s Qualcomm AI Engine Direct?<a class="headerlink" href="#what-s-qualcomm-ai-engine-direct" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://developer.qualcomm.com/software/qualcomm-ai-engine-direct-sdk">Qualcomm AI Engine Direct</a>
is designed to provide unified, low-level APIs for AI development.</p>
<p>Developers can interact with various accelerators on Qualcomm SoCs with these set of APIs, including
Kryo CPU, Adreno GPU, and Hexagon processors. More details can be found <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-10/overview.html">here</a>.</p>
<p>Currently, this ExecuTorch Backend can delegate AI computations to Hexagon processors through Qualcomm AI Engine Direct APIs.</p>
</section>
<section id="prerequisites-hardware-and-software">
<h2>Prerequisites (Hardware and Software)<a class="headerlink" href="#prerequisites-hardware-and-software" title="Link to this heading">#</a></h2>
<section id="host-os">
<h3>Host OS<a class="headerlink" href="#host-os" title="Link to this heading">#</a></h3>
<p>The QNN Backend is currently verified on the following Linux host operating systems:</p>
<ul class="simple">
<li><p><strong>Ubuntu 22.04 LTS (x64)</strong></p></li>
<li><p><strong>CentOS Stream 9</strong></p></li>
<li><p><strong>Windows Subsystem for Linux (WSL)</strong> with Ubuntu 22.04</p></li>
</ul>
<p>In general, we verify the backend on the same OS versions that the QNN SDK is officially validated against.<br />
The exact supported versions are documented in the QNN SDK.</p>
<section id="windows-wsl-setup">
<h4>Windows (WSL) Setup<a class="headerlink" href="#windows-wsl-setup" title="Link to this heading">#</a></h4>
<p>To install Ubuntu 22.04 on WSL, run the following command in PowerShell or Windows Terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wsl<span class="w"> </span>--install<span class="w"> </span>-d<span class="w"> </span>ubuntu<span class="w"> </span><span class="m">22</span>.04
</pre></div>
</div>
<p>This command will install WSL and set up Ubuntu 22.04 as the default Linux distribution.</p>
<p>For more details and troubleshooting, refer to the official Microsoft WSL installation guide:
👉 <a class="reference external" href="https://learn.microsoft.com/en-us/windows/wsl/install">Install WSL | Microsoft Learn</a></p>
</section>
</section>
<section id="hardware">
<h3>Hardware:<a class="headerlink" href="#hardware" title="Link to this heading">#</a></h3>
<p>You will need an Android / Linux device with adb-connected running on one of below Qualcomm SoCs:</p>
<ul class="simple">
<li><p>SA8295</p></li>
<li><p>SM8450 (Snapdragon 8 Gen 1)</p></li>
<li><p>SM8475 (Snapdragon 8 Gen 1+)</p></li>
<li><p>SM8550 (Snapdragon 8 Gen 2)</p></li>
<li><p>SM8650 (Snapdragon 8 Gen 3)</p></li>
<li><p>SM8750 (Snapdragon 8 Elite)</p></li>
<li><p>SSG2115P</p></li>
<li><p>SSG2125P</p></li>
<li><p>SXR1230P (Linux Embedded)</p></li>
<li><p>SXR2230P</p></li>
<li><p>SXR2330P</p></li>
<li><p>QCM6490</p></li>
<li><p>QCS9100</p></li>
</ul>
<p>This example is verified with SM8550 and SM8450.</p>
</section>
<section id="software">
<h3>Software:<a class="headerlink" href="#software" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Follow ExecuTorch recommended Python version.</p></li>
<li><p>A compiler to compile AOT parts, e.g., the GCC compiler comes with Ubuntu LTS. g++ version need to be 13 or higher.</p></li>
<li><p><a class="reference external" href="https://developer.android.com/ndk">Android NDK</a>. This example is verified with NDK 26c.</p></li>
<li><p>(Optional) Target toolchain for linux embedded platform.</p></li>
<li><p><a class="reference external" href="https://developer.qualcomm.com/software/qualcomm-ai-engine-direct-sdk">Qualcomm AI Engine Direct SDK</a></p>
<ul>
<li><p>Click the “Get Software” button to download the latest version of the QNN SDK.</p></li>
<li><p>Although newer versions are available, we have verified and recommend using QNN 2.37.0 for stability.</p></li>
<li><p>You can download it directly from the following link: <a class="reference external" href="https://softwarecenter.qualcomm.com/api/download/software/sdks/Qualcomm_AI_Runtime_Community/All/2.37.0.250724/v2.37.0.250724.zip">QNN 2.37.0</a></p></li>
</ul>
</li>
</ul>
<p>The directory with installed Qualcomm AI Engine Direct SDK looks like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>├── benchmarks
├── bin
├── docs
├── examples
├── include
├── lib
├── LICENSE.pdf
├── NOTICE.txt
├── NOTICE_WINDOWS.txt
├── QNN_NOTICE.txt
├── QNN_README.txt
├── QNN_ReleaseNotes.txt
├── ReleaseNotes.txt
├── ReleaseNotesWindows.txt
├── sdk.yaml
└── share
</pre></div>
</div>
</section>
</section>
<section id="setting-up-your-developer-environment">
<h2>Setting up your developer environment<a class="headerlink" href="#setting-up-your-developer-environment" title="Link to this heading">#</a></h2>
<section id="conventions">
<h3>Conventions<a class="headerlink" href="#conventions" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">$QNN_SDK_ROOT</span></code> refers to the root of Qualcomm AI Engine Direct SDK,
i.e., the directory containing <code class="docutils literal notranslate"><span class="pre">QNN_README.txt</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">$ANDROID_NDK_ROOT</span></code> refers to the root of Android NDK.</p>
<p><code class="docutils literal notranslate"><span class="pre">$EXECUTORCH_ROOT</span></code> refers to the root of executorch git repository.</p>
</section>
<section id="setup-environment-variables">
<h3>Setup environment variables<a class="headerlink" href="#setup-environment-variables" title="Link to this heading">#</a></h3>
<p>We set <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> to make sure the dynamic linker can find QNN libraries.</p>
<p>Further, we set <code class="docutils literal notranslate"><span class="pre">PYTHONPATH</span></code> because it’s easier to develop and import ExecuTorch
Python APIs.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$QNN_SDK_ROOT</span>/lib/x86_64-linux-clang/:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$EXECUTORCH_ROOT</span>/..
</pre></div>
</div>
</section>
</section>
<section id="build">
<h2>Build<a class="headerlink" href="#build" title="Link to this heading">#</a></h2>
<p>An example script for the below building instructions is <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/backends/qualcomm/scripts/build.sh">here</a>.
We recommend to use the script because the ExecuTorch build-command can change from time to time.
The above script is actively used. It is updated more frequently than this tutorial.
An example usage is</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="nv">$EXECUTORCH_ROOT</span>
<span class="c1"># android target</span>
./backends/qualcomm/scripts/build.sh
<span class="c1"># (optional) linux embedded target</span>
./backends/qualcomm/scripts/build.sh<span class="w"> </span>--enable_linux_embedded
<span class="c1"># for release build</span>
./backends/qualcomm/scripts/build.sh<span class="w"> </span>--release
</pre></div>
</div>
</section>
<section id="deploying-and-running-on-device">
<h2>Deploying and running on device<a class="headerlink" href="#deploying-and-running-on-device" title="Link to this heading">#</a></h2>
<section id="aot-compile-a-model">
<h3>AOT compile a model<a class="headerlink" href="#aot-compile-a-model" title="Link to this heading">#</a></h3>
<p>Refer to <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/examples/qualcomm/scripts/deeplab_v3.py">this script</a> for the exact flow.
We use deeplab-v3-resnet101 as an example in this tutorial. Run below commands to compile:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="nv">$EXECUTORCH_ROOT</span>

python<span class="w"> </span>-m<span class="w"> </span>examples.qualcomm.scripts.deeplab_v3<span class="w"> </span>-b<span class="w"> </span>build-android<span class="w"> </span>-m<span class="w"> </span>SM8550<span class="w"> </span>--compile_only<span class="w"> </span>--download
</pre></div>
</div>
<p>You might see something like below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">INFO</span><span class="p">][</span><span class="n">Qnn</span> <span class="n">ExecuTorch</span><span class="p">]</span> <span class="n">Destroy</span> <span class="n">Qnn</span> <span class="n">context</span>
<span class="p">[</span><span class="n">INFO</span><span class="p">][</span><span class="n">Qnn</span> <span class="n">ExecuTorch</span><span class="p">]</span> <span class="n">Destroy</span> <span class="n">Qnn</span> <span class="n">device</span>
<span class="p">[</span><span class="n">INFO</span><span class="p">][</span><span class="n">Qnn</span> <span class="n">ExecuTorch</span><span class="p">]</span> <span class="n">Destroy</span> <span class="n">Qnn</span> <span class="n">backend</span>

<span class="n">opcode</span>         <span class="n">name</span>                      <span class="n">target</span>                       <span class="n">args</span>                           <span class="n">kwargs</span>
<span class="o">-------------</span>  <span class="o">------------------------</span>  <span class="o">---------------------------</span>  <span class="o">-----------------------------</span>  <span class="o">--------</span>
<span class="n">placeholder</span>    <span class="n">arg684_1</span>                  <span class="n">arg684_1</span>                     <span class="p">()</span>                             <span class="p">{}</span>
<span class="n">get_attr</span>       <span class="n">lowered_module_0</span>          <span class="n">lowered_module_0</span>             <span class="p">()</span>                             <span class="p">{}</span>
<span class="n">call_function</span>  <span class="n">executorch_call_delegate</span>  <span class="n">executorch_call_delegate</span>     <span class="p">(</span><span class="n">lowered_module_0</span><span class="p">,</span> <span class="n">arg684_1</span><span class="p">)</span>   <span class="p">{}</span>
<span class="n">call_function</span>  <span class="n">getitem</span>                   <span class="o">&lt;</span><span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">function</span> <span class="n">getitem</span><span class="o">&gt;</span>  <span class="p">(</span><span class="n">executorch_call_delegate</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="p">{}</span>
<span class="n">call_function</span>  <span class="n">getitem_1</span>                 <span class="o">&lt;</span><span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">function</span> <span class="n">getitem</span><span class="o">&gt;</span>  <span class="p">(</span><span class="n">executorch_call_delegate</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="p">{}</span>
<span class="n">output</span>         <span class="n">output</span>                    <span class="n">output</span>                       <span class="p">([</span><span class="n">getitem_1</span><span class="p">,</span> <span class="n">getitem</span><span class="p">],)</span>        <span class="p">{}</span>
</pre></div>
</div>
<p>The compiled model is <code class="docutils literal notranslate"><span class="pre">./deeplab_v3/dlv3_qnn.pte</span></code>.</p>
</section>
<section id="test-model-inference-on-qnn-htp-emulator">
<h3>Test model inference on QNN HTP emulator<a class="headerlink" href="#test-model-inference-on-qnn-htp-emulator" title="Link to this heading">#</a></h3>
<p>We can test model inferences before deploying it to a device by HTP emulator.</p>
<p>Let’s build <code class="docutils literal notranslate"><span class="pre">qnn_executor_runner</span></code> for a x64 host:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># assuming the AOT component is built.</span>
<span class="nb">cd</span><span class="w"> </span><span class="nv">$EXECUTORCH_ROOT</span>/build-x86
cmake<span class="w"> </span>../examples/qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-DCMAKE_PREFIX_PATH<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$PWD</span><span class="s2">/lib/cmake/ExecuTorch;</span><span class="nv">$PWD</span><span class="s2">/third-party/gflags;&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-DCMAKE_FIND_ROOT_PATH_MODE_PACKAGE<span class="o">=</span>BOTH<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-DPYTHON_EXECUTABLE<span class="o">=</span>python3<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-Bexamples/qualcomm

cmake<span class="w"> </span>--build<span class="w"> </span>examples/qualcomm<span class="w"> </span>-j<span class="k">$(</span>nproc<span class="k">)</span>

<span class="c1"># qnn_executor_runner can be found under examples/qualcomm/executor_runner</span>
<span class="c1"># The full path is $EXECUTORCH_ROOT/build-x86/examples/qualcomm/executor_runner/qnn_executor_runner</span>
ls<span class="w"> </span>examples/qualcomm/executor_runner
</pre></div>
</div>
<p>To run the HTP emulator, the dynamic linker needs to access QNN libraries and <code class="docutils literal notranslate"><span class="pre">libqnn_executorch_backend.so</span></code>.
We set the below two paths to <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> environment variable:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">$QNN_SDK_ROOT/lib/x86_64-linux-clang/</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">$EXECUTORCH_ROOT/build-x86/lib/</span></code></p></li>
</ol>
<p>The first path is for QNN libraries including HTP emulator. It has been configured in the AOT compilation section.</p>
<p>The second path is for <code class="docutils literal notranslate"><span class="pre">libqnn_executorch_backend.so</span></code>.</p>
<p>So, we can run <code class="docutils literal notranslate"><span class="pre">./deeplab_v3/dlv3_qnn.pte</span></code> by:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="nv">$EXECUTORCH_ROOT</span>/build-x86
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$EXECUTORCH_ROOT</span>/build-x86/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
examples/qualcomm/executor_runner/qnn_executor_runner<span class="w"> </span>--model_path<span class="w"> </span>../deeplab_v3/dlv3_qnn.pte
</pre></div>
</div>
<p>We should see some outputs like the below. Note that the emulator can take some time to finish.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>I<span class="w"> </span><span class="m">00</span>:00:00.354662<span class="w"> </span>executorch:qnn_executor_runner.cpp:213<span class="o">]</span><span class="w"> </span>Method<span class="w"> </span>loaded.
I<span class="w"> </span><span class="m">00</span>:00:00.356460<span class="w"> </span>executorch:qnn_executor_runner.cpp:261<span class="o">]</span><span class="w"> </span>ignoring<span class="w"> </span>error<span class="w"> </span>from<span class="w"> </span>set_output_data_ptr<span class="o">()</span>:<span class="w"> </span>0x2
I<span class="w"> </span><span class="m">00</span>:00:00.357991<span class="w"> </span>executorch:qnn_executor_runner.cpp:261<span class="o">]</span><span class="w"> </span>ignoring<span class="w"> </span>error<span class="w"> </span>from<span class="w"> </span>set_output_data_ptr<span class="o">()</span>:<span class="w"> </span>0x2
I<span class="w"> </span><span class="m">00</span>:00:00.357996<span class="w"> </span>executorch:qnn_executor_runner.cpp:265<span class="o">]</span><span class="w"> </span>Inputs<span class="w"> </span>prepared.

I<span class="w"> </span><span class="m">00</span>:01:09.328144<span class="w"> </span>executorch:qnn_executor_runner.cpp:414<span class="o">]</span><span class="w"> </span>Model<span class="w"> </span>executed<span class="w"> </span>successfully.
I<span class="w"> </span><span class="m">00</span>:01:09.328159<span class="w"> </span>executorch:qnn_executor_runner.cpp:421<span class="o">]</span><span class="w"> </span>Write<span class="w"> </span>etdump<span class="w"> </span>to<span class="w"> </span>etdump.etdp,<span class="w"> </span><span class="nv">Size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">424</span>
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span><span class="o">[</span>Qnn<span class="w"> </span>ExecuTorch<span class="o">]</span>:<span class="w"> </span>Destroy<span class="w"> </span>Qnn<span class="w"> </span>backend<span class="w"> </span>parameters
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span><span class="o">[</span>Qnn<span class="w"> </span>ExecuTorch<span class="o">]</span>:<span class="w"> </span>Destroy<span class="w"> </span>Qnn<span class="w"> </span>context
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span><span class="o">[</span>Qnn<span class="w"> </span>ExecuTorch<span class="o">]</span>:<span class="w"> </span>Destroy<span class="w"> </span>Qnn<span class="w"> </span>device
<span class="o">[</span>INFO<span class="o">]</span><span class="w"> </span><span class="o">[</span>Qnn<span class="w"> </span>ExecuTorch<span class="o">]</span>:<span class="w"> </span>Destroy<span class="w"> </span>Qnn<span class="w"> </span>backend
</pre></div>
</div>
</section>
<section id="run-model-inference-on-an-android-smartphone-with-qualcomm-socs">
<h3>Run model inference on an Android smartphone with Qualcomm SoCs<a class="headerlink" href="#run-model-inference-on-an-android-smartphone-with-qualcomm-socs" title="Link to this heading">#</a></h3>
<p><em><strong>Step 1</strong></em>. We need to push required QNN libraries to the device.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># make sure you have write-permission on below path.</span>
<span class="nv">DEVICE_DIR</span><span class="o">=</span>/data/local/tmp/executorch_qualcomm_tutorial/
adb<span class="w"> </span>shell<span class="w"> </span><span class="s2">&quot;mkdir -p </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span><span class="s2">&quot;</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/aarch64-android/libQnnHtp.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/aarch64-android/libQnnSystem.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/aarch64-android/libQnnHtpV69Stub.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/aarch64-android/libQnnHtpV73Stub.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/aarch64-android/libQnnHtpV75Stub.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/aarch64-android/libQnnHtpV79Stub.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/hexagon-v69/unsigned/libQnnHtpV69Skel.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/hexagon-v73/unsigned/libQnnHtpV73Skel.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/hexagon-v75/unsigned/libQnnHtpV75Skel.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/hexagon-v79/unsigned/libQnnHtpV79Skel.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
</pre></div>
</div>
<p><em><strong>Step 2</strong></em>.  We also need to indicate dynamic linkers on Android and Hexagon
where to find these libraries by setting <code class="docutils literal notranslate"><span class="pre">ADSP_LIBRARY_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>.
So, we can run <code class="docutils literal notranslate"><span class="pre">qnn_executor_runner</span></code> like</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>push<span class="w"> </span>./deeplab_v3/dlv3_qnn.pte<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">EXECUTORCH_ROOT</span><span class="si">}</span>/build-android/examples/qualcomm/executor_runner/qnn_executor_runner<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">EXECUTORCH_ROOT</span><span class="si">}</span>/build-android/lib/libqnn_executorch_backend.so<span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span>
adb<span class="w"> </span>shell<span class="w"> </span><span class="s2">&quot;cd </span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span><span class="s2"> \</span>
<span class="s2">           &amp;&amp; export LD_LIBRARY_PATH=</span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span><span class="s2"> \</span>
<span class="s2">           &amp;&amp; export ADSP_LIBRARY_PATH=</span><span class="si">${</span><span class="nv">DEVICE_DIR</span><span class="si">}</span><span class="s2"> \</span>
<span class="s2">           &amp;&amp; ./qnn_executor_runner --model_path ./dlv3_qnn.pte&quot;</span>
</pre></div>
</div>
<p>You should see something like below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">I</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mf">00.257354</span> <span class="n">executorch</span><span class="p">:</span><span class="n">qnn_executor_runner</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">213</span><span class="p">]</span> <span class="n">Method</span> <span class="n">loaded</span><span class="o">.</span>
<span class="n">I</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mf">00.323502</span> <span class="n">executorch</span><span class="p">:</span><span class="n">qnn_executor_runner</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">262</span><span class="p">]</span> <span class="n">ignoring</span> <span class="n">error</span> <span class="kn">from</span><span class="w"> </span><span class="nn">set_output_data_ptr</span><span class="p">():</span> <span class="mh">0x2</span>
<span class="n">I</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mf">00.357496</span> <span class="n">executorch</span><span class="p">:</span><span class="n">qnn_executor_runner</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">262</span><span class="p">]</span> <span class="n">ignoring</span> <span class="n">error</span> <span class="kn">from</span><span class="w"> </span><span class="nn">set_output_data_ptr</span><span class="p">():</span> <span class="mh">0x2</span>
<span class="n">I</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mf">00.357555</span> <span class="n">executorch</span><span class="p">:</span><span class="n">qnn_executor_runner</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">265</span><span class="p">]</span> <span class="n">Inputs</span> <span class="n">prepared</span><span class="o">.</span>
<span class="n">I</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mf">00.364824</span> <span class="n">executorch</span><span class="p">:</span><span class="n">qnn_executor_runner</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">414</span><span class="p">]</span> <span class="n">Model</span> <span class="n">executed</span> <span class="n">successfully</span><span class="o">.</span>
<span class="n">I</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mf">00.364875</span> <span class="n">executorch</span><span class="p">:</span><span class="n">qnn_executor_runner</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">425</span><span class="p">]</span> <span class="n">Write</span> <span class="n">etdump</span> <span class="n">to</span> <span class="n">etdump</span><span class="o">.</span><span class="n">etdp</span><span class="p">,</span> <span class="n">Size</span> <span class="o">=</span> <span class="mi">424</span>
<span class="p">[</span><span class="n">INFO</span><span class="p">]</span> <span class="p">[</span><span class="n">Qnn</span> <span class="n">ExecuTorch</span><span class="p">]:</span> <span class="n">Destroy</span> <span class="n">Qnn</span> <span class="n">backend</span> <span class="n">parameters</span>
<span class="p">[</span><span class="n">INFO</span><span class="p">]</span> <span class="p">[</span><span class="n">Qnn</span> <span class="n">ExecuTorch</span><span class="p">]:</span> <span class="n">Destroy</span> <span class="n">Qnn</span> <span class="n">context</span>
<span class="p">[</span><span class="n">INFO</span><span class="p">]</span> <span class="p">[</span><span class="n">Qnn</span> <span class="n">ExecuTorch</span><span class="p">]:</span> <span class="n">Destroy</span> <span class="n">Qnn</span> <span class="n">backend</span>
</pre></div>
</div>
<p>The model is merely executed. If we want to feed real inputs and get model outputs, we can use</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="nv">$EXECUTORCH_ROOT</span>
<span class="c1"># android</span>
python<span class="w"> </span>-m<span class="w"> </span>examples.qualcomm.scripts.deeplab_v3<span class="w"> </span>-b<span class="w"> </span>build-android<span class="w"> </span>-m<span class="w"> </span>SM8550<span class="w"> </span>--download<span class="w"> </span>-s<span class="w"> </span>&lt;device_serial&gt;
<span class="c1"># (optional) linux embedded</span>
python<span class="w"> </span>-m<span class="w"> </span>examples.qualcomm.scripts.deeplab_v3<span class="w"> </span>-b<span class="w"> </span>build-oe-linux<span class="w"> </span>-m<span class="w"> </span>SXR1230P<span class="w"> </span>--download<span class="w"> </span>-s<span class="w"> </span>&lt;device_serial&gt;<span class="w"> </span>-t<span class="w"> </span>aarch64-oe-linux-gcc-9.3
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">&lt;device_serial&gt;</span></code> can be found by <code class="docutils literal notranslate"><span class="pre">adb</span> <span class="pre">devices</span></code> command.</p>
<p>After the above command, pre-processed inputs and outputs are put in <code class="docutils literal notranslate"><span class="pre">$EXECUTORCH_ROOT/deeplab_v3</span></code> and <code class="docutils literal notranslate"><span class="pre">$EXECUTORCH_ROOT/deeplab_v3/outputs</span></code> folder.</p>
<p>The command-line arguments are written in <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/examples/qualcomm/utils.py#L139">utils.py</a>.
The model, inputs, and output location are passed to <code class="docutils literal notranslate"><span class="pre">qnn_executorch_runner</span></code> by <code class="docutils literal notranslate"><span class="pre">--model_path</span></code>, <code class="docutils literal notranslate"><span class="pre">--input_list_path</span></code>, and <code class="docutils literal notranslate"><span class="pre">--output_folder_path</span></code>.</p>
</section>
<section id="run-android-llamademo-with-qnn-backend">
<h3>Run <a class="reference external" href="https://github.com/meta-pytorch/executorch-examples/tree/main/llm/android/LlamaDemo">Android LlamaDemo</a> with QNN backend<a class="headerlink" href="#run-android-llamademo-with-qnn-backend" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">$DEMO_APP</span></code> refers to the root of the executorch android demo, i.e., the directory containing <code class="docutils literal notranslate"><span class="pre">build.gradle.kts</span></code>.</p>
<p><em><strong>Step 1</strong></em>: Rebuild ExecuTorch AAR</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build the AAR</span>
<span class="nb">cd</span><span class="w"> </span><span class="nv">$EXECUTORCH_ROOT</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">BUILD_AAR_DIR</span><span class="o">=</span><span class="nv">$EXECUTORCH_ROOT</span>/aar-out
./scripts/build_android_library.sh
</pre></div>
</div>
<p><em><strong>Step 2</strong></em>: Copy AAR to Android Project</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cp<span class="w"> </span><span class="nv">$EXECUTORCH_ROOT</span>/aar-out/executorch.aar<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="nv">$DEMO_APP</span>/app/libs/executorch.aar
</pre></div>
</div>
<p><em><strong>Step 3</strong></em>: Build Android APK</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="nv">$DEMO_APP</span>
./gradlew<span class="w"> </span>clean<span class="w"> </span>assembleDebug<span class="w"> </span>-PuseLocalAar<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p><em><strong>Step 4</strong></em>: Install on Device</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>app/build/outputs/apk/debug/app-debug.apk
</pre></div>
</div>
<p><em><strong>Step 5</strong></em>: Push model</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>shell<span class="w"> </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/data/local/tmp/llama
adb<span class="w"> </span>push<span class="w"> </span>model.pte<span class="w"> </span>/data/local/tmp/llama
adb<span class="w"> </span>push<span class="w"> </span>tokenizer.bin<span class="w"> </span>/data/local/tmp/llama
</pre></div>
</div>
<p><em><strong>Step 6</strong></em>: Run the Llama Demo</p>
<ul class="simple">
<li><p>Open the App on Android</p></li>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">QUALCOMM</span></code> backend</p></li>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">model.pte</span></code> Model</p></li>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">tokenizer.bin</span></code> Tokenizer</p></li>
<li><p>Select Model Type</p></li>
<li><p>Click LOAD MODEL</p></li>
<li><p>It should show <code class="docutils literal notranslate"><span class="pre">Successfully</span> <span class="pre">loaded</span> <span class="pre">model.</span></code></p></li>
</ul>
<section id="verification-steps">
<h4>Verification Steps<a class="headerlink" href="#verification-steps" title="Link to this heading">#</a></h4>
<p><em><strong>Step 1</strong></em>. Verify AAR Contains Your Changes</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check for debug strings in the AAR</span>
unzip<span class="w"> </span>-p<span class="w"> </span><span class="nv">$DEMO_APP</span>/app/libs/executorch.aar<span class="w"> </span>jni/arm64-v8a/libexecutorch.so<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>strings<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;QNN&quot;</span><span class="w">   </span><span class="c1"># Replace &quot;QNN&quot; with your actual debug string if needed</span>
</pre></div>
</div>
<p>If found, your changes are in the AAR.</p>
<p><em><strong>Step 2</strong></em>. Verify APK Contains Correct Libraries</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check QNN library version in APK</span>
<span class="nb">cd</span><span class="w"> </span><span class="nv">$DEMO_APP</span>
unzip<span class="w"> </span>-l<span class="w"> </span>app/build/outputs/apk/debug/app-debug.apk<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;libQnnHtp.so&quot;</span>
</pre></div>
</div>
<p>Expected size for QNN 2.37.0: ~2,465,440 bytes</p>
<p><em><strong>Step 3</strong></em>. Monitor Logs During Model Loading</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>logcat<span class="w"> </span>-c
adb<span class="w"> </span>logcat<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-E<span class="w"> </span><span class="s2">&quot;ExecuTorch&quot;</span>
</pre></div>
</div>
</section>
<section id="common-issues-and-solutions">
<h4>Common Issues and Solutions<a class="headerlink" href="#common-issues-and-solutions" title="Link to this heading">#</a></h4>
<section id="issue-1-error-18-invalidargument">
<h5>Issue 1: Error 18 (InvalidArgument)<a class="headerlink" href="#issue-1-error-18-invalidargument" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Cause</strong>: Wrong parameter order in Runner constructor or missing QNN config</p></li>
<li><p><strong>Solution</strong>: Check <code class="docutils literal notranslate"><span class="pre">$EXECUTORCH_ROOT/examples/qualcomm/oss_scripts/llama/runner/runner.h</span></code> for the correct constructor signature.</p></li>
</ul>
</section>
<section id="issue-2-error-1-internal-with-qnn-api-version-mismatch">
<h5>Issue 2: Error 1 (Internal) with QNN API Version Mismatch<a class="headerlink" href="#issue-2-error-1-internal-with-qnn-api-version-mismatch" title="Link to this heading">#</a></h5>
<ul>
<li><p><strong>Symptoms</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="p">[</span><span class="n">Qnn</span> <span class="n">ExecuTorch</span><span class="p">]:</span> <span class="n">Qnn</span> <span class="n">API</span> <span class="n">version</span> <span class="mf">2.33.0</span> <span class="ow">is</span> <span class="n">mismatched</span>
<span class="n">E</span> <span class="p">[</span><span class="n">Qnn</span> <span class="n">ExecuTorch</span><span class="p">]:</span> <span class="n">Using</span> <span class="n">newer</span> <span class="n">context</span> <span class="n">binary</span> <span class="n">on</span> <span class="n">old</span> <span class="n">SDK</span>
<span class="n">E</span> <span class="p">[</span><span class="n">Qnn</span> <span class="n">ExecuTorch</span><span class="p">]:</span> <span class="n">Can</span><span class="s1">&#39;t create context from binary. Error 5000</span>
</pre></div>
</div>
</li>
<li><p><strong>Cause</strong>: Model compiled with QNN SDK version X but APK uses QNN runtime version Y</p></li>
<li><p><strong>Solution</strong>:</p>
<ul class="simple">
<li><p>Update <code class="docutils literal notranslate"><span class="pre">build.gradle.kts</span></code> with matching QNN runtime version</p></li>
</ul>
<blockquote>
<div><p><strong>Note:</strong> The version numbers below (<code class="docutils literal notranslate"><span class="pre">2.33.0</span></code> and <code class="docutils literal notranslate"><span class="pre">2.37.0</span></code>) are examples only. Please check for the latest compatible QNN runtime version or match your QNN SDK version to avoid API mismatches.</p>
</div></blockquote>
<p><strong>Before</strong>:</p>
<div class="highlight-kotlin notranslate"><div class="highlight"><pre><span></span><span class="n">implementation</span><span class="p">(</span><span class="s">&quot;com.qualcomm.qti:qnn-runtime:2.33.0&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>After</strong>:</p>
<div class="highlight-kotlin notranslate"><div class="highlight"><pre><span></span><span class="n">implementation</span><span class="p">(</span><span class="s">&quot;com.qualcomm.qti:qnn-runtime:2.37.0&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Or recompile model with matching QNN SDK version</p></li>
</ul>
</li>
</ul>
</section>
<section id="issue-3-native-code-changes-not-applied">
<h5>Issue 3: Native Code Changes Not Applied<a class="headerlink" href="#issue-3-native-code-changes-not-applied" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Symptoms</strong>:</p>
<ul>
<li><p>Debug logs don’t appear</p></li>
<li><p>Behavior doesn’t change</p></li>
</ul>
</li>
<li><p><strong>Cause</strong>:</p>
<ul>
<li><p>Gradle using Maven dependency instead of local AAR</p></li>
</ul>
</li>
<li><p><strong>Solution</strong>:</p>
<ul>
<li><p>Always build with <code class="docutils literal notranslate"><span class="pre">-PuseLocalAar=true</span></code> flag</p></li>
</ul>
</li>
</ul>
</section>
<section id="issue-4-logs-not-appearing">
<h5>Issue 4: Logs Not Appearing<a class="headerlink" href="#issue-4-logs-not-appearing" title="Link to this heading">#</a></h5>
<ul>
<li><p><strong>Cause</strong>: Wrong logging tag filter</p></li>
<li><p><strong>Solution</strong>: QNN uses “ExecuTorch” tag:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>logcat<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;ExecuTorch&quot;</span>
</pre></div>
</div>
</li>
</ul>
</section>
</section>
</section>
</section>
<section id="supported-model-list">
<h2>Supported model list<a class="headerlink" href="#supported-model-list" title="Link to this heading">#</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">$EXECUTORCH_ROOT/examples/qualcomm/scripts/</span></code> and <code class="docutils literal notranslate"><span class="pre">$EXECUTORCH_ROOT/examples/qualcomm/oss_scripts/</span></code> to the list of supported models.</p>
<p>Each script demonstrates:</p>
<ul class="simple">
<li><p>Model export (torch.export)</p></li>
<li><p>Quantization (PTQ/QAT)</p></li>
<li><p>Lowering and compilation to QNN delegate</p></li>
</ul>
<p>Deployment on device or HTP emulator</p>
</section>
<section id="how-to-support-a-custom-model-in-htp-backend">
<h2>How to Support a Custom Model in HTP Backend<a class="headerlink" href="#how-to-support-a-custom-model-in-htp-backend" title="Link to this heading">#</a></h2>
<section id="step-by-step-implementation-guide">
<h3>Step-by-Step Implementation Guide<a class="headerlink" href="#step-by-step-implementation-guide" title="Link to this heading">#</a></h3>
<p>Please reference <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/examples/qualcomm/scripts/export_example.py">the simple example</a> and <a class="reference external" href="https://github.com/pytorch/executorch/tree/main/examples/qualcomm/scripts">more complicated examples</a> for reference</p>
<section id="step-1-prepare-your-model">
<h4>Step 1: Prepare Your Model<a class="headerlink" href="#step-1-prepare-your-model" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Initialize your custom model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YourModelClass</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Your custom PyTorch model</span>

<span class="c1"># Create example inputs (adjust shape as needed)</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),)</span>  <span class="c1"># Example input tensor</span>
</pre></div>
</div>
</section>
<section id="step-2-optional-quantize-your-model">
<h4>Step 2: [Optional] Quantize Your Model<a class="headerlink" href="#step-2-optional-quantize-your-model" title="Link to this heading">#</a></h4>
<p>Choose between quantization approaches, post training quantization (PTQ) or quantization aware training (QAT):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">executorch.backends.qualcomm.quantizer.quantizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">QnnQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.pt2e.quantize_pt2e</span><span class="w"> </span><span class="kn">import</span> <span class="n">prepare_pt2e</span><span class="p">,</span> <span class="n">prepare_qat_pt2e</span><span class="p">,</span> <span class="n">convert_pt2e</span>

<span class="n">quantizer</span> <span class="o">=</span> <span class="n">QnnQuantizer</span><span class="p">()</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>

<span class="c1"># PTQ (Post-Training Quantization)</span>
<span class="k">if</span> <span class="n">quantization_type</span> <span class="o">==</span> <span class="s2">&quot;ptq&quot;</span><span class="p">:</span>
    <span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare_pt2e</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">quantizer</span><span class="p">)</span>
    <span class="c1"># Calibration loop would go here</span>
    <span class="n">prepared_model</span><span class="p">(</span><span class="o">*</span><span class="n">example_inputs</span><span class="p">)</span>

<span class="c1"># QAT (Quantization-Aware Training)</span>
<span class="k">elif</span> <span class="n">quantization_type</span> <span class="o">==</span> <span class="s2">&quot;qat&quot;</span><span class="p">:</span>
    <span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare_qat_pt2e</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">quantizer</span><span class="p">)</span>
    <span class="c1"># Training loop would go here</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training_steps</span><span class="p">):</span>
        <span class="n">prepared_model</span><span class="p">(</span><span class="o">*</span><span class="n">example_inputs</span><span class="p">)</span>

<span class="c1"># Convert to quantized model</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">convert_pt2e</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">QNNQuantizer</span></code> is configurable, with the default setting being <strong>8a8w</strong>. For advanced users, refer to the <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/backends/qualcomm/quantizer/quantizer.py"><code class="docutils literal notranslate"><span class="pre">QnnQuantizer</span></code></a> documentation for details.</p>
<section id="supported-quantization-schemes">
<h5>Supported Quantization Schemes<a class="headerlink" href="#supported-quantization-schemes" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>8a8w</strong> (default)</p></li>
<li><p><strong>16a16w</strong></p></li>
<li><p><strong>16a8w</strong></p></li>
<li><p><strong>16a4w</strong></p></li>
<li><p><strong>16a4w_block</strong></p></li>
</ul>
</section>
<section id="customization-options">
<h5>Customization Options<a class="headerlink" href="#customization-options" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Per-node annotation</strong>: Use <code class="docutils literal notranslate"><span class="pre">custom_quant_annotations</span></code>.</p></li>
<li><p><strong>Per-module (<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>) annotation</strong>: Use <code class="docutils literal notranslate"><span class="pre">submodule_qconfig_list</span></code>.</p></li>
</ul>
</section>
<section id="additional-features">
<h5>Additional Features<a class="headerlink" href="#additional-features" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Node exclusion</strong>: Discard specific nodes via <code class="docutils literal notranslate"><span class="pre">discard_nodes</span></code>.</p></li>
<li><p><strong>Blockwise quantization</strong>: Configure block sizes with <code class="docutils literal notranslate"><span class="pre">block_size_map</span></code>.</p></li>
</ul>
<p>For practical examples, see <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/backends/qualcomm/tests/test_qnn_delegate.py"><code class="docutils literal notranslate"><span class="pre">test_qnn_delegate.py</span></code></a>.</p>
</section>
</section>
<section id="step-3-configure-compile-specs">
<h4>Step 3: Configure Compile Specs<a class="headerlink" href="#step-3-configure-compile-specs" title="Link to this heading">#</a></h4>
<p>During this step, you will need to specify the target SoC, data type, and other QNN compiler spec.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">executorch.backends.qualcomm.utils.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">generate_qnn_executorch_compiler_spec</span><span class="p">,</span>
    <span class="n">generate_htp_compiler_spec</span><span class="p">,</span>
    <span class="n">QcomChipset</span><span class="p">,</span>
    <span class="n">to_edge_transform_and_lower_to_qnn</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># HTP Compiler Configuration</span>
<span class="n">backend_options</span> <span class="o">=</span> <span class="n">generate_htp_compiler_spec</span><span class="p">(</span>
    <span class="n">use_fp16</span><span class="o">=</span><span class="ow">not</span> <span class="n">quantized</span><span class="p">,</span>  <span class="c1"># False for quantized models</span>
<span class="p">)</span>

<span class="c1"># QNN Compiler Spec</span>
<span class="n">compile_spec</span> <span class="o">=</span> <span class="n">generate_qnn_executorch_compiler_spec</span><span class="p">(</span>
    <span class="n">soc_model</span><span class="o">=</span><span class="n">QcomChipset</span><span class="o">.</span><span class="n">SM8650</span><span class="p">,</span>  <span class="c1"># Your target SoC</span>
    <span class="n">backend_options</span><span class="o">=</span><span class="n">backend_options</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-4-lower-and-export-the-model">
<h4>Step 4: Lower and Export the Model<a class="headerlink" href="#step-4-lower-and-export-the-model" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lower to QNN backend</span>
<span class="n">delegated_program</span> <span class="o">=</span> <span class="n">to_edge_transform_and_lower_to_qnn</span><span class="p">(</span>
    <span class="n">quantized_model</span> <span class="k">if</span> <span class="n">quantized</span> <span class="k">else</span> <span class="n">model</span><span class="p">,</span>
    <span class="n">example_inputs</span><span class="p">,</span>
    <span class="n">compile_spec</span>
<span class="p">)</span>

<span class="c1"># Export to ExecuTorch format</span>
<span class="n">executorch_program</span> <span class="o">=</span> <span class="n">delegated_program</span><span class="o">.</span><span class="n">to_executorch</span><span class="p">()</span>

<span class="c1"># Save the compiled model</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;custom_model_qnn.pte&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">executorch_program</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model successfully exported to </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="deep-dive">
<h2>Deep Dive<a class="headerlink" href="#deep-dive" title="Link to this heading">#</a></h2>
<section id="partitioner-api">
<h3>Partitioner API<a class="headerlink" href="#partitioner-api" title="Link to this heading">#</a></h3>
<p>The <strong>QnnPartitioner</strong> identifies and groups supported subgraphs for execution on the QNN backend.<br />
It uses <code class="docutils literal notranslate"><span class="pre">QnnOperatorSupport</span></code> to check node-level compatibility with the Qualcomm backend via QNN SDK APIs.</p>
<p>The partitioner tags supported nodes with a <code class="docutils literal notranslate"><span class="pre">delegation_tag</span></code> and handles constants, buffers, and mutable states appropriately.
Please checkout <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/backends/qualcomm/partition/qnn_partitioner.py#L125">QNNPartitioner</a> for the latest changes. It mostly supports the following 4 inputs, and only compile spec is required</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">QnnPartitioner</span><span class="p">(</span><span class="n">Partitioner</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    QnnPartitioner identifies subgraphs that can be lowered to QNN backend, by tagging nodes for delegation,</span>
<span class="sd">    and manages special cases such as mutable buffers and consumed constants.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">compiler_specs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">CompileSpec</span><span class="p">],</span>
        <span class="n">skip_node_id_set</span><span class="p">:</span> <span class="nb">set</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">skip_node_op_set</span><span class="p">:</span> <span class="nb">set</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">skip_mutable_buffer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="quantization">
<h3>Quantization<a class="headerlink" href="#quantization" title="Link to this heading">#</a></h3>
<p>Quantization in the QNN backend supports multiple data bit-widths and training modes (PTQ/QAT).
The QnnQuantizer defines quantization configurations and annotations compatible with Qualcomm hardware.</p>
<p>Supported schemes include:</p>
<ul class="simple">
<li><p>8a8w (default)</p></li>
<li><p>16a16w</p></li>
<li><p>16a8w</p></li>
<li><p>16a4w</p></li>
<li><p>16a4w_block</p></li>
</ul>
<p>Highlights:</p>
<ul class="simple">
<li><p>QuantDtype enumerates bit-width combinations for activations and weights.</p></li>
<li><p>ModuleQConfig manages per-layer quantization behavior and observers.</p></li>
<li><p>QnnQuantizer integrates with PT2E prepare/convert flow to annotate and quantize models.</p></li>
</ul>
<p>Supports:</p>
<ul class="simple">
<li><p>Per-channel and per-block quantization</p></li>
<li><p>Custom quant annotation via custom_quant_annotations</p></li>
<li><p>Skipping specific nodes or ops</p></li>
<li><p>Per-module customization via submodule_qconfig_list</p></li>
</ul>
<p>For details, see: backends/qualcomm/quantizer/quantizer.py</p>
</section>
<section id="operator-support">
<h3>Operator Support<a class="headerlink" href="#operator-support" title="Link to this heading">#</a></h3>
<p>[The full operator support matrix](https://github.com/pytorch/executorch/tree/f32cdc3de6f7176d70a80228f1a60bcd45d93437/backends/qualcomm/builders#operator-support-status is tracked and frequently updated in the ExecuTorch repository.</p>
<p>It lists:</p>
<ul class="simple">
<li><p>Supported PyTorch ops (aten.*, custom ops)</p></li>
<li><p>Planned ops</p></li>
<li><p>Deprecated ops</p></li>
</ul>
<p>This matrix directly corresponds to the implementations in: <a class="reference external" href="https://github.com/pytorch/executorch/tree/main/backends/qualcomm/builders">executorch/backends/qualcomm/builders/node_visitors/*.py</a></p>
</section>
<section id="custom-ops-support">
<h3>Custom Ops Support<a class="headerlink" href="#custom-ops-support" title="Link to this heading">#</a></h3>
<p>You can extend QNN backend support for your own operators.
Follow the <a class="reference external" href="https://github.com/pytorch/executorch/tree/f32cdc3de6f7176d70a80228f1a60bcd45d93437/examples/qualcomm/custom_op#custom-operator-support">tutorial</a>:</p>
<p>It covers:</p>
<ul class="simple">
<li><p>Writing new NodeVisitor for your op</p></li>
<li><p>Registering via &#64;register_node_visitor</p></li>
<li><p>Creating and linking libQnnOp*.so for the delegate</p></li>
<li><p>Testing and verifying custom kernels on HTP</p></li>
</ul>
</section>
</section>
<section id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Link to this heading">#</a></h2>
<p>If you encounter any issues while reproducing the tutorial, please file a github
<a class="reference external" href="https://github.com/pytorch/executorch/issues">issue</a> on ExecuTorch repo and tag use <code class="docutils literal notranslate"><span class="pre">#qcom_aisw</span></code> tag</p>
<section id="debugging-tips">
<h3>Debugging tips<a class="headerlink" href="#debugging-tips" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Before trying any complicated models, try out <a class="reference external" href="https://github.com/pytorch/executorch/tree/f32cdc3de6f7176d70a80228f1a60bcd45d93437/examples/qualcomm#simple-examples-to-verify-the-backend-is-working">a simple model example</a> and see it if works one device.</p></li>
</ul>
</section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="backends/vulkan/tutorials/etvk-llama-tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exporting Llama 3.2 1B/3B Instruct to ExecuTorch Vulkan and running on device</p>
      </div>
    </a>
    <a class="right-next"
       href="backends-mediatek.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">MediaTek Backend</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="backends/vulkan/tutorials/etvk-llama-tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exporting Llama 3.2 1B/3B Instruct to ExecuTorch Vulkan and running on device</p>
      </div>
    </a>
    <a class="right-next"
       href="backends-mediatek.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">MediaTek Backend</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-qualcomm-ai-engine-direct">What’s Qualcomm AI Engine Direct?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites-hardware-and-software">Prerequisites (Hardware and Software)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#host-os">Host OS</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#windows-wsl-setup">Windows (WSL) Setup</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware">Hardware:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#software">Software:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-your-developer-environment">Setting up your developer environment</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conventions">Conventions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-environment-variables">Setup environment variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build">Build</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deploying-and-running-on-device">Deploying and running on device</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aot-compile-a-model">AOT compile a model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-model-inference-on-qnn-htp-emulator">Test model inference on QNN HTP emulator</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-model-inference-on-an-android-smartphone-with-qualcomm-socs">Run model inference on an Android smartphone with Qualcomm SoCs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-android-llamademo-with-qnn-backend">Run Android LlamaDemo with QNN backend</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#verification-steps">Verification Steps</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-issues-and-solutions">Common Issues and Solutions</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#issue-1-error-18-invalidargument">Issue 1: Error 18 (InvalidArgument)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#issue-2-error-1-internal-with-qnn-api-version-mismatch">Issue 2: Error 1 (Internal) with QNN API Version Mismatch</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#issue-3-native-code-changes-not-applied">Issue 3: Native Code Changes Not Applied</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#issue-4-logs-not-appearing">Issue 4: Logs Not Appearing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-model-list">Supported model list</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-support-a-custom-model-in-htp-backend">How to Support a Custom Model in HTP Backend</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-implementation-guide">Step-by-Step Implementation Guide</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-prepare-your-model">Step 1: Prepare Your Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-optional-quantize-your-model">Step 2: [Optional] Quantize Your Model</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-quantization-schemes">Supported Quantization Schemes</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#customization-options">Customization Options</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-features">Additional Features</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-configure-compile-specs">Step 3: Configure Compile Specs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-lower-and-export-the-model">Step 4: Lower and Export the Model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive">Deep Dive</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partitioner-api">Partitioner API</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization">Quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-support">Operator Support</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-ops-support">Custom Ops Support</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#faq">FAQ</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-tips">Debugging tips</a></li>
</ul>
</li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/executorch/edit/main/docs/source/backends-qualcomm.md">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/backends-qualcomm.md.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, ExecuTorch.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Qualcomm AI Engine Backend",
       "headline": "Qualcomm AI Engine Backend",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/backends-qualcomm.html",
       "articleBody": "Qualcomm AI Engine Backend# In this tutorial we will walk you through the process of getting started to build ExecuTorch for Qualcomm AI Engine Direct and running a model on it. Qualcomm AI Engine Direct is also referred to as QNN in the source and documentation. What you will learn in this tutorial: In this tutorial you will learn how to lower and deploy a model for Qualcomm AI Engine Direct. Tutorials we recommend you complete before this: Introduction to ExecuTorch Getting Started Building ExecuTorch with CMake What\u2019s Qualcomm AI Engine Direct?# Qualcomm AI Engine Direct is designed to provide unified, low-level APIs for AI development. Developers can interact with various accelerators on Qualcomm SoCs with these set of APIs, including Kryo CPU, Adreno GPU, and Hexagon processors. More details can be found here. Currently, this ExecuTorch Backend can delegate AI computations to Hexagon processors through Qualcomm AI Engine Direct APIs. Prerequisites (Hardware and Software)# Host OS# The QNN Backend is currently verified on the following Linux host operating systems: Ubuntu 22.04 LTS (x64) CentOS Stream 9 Windows Subsystem for Linux (WSL) with Ubuntu 22.04 In general, we verify the backend on the same OS versions that the QNN SDK is officially validated against. The exact supported versions are documented in the QNN SDK. Windows (WSL) Setup# To install Ubuntu 22.04 on WSL, run the following command in PowerShell or Windows Terminal: wsl --install -d ubuntu 22.04 This command will install WSL and set up Ubuntu 22.04 as the default Linux distribution. For more details and troubleshooting, refer to the official Microsoft WSL installation guide: \ud83d\udc49 Install WSL | Microsoft Learn Hardware:# You will need an Android / Linux device with adb-connected running on one of below Qualcomm SoCs: SA8295 SM8450 (Snapdragon 8 Gen 1) SM8475 (Snapdragon 8 Gen 1+) SM8550 (Snapdragon 8 Gen 2) SM8650 (Snapdragon 8 Gen 3) SM8750 (Snapdragon 8 Elite) SSG2115P SSG2125P SXR1230P (Linux Embedded) SXR2230P SXR2330P QCM6490 QCS9100 This example is verified with SM8550 and SM8450. Software:# Follow ExecuTorch recommended Python version. A compiler to compile AOT parts, e.g., the GCC compiler comes with Ubuntu LTS. g++ version need to be 13 or higher. Android NDK. This example is verified with NDK 26c. (Optional) Target toolchain for linux embedded platform. Qualcomm AI Engine Direct SDK Click the \u201cGet Software\u201d button to download the latest version of the QNN SDK. Although newer versions are available, we have verified and recommend using QNN 2.37.0 for stability. You can download it directly from the following link: QNN 2.37.0 The directory with installed Qualcomm AI Engine Direct SDK looks like: \u251c\u2500\u2500 benchmarks \u251c\u2500\u2500 bin \u251c\u2500\u2500 docs \u251c\u2500\u2500 examples \u251c\u2500\u2500 include \u251c\u2500\u2500 lib \u251c\u2500\u2500 LICENSE.pdf \u251c\u2500\u2500 NOTICE.txt \u251c\u2500\u2500 NOTICE_WINDOWS.txt \u251c\u2500\u2500 QNN_NOTICE.txt \u251c\u2500\u2500 QNN_README.txt \u251c\u2500\u2500 QNN_ReleaseNotes.txt \u251c\u2500\u2500 ReleaseNotes.txt \u251c\u2500\u2500 ReleaseNotesWindows.txt \u251c\u2500\u2500 sdk.yaml \u2514\u2500\u2500 share Setting up your developer environment# Conventions# $QNN_SDK_ROOT refers to the root of Qualcomm AI Engine Direct SDK, i.e., the directory containing QNN_README.txt. $ANDROID_NDK_ROOT refers to the root of Android NDK. $EXECUTORCH_ROOT refers to the root of executorch git repository. Setup environment variables# We set LD_LIBRARY_PATH to make sure the dynamic linker can find QNN libraries. Further, we set PYTHONPATH because it\u2019s easier to develop and import ExecuTorch Python APIs. export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib/x86_64-linux-clang/:$LD_LIBRARY_PATH export PYTHONPATH=$EXECUTORCH_ROOT/.. Build# An example script for the below building instructions is here. We recommend to use the script because the ExecuTorch build-command can change from time to time. The above script is actively used. It is updated more frequently than this tutorial. An example usage is cd $EXECUTORCH_ROOT # android target ./backends/qualcomm/scripts/build.sh # (optional) linux embedded target ./backends/qualcomm/scripts/build.sh --enable_linux_embedded # for release build ./backends/qualcomm/scripts/build.sh --release Deploying and running on device# AOT compile a model# Refer to this script for the exact flow. We use deeplab-v3-resnet101 as an example in this tutorial. Run below commands to compile: cd $EXECUTORCH_ROOT python -m examples.qualcomm.scripts.deeplab_v3 -b build-android -m SM8550 --compile_only --download You might see something like below: [INFO][Qnn ExecuTorch] Destroy Qnn context [INFO][Qnn ExecuTorch] Destroy Qnn device [INFO][Qnn ExecuTorch] Destroy Qnn backend opcode name target args kwargs ------------- ------------------------ --------------------------- ----------------------------- -------- placeholder arg684_1 arg684_1 () {} get_attr lowered_module_0 lowered_module_0 () {} call_function executorch_call_delegate executorch_call_delegate (lowered_module_0, arg684_1) {} call_function getitem \u003cbuilt-in function getitem\u003e (executorch_call_delegate, 0) {} call_function getitem_1 \u003cbuilt-in function getitem\u003e (executorch_call_delegate, 1) {} output output output ([getitem_1, getitem],) {} The compiled model is ./deeplab_v3/dlv3_qnn.pte. Test model inference on QNN HTP emulator# We can test model inferences before deploying it to a device by HTP emulator. Let\u2019s build qnn_executor_runner for a x64 host: # assuming the AOT component is built. cd $EXECUTORCH_ROOT/build-x86 cmake ../examples/qualcomm \\ -DCMAKE_PREFIX_PATH=\"$PWD/lib/cmake/ExecuTorch;$PWD/third-party/gflags;\" \\ -DCMAKE_FIND_ROOT_PATH_MODE_PACKAGE=BOTH \\ -DPYTHON_EXECUTABLE=python3 \\ -Bexamples/qualcomm cmake --build examples/qualcomm -j$(nproc) # qnn_executor_runner can be found under examples/qualcomm/executor_runner # The full path is $EXECUTORCH_ROOT/build-x86/examples/qualcomm/executor_runner/qnn_executor_runner ls examples/qualcomm/executor_runner To run the HTP emulator, the dynamic linker needs to access QNN libraries and libqnn_executorch_backend.so. We set the below two paths to LD_LIBRARY_PATH environment variable: $QNN_SDK_ROOT/lib/x86_64-linux-clang/ $EXECUTORCH_ROOT/build-x86/lib/ The first path is for QNN libraries including HTP emulator. It has been configured in the AOT compilation section. The second path is for libqnn_executorch_backend.so. So, we can run ./deeplab_v3/dlv3_qnn.pte by: cd $EXECUTORCH_ROOT/build-x86 export LD_LIBRARY_PATH=$EXECUTORCH_ROOT/build-x86/lib/:$LD_LIBRARY_PATH examples/qualcomm/executor_runner/qnn_executor_runner --model_path ../deeplab_v3/dlv3_qnn.pte We should see some outputs like the below. Note that the emulator can take some time to finish. I 00:00:00.354662 executorch:qnn_executor_runner.cpp:213] Method loaded. I 00:00:00.356460 executorch:qnn_executor_runner.cpp:261] ignoring error from set_output_data_ptr(): 0x2 I 00:00:00.357991 executorch:qnn_executor_runner.cpp:261] ignoring error from set_output_data_ptr(): 0x2 I 00:00:00.357996 executorch:qnn_executor_runner.cpp:265] Inputs prepared. I 00:01:09.328144 executorch:qnn_executor_runner.cpp:414] Model executed successfully. I 00:01:09.328159 executorch:qnn_executor_runner.cpp:421] Write etdump to etdump.etdp, Size = 424 [INFO] [Qnn ExecuTorch]: Destroy Qnn backend parameters [INFO] [Qnn ExecuTorch]: Destroy Qnn context [INFO] [Qnn ExecuTorch]: Destroy Qnn device [INFO] [Qnn ExecuTorch]: Destroy Qnn backend Run model inference on an Android smartphone with Qualcomm SoCs# Step 1. We need to push required QNN libraries to the device. # make sure you have write-permission on below path. DEVICE_DIR=/data/local/tmp/executorch_qualcomm_tutorial/ adb shell \"mkdir -p ${DEVICE_DIR}\" adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnHtp.so ${DEVICE_DIR} adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnSystem.so ${DEVICE_DIR} adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnHtpV69Stub.so ${DEVICE_DIR} adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnHtpV73Stub.so ${DEVICE_DIR} adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnHtpV75Stub.so ${DEVICE_DIR} adb push ${QNN_SDK_ROOT}/lib/aarch64-android/libQnnHtpV79Stub.so ${DEVICE_DIR} adb push ${QNN_SDK_ROOT}/lib/hexagon-v69/unsigned/libQnnHtpV69Skel.so ${DEVICE_DIR} adb push ${QNN_SDK_ROOT}/lib/hexagon-v73/unsigned/libQnnHtpV73Skel.so ${DEVICE_DIR} adb push ${QNN_SDK_ROOT}/lib/hexagon-v75/unsigned/libQnnHtpV75Skel.so ${DEVICE_DIR} adb push ${QNN_SDK_ROOT}/lib/hexagon-v79/unsigned/libQnnHtpV79Skel.so ${DEVICE_DIR} Step 2. We also need to indicate dynamic linkers on Android and Hexagon where to find these libraries by setting ADSP_LIBRARY_PATH and LD_LIBRARY_PATH. So, we can run qnn_executor_runner like adb push ./deeplab_v3/dlv3_qnn.pte ${DEVICE_DIR} adb push ${EXECUTORCH_ROOT}/build-android/examples/qualcomm/executor_runner/qnn_executor_runner ${DEVICE_DIR} adb push ${EXECUTORCH_ROOT}/build-android/lib/libqnn_executorch_backend.so ${DEVICE_DIR} adb shell \"cd ${DEVICE_DIR} \\ \u0026\u0026 export LD_LIBRARY_PATH=${DEVICE_DIR} \\ \u0026\u0026 export ADSP_LIBRARY_PATH=${DEVICE_DIR} \\ \u0026\u0026 ./qnn_executor_runner --model_path ./dlv3_qnn.pte\" You should see something like below: I 00:00:00.257354 executorch:qnn_executor_runner.cpp:213] Method loaded. I 00:00:00.323502 executorch:qnn_executor_runner.cpp:262] ignoring error from set_output_data_ptr(): 0x2 I 00:00:00.357496 executorch:qnn_executor_runner.cpp:262] ignoring error from set_output_data_ptr(): 0x2 I 00:00:00.357555 executorch:qnn_executor_runner.cpp:265] Inputs prepared. I 00:00:00.364824 executorch:qnn_executor_runner.cpp:414] Model executed successfully. I 00:00:00.364875 executorch:qnn_executor_runner.cpp:425] Write etdump to etdump.etdp, Size = 424 [INFO] [Qnn ExecuTorch]: Destroy Qnn backend parameters [INFO] [Qnn ExecuTorch]: Destroy Qnn context [INFO] [Qnn ExecuTorch]: Destroy Qnn backend The model is merely executed. If we want to feed real inputs and get model outputs, we can use cd $EXECUTORCH_ROOT # android python -m examples.qualcomm.scripts.deeplab_v3 -b build-android -m SM8550 --download -s \u003cdevice_serial\u003e # (optional) linux embedded python -m examples.qualcomm.scripts.deeplab_v3 -b build-oe-linux -m SXR1230P --download -s \u003cdevice_serial\u003e -t aarch64-oe-linux-gcc-9.3 The \u003cdevice_serial\u003e can be found by adb devices command. After the above command, pre-processed inputs and outputs are put in $EXECUTORCH_ROOT/deeplab_v3 and $EXECUTORCH_ROOT/deeplab_v3/outputs folder. The command-line arguments are written in utils.py. The model, inputs, and output location are passed to qnn_executorch_runner by --model_path, --input_list_path, and --output_folder_path. Run Android LlamaDemo with QNN backend# $DEMO_APP refers to the root of the executorch android demo, i.e., the directory containing build.gradle.kts. Step 1: Rebuild ExecuTorch AAR # Build the AAR cd $EXECUTORCH_ROOT export BUILD_AAR_DIR=$EXECUTORCH_ROOT/aar-out ./scripts/build_android_library.sh Step 2: Copy AAR to Android Project cp $EXECUTORCH_ROOT/aar-out/executorch.aar \\ $DEMO_APP/app/libs/executorch.aar Step 3: Build Android APK cd $DEMO_APP ./gradlew clean assembleDebug -PuseLocalAar=true Step 4: Install on Device adb install -r app/build/outputs/apk/debug/app-debug.apk Step 5: Push model adb shell mkdir -p /data/local/tmp/llama adb push model.pte /data/local/tmp/llama adb push tokenizer.bin /data/local/tmp/llama Step 6: Run the Llama Demo Open the App on Android Select QUALCOMM backend Select model.pte Model Select tokenizer.bin Tokenizer Select Model Type Click LOAD MODEL It should show Successfully loaded model. Verification Steps# Step 1. Verify AAR Contains Your Changes # Check for debug strings in the AAR unzip -p $DEMO_APP/app/libs/executorch.aar jni/arm64-v8a/libexecutorch.so | \\ strings | grep \"QNN\" # Replace \"QNN\" with your actual debug string if needed If found, your changes are in the AAR. Step 2. Verify APK Contains Correct Libraries # Check QNN library version in APK cd $DEMO_APP unzip -l app/build/outputs/apk/debug/app-debug.apk | grep \"libQnnHtp.so\" Expected size for QNN 2.37.0: ~2,465,440 bytes Step 3. Monitor Logs During Model Loading adb logcat -c adb logcat | grep -E \"ExecuTorch\" Common Issues and Solutions# Issue 1: Error 18 (InvalidArgument)# Cause: Wrong parameter order in Runner constructor or missing QNN config Solution: Check $EXECUTORCH_ROOT/examples/qualcomm/oss_scripts/llama/runner/runner.h for the correct constructor signature. Issue 2: Error 1 (Internal) with QNN API Version Mismatch# Symptoms: W [Qnn ExecuTorch]: Qnn API version 2.33.0 is mismatched E [Qnn ExecuTorch]: Using newer context binary on old SDK E [Qnn ExecuTorch]: Can\u0027t create context from binary. Error 5000 Cause: Model compiled with QNN SDK version X but APK uses QNN runtime version Y Solution: Update build.gradle.kts with matching QNN runtime version Note: The version numbers below (2.33.0 and 2.37.0) are examples only. Please check for the latest compatible QNN runtime version or match your QNN SDK version to avoid API mismatches. Before: implementation(\"com.qualcomm.qti:qnn-runtime:2.33.0\") After: implementation(\"com.qualcomm.qti:qnn-runtime:2.37.0\") Or recompile model with matching QNN SDK version Issue 3: Native Code Changes Not Applied# Symptoms: Debug logs don\u2019t appear Behavior doesn\u2019t change Cause: Gradle using Maven dependency instead of local AAR Solution: Always build with -PuseLocalAar=true flag Issue 4: Logs Not Appearing# Cause: Wrong logging tag filter Solution: QNN uses \u201cExecuTorch\u201d tag: adb logcat | grep \"ExecuTorch\" Supported model list# Please refer to $EXECUTORCH_ROOT/examples/qualcomm/scripts/ and $EXECUTORCH_ROOT/examples/qualcomm/oss_scripts/ to the list of supported models. Each script demonstrates: Model export (torch.export) Quantization (PTQ/QAT) Lowering and compilation to QNN delegate Deployment on device or HTP emulator How to Support a Custom Model in HTP Backend# Step-by-Step Implementation Guide# Please reference the simple example and more complicated examples for reference Step 1: Prepare Your Model# import torch # Initialize your custom model model = YourModelClass().eval() # Your custom PyTorch model # Create example inputs (adjust shape as needed) example_inputs = (torch.randn(1, 3, 224, 224),) # Example input tensor Step 2: [Optional] Quantize Your Model# Choose between quantization approaches, post training quantization (PTQ) or quantization aware training (QAT): from executorch.backends.qualcomm.quantizer.quantizer import QnnQuantizer from torchao.quantization.pt2e.quantize_pt2e import prepare_pt2e, prepare_qat_pt2e, convert_pt2e quantizer = QnnQuantizer() m = torch.export.export(model, example_inputs, strict=True).module() # PTQ (Post-Training Quantization) if quantization_type == \"ptq\": prepared_model = prepare_pt2e(m, quantizer) # Calibration loop would go here prepared_model(*example_inputs) # QAT (Quantization-Aware Training) elif quantization_type == \"qat\": prepared_model = prepare_qat_pt2e(m, quantizer) # Training loop would go here for _ in range(training_steps): prepared_model(*example_inputs) # Convert to quantized model quantized_model = convert_pt2e(prepared_model) The QNNQuantizer is configurable, with the default setting being 8a8w. For advanced users, refer to the QnnQuantizer documentation for details. Supported Quantization Schemes# 8a8w (default) 16a16w 16a8w 16a4w 16a4w_block Customization Options# Per-node annotation: Use custom_quant_annotations. Per-module (nn.Module) annotation: Use submodule_qconfig_list. Additional Features# Node exclusion: Discard specific nodes via discard_nodes. Blockwise quantization: Configure block sizes with block_size_map. For practical examples, see test_qnn_delegate.py. Step 3: Configure Compile Specs# During this step, you will need to specify the target SoC, data type, and other QNN compiler spec. from executorch.backends.qualcomm.utils.utils import ( generate_qnn_executorch_compiler_spec, generate_htp_compiler_spec, QcomChipset, to_edge_transform_and_lower_to_qnn, ) # HTP Compiler Configuration backend_options = generate_htp_compiler_spec( use_fp16=not quantized, # False for quantized models ) # QNN Compiler Spec compile_spec = generate_qnn_executorch_compiler_spec( soc_model=QcomChipset.SM8650, # Your target SoC backend_options=backend_options, ) Step 4: Lower and Export the Model# # Lower to QNN backend delegated_program = to_edge_transform_and_lower_to_qnn( quantized_model if quantized else model, example_inputs, compile_spec ) # Export to ExecuTorch format executorch_program = delegated_program.to_executorch() # Save the compiled model model_name = \"custom_model_qnn.pte\" with open(model_name, \"wb\") as f: f.write(executorch_program.buffer) print(f\"Model successfully exported to {model_name}\") Deep Dive# Partitioner API# The QnnPartitioner identifies and groups supported subgraphs for execution on the QNN backend. It uses QnnOperatorSupport to check node-level compatibility with the Qualcomm backend via QNN SDK APIs. The partitioner tags supported nodes with a delegation_tag and handles constants, buffers, and mutable states appropriately. Please checkout QNNPartitioner for the latest changes. It mostly supports the following 4 inputs, and only compile spec is required class QnnPartitioner(Partitioner): \"\"\" QnnPartitioner identifies subgraphs that can be lowered to QNN backend, by tagging nodes for delegation, and manages special cases such as mutable buffers and consumed constants. \"\"\" def __init__( self, compiler_specs: List[CompileSpec], skip_node_id_set: set = None, skip_node_op_set: set = None, skip_mutable_buffer: bool = False, ): ... Quantization# Quantization in the QNN backend supports multiple data bit-widths and training modes (PTQ/QAT). The QnnQuantizer defines quantization configurations and annotations compatible with Qualcomm hardware. Supported schemes include: 8a8w (default) 16a16w 16a8w 16a4w 16a4w_block Highlights: QuantDtype enumerates bit-width combinations for activations and weights. ModuleQConfig manages per-layer quantization behavior and observers. QnnQuantizer integrates with PT2E prepare/convert flow to annotate and quantize models. Supports: Per-channel and per-block quantization Custom quant annotation via custom_quant_annotations Skipping specific nodes or ops Per-module customization via submodule_qconfig_list For details, see: backends/qualcomm/quantizer/quantizer.py Operator Support# [The full operator support matrix](https://github.com/pytorch/executorch/tree/f32cdc3de6f7176d70a80228f1a60bcd45d93437/backends/qualcomm/builders#operator-support-status is tracked and frequently updated in the ExecuTorch repository. It lists: Supported PyTorch ops (aten.*, custom ops) Planned ops Deprecated ops This matrix directly corresponds to the implementations in: executorch/backends/qualcomm/builders/node_visitors/*.py Custom Ops Support# You can extend QNN backend support for your own operators. Follow the tutorial: It covers: Writing new NodeVisitor for your op Registering via @register_node_visitor Creating and linking libQnnOp*.so for the delegate Testing and verifying custom kernels on HTP FAQ# If you encounter any issues while reproducing the tutorial, please file a github issue on ExecuTorch repo and tag use #qcom_aisw tag Debugging tips# Before trying any complicated models, try out a simple model example and see it if works one device.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/backends-qualcomm.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>