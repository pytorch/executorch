
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Kernel Registration &#8212; ExecuTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=a8da1a53"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'kernel-library-custom-aten-kernel';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.pytorch.org/executorch/executorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://docs.pytorch.org/executorch/kernel-library-custom-aten-kernel.html" />
    <link rel="icon" href="_static/executorch-chip-logo.svg"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Kernel Library Selective Build" href="kernel-library-selective-build.html" />
    <link rel="prev" title="Overview of ExecuTorch’s Kernel Libraries" href="kernel-library-overview.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/executorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/et-logo.png" class="logo__image only-light" alt="ExecuTorch main documentation - Home"/>
    <script>document.write(`<img src="_static/et-logo.png" class="logo__image only-dark" alt="ExecuTorch main documentation - Home"/>`);</script>
  
  
</a></div>
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="intro-section.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="quick-start-section.html">
    Quick Start
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="edge-platforms-section.html">
    Edge
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="backends-section.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="llm/working-with-llms.html">
    LLMs
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="advanced-topics-section.html">
    Advanced
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tools-section.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api-section.html">
    API
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="support-section.html">
    Support
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="intro-section.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="quick-start-section.html">
    Quick Start
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="edge-platforms-section.html">
    Edge
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="backends-section.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="llm/working-with-llms.html">
    LLMs
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="advanced-topics-section.html">
    Advanced
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tools-section.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api-section.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="support-section.html">
    Support
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="quantization-optimization.html">Quantization &amp; Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="quantization-overview.html">Quantization Overview</a></li>

<li class="toctree-l2"><a class="reference internal" href="runtime-profiling.html">Profiling Models in ExecuTorch</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="using-executorch-export.html">Model Export and Lowering</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="kernel-library-advanced.html">Kernel Library Deep Dive</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="kernel-library-overview.html">Overview of ExecuTorch’s Kernel Libraries</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Kernel Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="kernel-library-selective-build.html">Kernel Library Selective Build</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="backend-delegate-advanced.html">Backend &amp; Delegates</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="backend-delegates-integration.html">Integrating a Backend Delegate into ExecuTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="backend-delegates-dependencies.html">Third-Party Dependency Management for Backend Delegates</a></li>
<li class="toctree-l2"><a class="reference internal" href="compiler-delegate-and-partitioner.html">Understanding Backends and Delegates</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug-backend-delegate.html">Debugging Delegation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="runtime-integration-advanced.html">Runtime &amp; Integration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="runtime-platform-abstraction-layer.html">Runtime Platform Abstraction Layer (PAL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="portable-cpp-programming.html">Portable C++ Programming</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="compiler-ir-advanced.html">Compiler &amp; IR</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="compiler-custom-compiler-passes.html">Custom Compiler Passes and Partitioners</a></li>
<li class="toctree-l2"><a class="reference internal" href="compiler-memory-planning.html">Memory Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ir-exir.html">Export IR Specification</a></li>
<li class="toctree-l2"><a class="reference internal" href="ir-ops-set-definition.html">Definition of the Core ATen Operator Set</a></li>
<li class="toctree-l2"><a class="reference internal" href="compiler-backend-dialect.html">Backend Dialect</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="file-formats-advanced.html">File Formats</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pte-file-format.html"><code class="docutils literal notranslate"><span class="pre">.pte</span></code> file format</a></li>
<li class="toctree-l2"><a class="reference internal" href="ptd-file-format.html"><code class="docutils literal notranslate"><span class="pre">.ptd</span></code> file format</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="advanced-topics-section.html" class="nav-link">Advanced</a></li>
    
    
    <li class="breadcrumb-item"><a href="kernel-library-advanced.html" class="nav-link">Kernel Library Deep Dive</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Kernel Registration</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="advanced-topics-section.html">
        <meta itemprop="name" content="Advanced">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="kernel-library-advanced.html">
        <meta itemprop="name" content="Kernel Library Deep Dive">
        <meta itemprop="position" content="2">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Kernel Registration">
        <meta itemprop="position" content="3">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="kernel-registration">
<h1>Kernel Registration<a class="headerlink" href="#kernel-registration" title="Link to this heading">#</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>At the last stage of <a class="reference internal" href="export-overview.html"><span class="std std-doc">ExecuTorch model exporting</span></a>, we lower the operators in the dialect to the <em>out variants</em> of the <a class="reference internal" href="ir-ops-set-definition.html"><span class="std std-doc">core ATen operators</span></a>. Then we serialize these operator names into the model artifact. During runtime execution, for each operator name we will need to find the actual <em>kernels</em>, i.e., the C++ functions that do the heavy-lifting calculations and return results.</p>
</section>
<section id="kernel-libraries">
<h2>Kernel Libraries<a class="headerlink" href="#kernel-libraries" title="Link to this heading">#</a></h2>
<section id="first-party-kernel-libraries">
<h3>First-party kernel libraries:<a class="headerlink" href="#first-party-kernel-libraries" title="Link to this heading">#</a></h3>
<p><strong><a class="reference external" href="https://github.com/pytorch/executorch/tree/main/kernels/portable">Portable kernel library</a></strong> is the in-house default kernel library that covers most of the core ATen operators. It’s easy to use/read and is written in portable C++17. However it’s not optimized for performance, because it’s not specialized for any certain target. Therefore we provide kernel registration APIs for ExecuTorch users to easily register their own optimized kernels.</p>
<p><strong><a class="reference external" href="https://github.com/pytorch/executorch/tree/main/kernels/optimized">Optimized kernel library</a></strong> specializes on performance for some of the operators, leveraging existing third party libraries such as <a class="reference external" href="https://gitlab.com/libeigen/eigen">EigenBLAS</a>. This works best along with the portable kernel library, with a good balance on portability and performance. One example of combining these two libraries can be found <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/configurations/CMakeLists.txt">here</a>.</p>
<p><strong><a class="reference external" href="https://github.com/pytorch/executorch/tree/main/kernels/quantized">Quantized kernel library</a></strong> implements operators for quantization and dequantization. These are out of core ATen operators but are vital to most of the production use cases.</p>
</section>
<section id="custom-kernel-libraries">
<h3>Custom kernel libraries:<a class="headerlink" href="#custom-kernel-libraries" title="Link to this heading">#</a></h3>
<p><strong>Custom kernels implementing core ATen ops</strong>. Even though we don’t have an internal example for custom kernels for core ATen ops, the optimized kernel library can be viewed as a good example. We have optimized <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/kernels/optimized/cpu/op_add.cpp"><code class="docutils literal notranslate"><span class="pre">add.out</span></code></a> and a portable <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/kernels/portable/cpu/op_add.cpp"><code class="docutils literal notranslate"><span class="pre">add.out</span></code></a>. When user is combining these two libraries, we provide APIs to choose which kernel to use for <code class="docutils literal notranslate"><span class="pre">add.out</span></code>. In order to author and use custom kernels implementing core ATen ops, using the <a class="reference internal" href="#yaml-entry-for-core-aten-op-out-variant"><span class="xref myst">YAML based approach</span></a> is recommended, because it provides full fledged support on</p>
<ol class="arabic simple">
<li><p>combining kernel libraries and define fallback kernels;</p></li>
<li><p>using selective build to minimize the kernel size.</p></li>
</ol>
<p>A <strong><a class="reference external" href="https://github.com/pytorch/executorch/tree/main/extension/llm/custom_ops">Custom operator</a></strong> is any operator that an ExecuTorch user defines outside of PyTorch’s <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml"><code class="docutils literal notranslate"><span class="pre">native_functions.yaml</span></code></a>.</p>
</section>
</section>
<section id="operator-kernel-contract">
<h2>Operator &amp; Kernel Contract<a class="headerlink" href="#operator-kernel-contract" title="Link to this heading">#</a></h2>
<p>All the kernels mentioned above, whether they are in-house or customized, should comply with the following requirements:</p>
<ul class="simple">
<li><p>Match the calling convention derived from operator schema. The kernel registration API will generate headers for the custom kernels as references.</p></li>
<li><p>Satisfy the dtype constraints defined in edge dialect. For tensors with certain dtypes as arguments, the result of a custom kernel needs to match the expected dtypes. The constraints are available in edge dialect ops.</p></li>
<li><p>Give correct result. We will provide a testing framework to automatically test the custom kernels.</p></li>
</ul>
</section>
<section id="apis">
<h2>APIs<a class="headerlink" href="#apis" title="Link to this heading">#</a></h2>
<p>These are the APIs available to register kernels/custom kernels/custom ops into ExecuTorch:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#yaml-entry-api-high-level-architecture">YAML Entry API</a></p>
<ul>
<li><p><a class="reference internal" href="#yaml-entry-api-for-core-aten-op-out-variant">for core ATen op with custom kernels</a></p></li>
<li><p><a class="reference internal" href="#yaml-entry-api-for-custom-ops">for custom ops</a></p></li>
<li><p><a class="reference internal" href="#cmake-macros">CMake Macros</a></p></li>
</ul>
</li>
<li><p>C++ API</p>
<ul>
<li><p><a class="reference internal" href="#c-api-for-custom-ops">for custom ops</a></p></li>
<li><p><a class="reference internal" href="#compile-and-link-the-custom-kernel">CMake Example</a></p></li>
</ul>
</li>
</ul>
<p>If it’s not clear which API to use, please see <a class="reference internal" href="#custom-ops-api-best-practices">Best Practices</a>.</p>
<section id="yaml-entry-api-high-level-architecture">
<h3>YAML Entry API High Level Architecture<a class="headerlink" href="#yaml-entry-api-high-level-architecture" title="Link to this heading">#</a></h3>
<p><img alt="" src="_images/kernel-library-custom-aten-kernel.png" /></p>
<p>ExecuTorch users are asked to provide:</p>
<ol class="arabic simple">
<li><p>the custom kernel library with C++ implementations</p></li>
<li><p>a YAML file associated with the library that describes what operators are being implemented by this library. For partial kernels, the yaml file also contains information on the dtypes and dim orders supported by the  kernel. More details in the API section.</p></li>
</ol>
</section>
<section id="yaml-entry-api-workflow">
<h3>YAML Entry API Workflow<a class="headerlink" href="#yaml-entry-api-workflow" title="Link to this heading">#</a></h3>
<p>At build time, the yaml files associated with kernel libraries will be passed to the <em>kernel resolver</em> along with the model op info (see selective build doc) and the outcome is a mapping between a combination of operator names and tensor metadata, to kernel symbols. Then codegen tools will use this mapping to generate C++ bindings that connect the kernels to ExecuTorch runtime. ExecuTorch users need to link this generated library into their application to use these kernels.</p>
<p>At static object initialization time, kernels will be registered into the ExecuTorch kernel registry.</p>
<p>At runtime initialization stage, ExecuTorch will use the operator name and argument metadata as a key to lookup for the kernels. For example, with “aten::add.out” and inputs being float tensors with dim order (0, 1, 2, 3), ExecuTorch will go into the kernel registry and lookup for a kernel that matches the name and the input metadata.</p>
</section>
<section id="yaml-entry-api-for-core-aten-op-out-variant">
<h3>YAML Entry API for Core ATen Op Out Variant<a class="headerlink" href="#yaml-entry-api-for-core-aten-op-out-variant" title="Link to this heading">#</a></h3>
<p>Top level attributes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">op</span></code> (if the operator appears in <code class="docutils literal notranslate"><span class="pre">native_functions.yaml</span></code>) or <code class="docutils literal notranslate"><span class="pre">func</span></code> for custom operator. The value for this key needs to be the full operator name (including overload name) for <code class="docutils literal notranslate"><span class="pre">op</span></code> key, or a full operator schema (namespace, operator name, operator overload name and schema string), if we are describing a custom operator. For schema syntax please refer to this <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md">instruction</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernels</span></code>: defines kernel information. It consists of <code class="docutils literal notranslate"><span class="pre">arg_meta</span></code> and <code class="docutils literal notranslate"><span class="pre">kernel_name</span></code>, which are bound together to describe “for input tensors with these metadata, use this kernel”.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">type_alias</span></code>(optional): we are giving aliases to possible dtype options. <code class="docutils literal notranslate"><span class="pre">T0:</span> <span class="pre">[Double,</span> <span class="pre">Float]</span></code> means <code class="docutils literal notranslate"><span class="pre">T0</span></code> can be one of <code class="docutils literal notranslate"><span class="pre">Double</span></code> or <code class="docutils literal notranslate"><span class="pre">Float</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim_order_alias</span></code>(optional): similar to <code class="docutils literal notranslate"><span class="pre">type_alias</span></code>, we are giving names to possible dim order options.</p></li>
</ul>
<p>Attributes under <code class="docutils literal notranslate"><span class="pre">kernels</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">arg_meta</span></code>: a list of “tensor arg name” entries. The values for these keys are dtypes and dim orders aliases, that are implemented by the corresponding <code class="docutils literal notranslate"><span class="pre">kernel_name</span></code>. This being <code class="docutils literal notranslate"><span class="pre">null</span></code> means the kernel will be used for all types of input.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_name</span></code>: the expected name of the C++ function that will implement this operator. You can put whatever you want to here, but you should follow the convention of replacing the <code class="docutils literal notranslate"><span class="pre">.</span></code> in the overload name with an underscore, and lowercasing all characters. In this example, <code class="docutils literal notranslate"><span class="pre">add.out</span></code> uses the C++ function named <code class="docutils literal notranslate"><span class="pre">add_out</span></code>. <code class="docutils literal notranslate"><span class="pre">add.Scalar_out</span></code> would become <code class="docutils literal notranslate"><span class="pre">add_scalar_out</span></code>, with a lowercase <code class="docutils literal notranslate"><span class="pre">S</span></code>. We support namespace for kernels, but note that we will be inserting a <code class="docutils literal notranslate"><span class="pre">native::</span></code> to the last level of namespace. So <code class="docutils literal notranslate"><span class="pre">custom::add_out</span></code> in the <code class="docutils literal notranslate"><span class="pre">kernel_name</span></code> will point to <code class="docutils literal notranslate"><span class="pre">custom::native::add_out</span></code>.</p></li>
</ul>
<p>Some examples of operator entry:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">op</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">add.out</span>
<span class="w">  </span><span class="nt">kernels</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">arg_meta</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">      </span><span class="nt">kernel_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torch::executor::add_out</span>
</pre></div>
</div>
<p>An out variant of a core ATen operator with a default kernel</p>
<p>ATen operator with a dtype/dim order specialized kernel (works for <code class="docutils literal notranslate"><span class="pre">Double</span></code> dtype and dim order needs to be (0, 1, 2, 3))</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">op</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">add.out</span>
<span class="w">  </span><span class="nt">type_alias</span><span class="p">:</span>
<span class="w">    </span><span class="nt">T0</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">Double</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">dim_order_alias</span><span class="p">:</span>
<span class="w">    </span><span class="nt">D0</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[[</span><span class="nv">0</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">1</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">2</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">3</span><span class="p p-Indicator">]]</span>
<span class="w">  </span><span class="nt">kernels</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">arg_meta</span><span class="p">:</span>
<span class="w">        </span><span class="nt">self</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">T0</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">D0</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">other</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">T0</span><span class="w"> </span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">D0</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">out</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">T0</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">D0</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="nt">kernel_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torch::executor::add_out</span>
</pre></div>
</div>
</section>
<section id="yaml-entry-api-for-custom-ops">
<h3>YAML Entry API for Custom Ops<a class="headerlink" href="#yaml-entry-api-for-custom-ops" title="Link to this heading">#</a></h3>
<p>As mentioned above, this option provides more support in terms of selective build and features such as merging operator libraries.</p>
<p>First we need to specify the operator schema as well as a <code class="docutils literal notranslate"><span class="pre">kernel</span></code> section. So instead of <code class="docutils literal notranslate"><span class="pre">op</span></code> we use <code class="docutils literal notranslate"><span class="pre">func</span></code> with the operator schema. As an example, here’s a yaml entry for a custom op:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">func</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">allclose.out(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False, bool dummy_param=False, *, Tensor(a!) out) -&gt; Tensor(a!)</span>
<span class="w">  </span><span class="nt">kernels</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">arg_meta</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">      </span><span class="nt">kernel_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torch::executor::allclose_out</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">kernel</span></code> section is the same as the one defined in core ATen ops. For operator schema, we are reusing the DSL defined in this <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md">README.md</a>, with a few differences:</p>
<section id="out-variants-only">
<h4>Out variants only<a class="headerlink" href="#out-variants-only" title="Link to this heading">#</a></h4>
<p>ExecuTorch only supports out-style operators, where:</p>
<ul class="simple">
<li><p>The caller provides the output Tensor or Tensor list in the final position with the name <code class="docutils literal notranslate"><span class="pre">out</span></code>.</p></li>
<li><p>The C++ function modifies and returns the same <code class="docutils literal notranslate"><span class="pre">out</span></code> argument.</p>
<ul>
<li><p>If the return type in the YAML file is <code class="docutils literal notranslate"><span class="pre">()</span></code> (which maps to void), the C++ function should still modify <code class="docutils literal notranslate"><span class="pre">out</span></code> but does not need to return anything.</p></li>
</ul>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">out</span></code> argument must be keyword-only, which means it needs to follow an argument named <code class="docutils literal notranslate"><span class="pre">*</span></code> like in the <code class="docutils literal notranslate"><span class="pre">add.out</span></code> example below.</p></li>
<li><p>Conventionally, these out operators are named using the pattern <code class="docutils literal notranslate"><span class="pre">&lt;name&gt;.out</span></code> or <code class="docutils literal notranslate"><span class="pre">&lt;name&gt;.&lt;overload&gt;_out</span></code>.</p></li>
</ul>
<p>Since all output values are returned via an <code class="docutils literal notranslate"><span class="pre">out</span></code> parameter, ExecuTorch ignores the actual C++ function return value. But, to be consistent, functions should always return <code class="docutils literal notranslate"><span class="pre">out</span></code> when the return type is non-<code class="docutils literal notranslate"><span class="pre">void</span></code>.</p>
</section>
<section id="can-only-return-tensor-or">
<h4>Can only return <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> or <code class="docutils literal notranslate"><span class="pre">()</span></code><a class="headerlink" href="#can-only-return-tensor-or" title="Link to this heading">#</a></h4>
<p>ExecuTorch only supports operators that return a single <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, or the unit type <code class="docutils literal notranslate"><span class="pre">()</span></code> (which maps to <code class="docutils literal notranslate"><span class="pre">void</span></code>). It does not support returning any other types, including lists, optionals, tuples, or scalars like <code class="docutils literal notranslate"><span class="pre">bool</span></code>.</p>
</section>
<section id="supported-argument-types">
<h4>Supported argument types<a class="headerlink" href="#supported-argument-types" title="Link to this heading">#</a></h4>
<p>ExecuTorch does not support all of the argument types that core PyTorch supports. Here’s a list of the argument types we currently support:</p>
<ul class="simple">
<li><p>Tensor</p></li>
<li><p>int</p></li>
<li><p>bool</p></li>
<li><p>float</p></li>
<li><p>str</p></li>
<li><p>Scalar</p></li>
<li><p>ScalarType</p></li>
<li><p>MemoryFormat</p></li>
<li><p>Device</p></li>
<li><p>Optional<Type></p></li>
<li><p>List<Type></p></li>
<li><p>List&lt;Optional<Type>&gt;</p></li>
<li><p>Optional&lt;List<Type>&gt;</p></li>
</ul>
</section>
<section id="cmake-macros">
<h4>CMake Macros<a class="headerlink" href="#cmake-macros" title="Link to this heading">#</a></h4>
<p>We provide build time macros to help users to build their kernel registration library. The macro takes the yaml file describing the kernel library as well as model operator metadata, and packages the generated C++ bindings into a C++ library. The macro is available on CMake.</p>
<p><code class="docutils literal notranslate"><span class="pre">generate_bindings_for_kernels(FUNCTIONS_YAML</span> <span class="pre">functions_yaml</span> <span class="pre">CUSTOM_OPS_YAML</span> <span class="pre">custom_ops_yaml)</span></code> takes a yaml file for core ATen op out variants and also a yaml file for custom ops, generate C++ bindings for kernel registration. It also depends on the selective build artifact generated by <code class="docutils literal notranslate"><span class="pre">gen_selected_ops()</span></code>, see selective build doc for more information. Then <code class="docutils literal notranslate"><span class="pre">gen_operators_lib</span></code> will package those bindings to be a C++ library. As an example:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="c"># SELECT_OPS_LIST: aten::add.out,aten::mm.out</span>
<span class="nb">gen_selected_ops</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="s2">&quot;${SELECT_OPS_LIST}&quot;</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">)</span>

<span class="c"># Look for functions.yaml associated with portable libs and generate C++ bindings</span>
<span class="nb">generate_bindings_for_kernels</span><span class="p">(</span><span class="s">FUNCTIONS_YAML</span><span class="w"> </span><span class="o">${</span><span class="nv">EXECUTORCH_ROOT</span><span class="o">}</span><span class="s">/kernels/portable/functions.yaml</span><span class="p">)</span>

<span class="c"># Prepare a C++ library called &quot;generated_lib&quot; with _kernel_lib being the portable library, executorch is a dependency of it.</span>
<span class="nb">gen_operators_lib</span><span class="p">(</span><span class="s2">&quot;generated_lib&quot;</span><span class="w"> </span><span class="s">KERNEL_LIBS</span><span class="w"> </span><span class="o">${</span><span class="nv">_kernel_lib</span><span class="o">}</span><span class="w"> </span><span class="s">DEPS</span><span class="w"> </span><span class="s">executorch</span><span class="p">)</span>

<span class="c"># Link &quot;generated_lib&quot; into the application:</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">executorch_binary</span><span class="w"> </span><span class="s">generated_lib</span><span class="p">)</span>
</pre></div>
</div>
<p>We also provide the ability to merge two yaml files, given a precedence. <code class="docutils literal notranslate"><span class="pre">merge_yaml(FUNCTIONS_YAML</span> <span class="pre">functions_yaml</span> <span class="pre">FALLBACK_YAML</span> <span class="pre">fallback_yaml</span> <span class="pre">OUTPUT_DIR</span> <span class="pre">out_dir)</span></code> merges functions_yaml and fallback_yaml into a single yaml, if there’s duplicate entries in functions_yaml and fallback_yaml, this macro will always take the one in functions_yaml.</p>
<p>Example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># functions.yaml</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">op</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">add.out</span>
<span class="w">  </span><span class="nt">kernels</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">arg_meta</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">      </span><span class="nt">kernel_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torch::executor::opt_add_out</span>
</pre></div>
</div>
<p>And out fallback:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># fallback.yaml</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">op</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">add.out</span>
<span class="w">  </span><span class="nt">kernels</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">arg_meta</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">      </span><span class="nt">kernel_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torch::executor::add_out</span>
</pre></div>
</div>
<p>The merged yaml will have the entry in functions.yaml.</p>
</section>
</section>
<section id="c-api-for-custom-ops">
<h3>C++ API for Custom Ops<a class="headerlink" href="#c-api-for-custom-ops" title="Link to this heading">#</a></h3>
<p>Unlike the YAML entry API, the C++ API only uses C++ macros <code class="docutils literal notranslate"><span class="pre">EXECUTORCH_LIBRARY</span></code> and <code class="docutils literal notranslate"><span class="pre">WRAP_TO_ATEN</span></code> for kernel registration, also without selective build support. It makes this API faster in terms of development speed, since users don’t have to do YAML authoring and build system tweaking.</p>
<p>Please refer to <a class="reference internal" href="#custom-ops-api-best-practices">Custom Ops Best Practices</a> on which API to use.</p>
<p>Similar to <a class="reference external" href="https://pytorch.org/cppdocs/library.html#library_8h_1a0bd5fb09d25dfb58e750d712fc5afb84"><code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY</span></code></a> in PyTorch, <code class="docutils literal notranslate"><span class="pre">EXECUTORCH_LIBRARY</span></code> takes the operator name and the C++ function name and register them into ExecuTorch runtime.</p>
<section id="prepare-custom-kernel-implementation">
<h4>Prepare custom kernel implementation<a class="headerlink" href="#prepare-custom-kernel-implementation" title="Link to this heading">#</a></h4>
<p>Define your custom operator schema for both functional variant (used in AOT compilation) and out variant (used in ExecuTorch runtime). The schema needs to follow PyTorch ATen convention (see <code class="docutils literal notranslate"><span class="pre">native_functions.yaml</span></code>). For example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">custom_linear(Tensor weight, Tensor input, Tensor(?) bias) -&gt; Tensor</span>
<span class="l l-Scalar l-Scalar-Plain">custom_linear.out(Tensor weight, Tensor input, Tensor(?) bias, *, Tensor(a!) out) -&gt; Tensor(a!)</span>
</pre></div>
</div>
<p>Then write your custom kernel according to the schema using ExecuTorch types, along with APIs to register to ExecuTorch runtime:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// custom_linear.h/custom_linear.cpp</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;executorch/runtime/kernel/kernel_includes.h&gt;</span>
<span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">custom_linear_out</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">out</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="c1">// calculation</span>
<span class="w">   </span><span class="k">return</span><span class="w"> </span><span class="n">out</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="use-a-c-macro-to-register-it-into-executorch">
<h4>Use a C++ macro to register it into ExecuTorch<a class="headerlink" href="#use-a-c-macro-to-register-it-into-executorch" title="Link to this heading">#</a></h4>
<p>Append the following line in the example above:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// custom_linear.h/custom_linear.cpp</span>
<span class="c1">// opset namespace myop</span>
<span class="n">EXECUTORCH_LIBRARY</span><span class="p">(</span><span class="n">myop</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;custom_linear.out&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">custom_linear_out</span><span class="p">);</span>
</pre></div>
</div>
<p>Now we need to write some wrapper for this op to show up in PyTorch, but don’t worry we don’t need to rewrite the kernel. Create a separate .cpp for this purpose:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// custom_linear_pytorch.cpp</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;custom_linear.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/library.h&gt;</span>

<span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">custom_linear</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">bias</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// initialize out</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">empty</span><span class="p">({</span><span class="n">weight</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)});</span>
<span class="w">    </span><span class="c1">// wrap kernel in custom_linear.cpp into ATen kernel</span>
<span class="w">    </span><span class="n">WRAP_TO_ATEN</span><span class="p">(</span><span class="n">custom_linear_out</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)(</span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">out</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">out</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// standard API to register ops into PyTorch</span>
<span class="n">TORCH_LIBRARY</span><span class="p">(</span><span class="n">myop</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;custom_linear(Tensor weight, Tensor input, Tensor(?) bias) -&gt; Tensor&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">custom_linear</span><span class="p">);</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;custom_linear.out(Tensor weight, Tensor input, Tensor(?) bias, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">WRAP_TO_ATEN</span><span class="p">(</span><span class="n">custom_linear_out</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="compile-and-link-the-custom-kernel">
<h4>Compile and link the custom kernel<a class="headerlink" href="#compile-and-link-the-custom-kernel" title="Link to this heading">#</a></h4>
<p>Link it into ExecuTorch runtime: In our <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> that builds the binary/application, we need to add custom_linear.h/cpp into the binary target. We can build a dynamically loaded library (.so or .dylib) and link it as well.</p>
<p>Here’s an example to do it:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="c"># For executorch_target_link_options_shared_lib</span>
<span class="nb">include</span><span class="p">(</span><span class="o">${</span><span class="nv">EXECUTORCH_ROOT</span><span class="o">}</span><span class="s">/tools/cmake/Utils.cmake</span><span class="p">)</span>

<span class="c"># Add a custom op library</span>
<span class="nb">add_library</span><span class="p">(</span><span class="s">custom_op_lib</span><span class="w"> </span><span class="s">SHARED</span><span class="w"> </span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/custom_op.cpp</span><span class="p">)</span>

<span class="c"># Include the header</span>
<span class="nb">target_include_directory</span><span class="p">(</span><span class="s">custom_op_lib</span><span class="w"> </span><span class="s">PUBLIC</span><span class="w"> </span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/include</span><span class="p">)</span>

<span class="c"># Link ExecuTorch library</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">custom_op_lib</span><span class="w"> </span><span class="s">PUBLIC</span><span class="w"> </span><span class="s">executorch</span><span class="p">)</span>

<span class="c"># Define a binary target</span>
<span class="nb">add_executable</span><span class="p">(</span><span class="s">custom_op_runner</span><span class="w"> </span><span class="s">PUBLIC</span><span class="w"> </span><span class="s">main.cpp</span><span class="p">)</span>

<span class="c"># Link this library with --whole-archive !! IMPORTANT !! this is to avoid the operators being stripped by linker</span>
<span class="nb">executorch_target_link_options_shared_lib</span><span class="p">(</span><span class="s">custom_op_lib</span><span class="p">)</span>

<span class="c"># Link custom op lib</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">custom_op_runner</span><span class="w"> </span><span class="s">PUBLIC</span><span class="w"> </span><span class="s">custom_op_lib</span><span class="p">)</span>
</pre></div>
</div>
<p>Link it into the PyTorch runtime: We need to package custom_linear.h, custom_linear.cpp and custom_linear_pytorch.cpp into a dynamically loaded library (.so or .dylib) and load it into our python environment. One way of doing this is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">load_library</span><span class="p">(</span><span class="s2">&quot;libcustom_linear.so/dylib&quot;</span><span class="p">)</span>

<span class="c1"># Now we have access to the custom op, backed by kernel implemented in custom_linear.cpp.</span>
<span class="n">op</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">myop</span><span class="o">.</span><span class="n">custom_linear</span><span class="o">.</span><span class="n">default</span>
</pre></div>
</div>
</section>
<section id="using-a-custom-operator-in-a-model">
<h4>Using a Custom Operator in a Model<a class="headerlink" href="#using-a-custom-operator-in-a-model" title="Link to this heading">#</a></h4>
<p>The custom operator can explicitly used in the PyTorch model, or you can write a transformation to replace instances of a core operator with the custom variant. For this example, you could find
all instances of <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> and replace them with <code class="docutils literal notranslate"><span class="pre">CustomLinear</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w">  </span><span class="nf">replace_linear_with_custom_linear</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span>
                <span class="n">name</span><span class="p">,</span>
                <span class="n">CustomLinear</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span>  <span class="n">child</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">child</span><span class="o">.</span><span class="n">bias</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">replace_linear_with_custom_linear</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
</pre></div>
</div>
<p>The remaining steps are the same as the normal flow. Now you can run this module in eager mode as well as export to ExecuTorch.</p>
</section>
</section>
<section id="custom-ops-api-best-practices">
<h3>Custom Ops API Best Practices<a class="headerlink" href="#custom-ops-api-best-practices" title="Link to this heading">#</a></h3>
<p>Given that we have 2 kernel registration APIs for custom ops, which API should we use? Here are some pros and cons for each API:</p>
<ul class="simple">
<li><p>C++ API:</p>
<ul>
<li><p>Pros:</p>
<ul>
<li><p>Only C++ code changes are needed</p></li>
<li><p>Resembles PyTorch custom ops C++ API</p></li>
<li><p>Low maintenance cost</p></li>
</ul>
</li>
<li><p>Cons:</p>
<ul>
<li><p>No selective build support</p></li>
<li><p>No centralized bookkeepping</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Yaml entry API:</p>
<ul>
<li><p>Pros:</p>
<ul>
<li><p>Has selective build support</p></li>
<li><p>Provides a centralized place for custom ops</p>
<ul>
<li><p>It shows what ops are being registered and what kernels are bound to these ops, for an application</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Cons:</p>
<ul>
<li><p>User needs to create and maintain yaml files</p></li>
<li><p>Relatively inflexible to change the op definition</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Overall if we are building an application and it uses custom ops, during the development phase it’s recommended to use the C++ API since it’s low-cost to use and flexible to change. Once the application moves to production phase where the custom ops definitions and the build systems are quite stable and binary size is to be considered, it is recommended to use the Yaml entry API.</p>
</section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="kernel-library-overview.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Overview of ExecuTorch’s Kernel Libraries</p>
      </div>
    </a>
    <a class="right-next"
       href="kernel-library-selective-build.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Kernel Library Selective Build</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="kernel-library-overview.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Overview of ExecuTorch’s Kernel Libraries</p>
      </div>
    </a>
    <a class="right-next"
       href="kernel-library-selective-build.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Kernel Library Selective Build</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-libraries">Kernel Libraries</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-party-kernel-libraries">First-party kernel libraries:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-kernel-libraries">Custom kernel libraries:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-kernel-contract">Operator &amp; Kernel Contract</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#apis">APIs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#yaml-entry-api-high-level-architecture">YAML Entry API High Level Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#yaml-entry-api-workflow">YAML Entry API Workflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#yaml-entry-api-for-core-aten-op-out-variant">YAML Entry API for Core ATen Op Out Variant</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#yaml-entry-api-for-custom-ops">YAML Entry API for Custom Ops</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#out-variants-only">Out variants only</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#can-only-return-tensor-or">Can only return <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> or <code class="docutils literal notranslate"><span class="pre">()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-argument-types">Supported argument types</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cmake-macros">CMake Macros</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-api-for-custom-ops">C++ API for Custom Ops</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-custom-kernel-implementation">Prepare custom kernel implementation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#use-a-c-macro-to-register-it-into-executorch">Use a C++ macro to register it into ExecuTorch</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#compile-and-link-the-custom-kernel">Compile and link the custom kernel</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-a-custom-operator-in-a-model">Using a Custom Operator in a Model</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-ops-api-best-practices">Custom Ops API Best Practices</a></li>
</ul>
</li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/executorch/edit/main/docs/source/kernel-library-custom-aten-kernel.md">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/kernel-library-custom-aten-kernel.md.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, ExecuTorch.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Kernel Registration",
       "headline": "Kernel Registration",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/kernel-library-custom-aten-kernel.html",
       "articleBody": "Kernel Registration# Overview# At the last stage of ExecuTorch model exporting, we lower the operators in the dialect to the out variants of the core ATen operators. Then we serialize these operator names into the model artifact. During runtime execution, for each operator name we will need to find the actual kernels, i.e., the C++ functions that do the heavy-lifting calculations and return results. Kernel Libraries# First-party kernel libraries:# Portable kernel library is the in-house default kernel library that covers most of the core ATen operators. It\u2019s easy to use/read and is written in portable C++17. However it\u2019s not optimized for performance, because it\u2019s not specialized for any certain target. Therefore we provide kernel registration APIs for ExecuTorch users to easily register their own optimized kernels. Optimized kernel library specializes on performance for some of the operators, leveraging existing third party libraries such as EigenBLAS. This works best along with the portable kernel library, with a good balance on portability and performance. One example of combining these two libraries can be found here. Quantized kernel library implements operators for quantization and dequantization. These are out of core ATen operators but are vital to most of the production use cases. Custom kernel libraries:# Custom kernels implementing core ATen ops. Even though we don\u2019t have an internal example for custom kernels for core ATen ops, the optimized kernel library can be viewed as a good example. We have optimized add.out and a portable add.out. When user is combining these two libraries, we provide APIs to choose which kernel to use for add.out. In order to author and use custom kernels implementing core ATen ops, using the YAML based approach is recommended, because it provides full fledged support on combining kernel libraries and define fallback kernels; using selective build to minimize the kernel size. A Custom operator is any operator that an ExecuTorch user defines outside of PyTorch\u2019s native_functions.yaml. Operator \u0026 Kernel Contract# All the kernels mentioned above, whether they are in-house or customized, should comply with the following requirements: Match the calling convention derived from operator schema. The kernel registration API will generate headers for the custom kernels as references. Satisfy the dtype constraints defined in edge dialect. For tensors with certain dtypes as arguments, the result of a custom kernel needs to match the expected dtypes. The constraints are available in edge dialect ops. Give correct result. We will provide a testing framework to automatically test the custom kernels. APIs# These are the APIs available to register kernels/custom kernels/custom ops into ExecuTorch: YAML Entry API for core ATen op with custom kernels for custom ops CMake Macros C++ API for custom ops CMake Example If it\u2019s not clear which API to use, please see Best Practices. YAML Entry API High Level Architecture# ExecuTorch users are asked to provide: the custom kernel library with C++ implementations a YAML file associated with the library that describes what operators are being implemented by this library. For partial kernels, the yaml file also contains information on the dtypes and dim orders supported by the kernel. More details in the API section. YAML Entry API Workflow# At build time, the yaml files associated with kernel libraries will be passed to the kernel resolver along with the model op info (see selective build doc) and the outcome is a mapping between a combination of operator names and tensor metadata, to kernel symbols. Then codegen tools will use this mapping to generate C++ bindings that connect the kernels to ExecuTorch runtime. ExecuTorch users need to link this generated library into their application to use these kernels. At static object initialization time, kernels will be registered into the ExecuTorch kernel registry. At runtime initialization stage, ExecuTorch will use the operator name and argument metadata as a key to lookup for the kernels. For example, with \u201caten::add.out\u201d and inputs being float tensors with dim order (0, 1, 2, 3), ExecuTorch will go into the kernel registry and lookup for a kernel that matches the name and the input metadata. YAML Entry API for Core ATen Op Out Variant# Top level attributes: op (if the operator appears in native_functions.yaml) or func for custom operator. The value for this key needs to be the full operator name (including overload name) for op key, or a full operator schema (namespace, operator name, operator overload name and schema string), if we are describing a custom operator. For schema syntax please refer to this instruction. kernels: defines kernel information. It consists of arg_meta and kernel_name, which are bound together to describe \u201cfor input tensors with these metadata, use this kernel\u201d. type_alias(optional): we are giving aliases to possible dtype options. T0: [Double, Float] means T0 can be one of Double or Float. dim_order_alias(optional): similar to type_alias, we are giving names to possible dim order options. Attributes under kernels: arg_meta: a list of \u201ctensor arg name\u201d entries. The values for these keys are dtypes and dim orders aliases, that are implemented by the corresponding kernel_name. This being null means the kernel will be used for all types of input. kernel_name: the expected name of the C++ function that will implement this operator. You can put whatever you want to here, but you should follow the convention of replacing the . in the overload name with an underscore, and lowercasing all characters. In this example, add.out uses the C++ function named add_out. add.Scalar_out would become add_scalar_out, with a lowercase S. We support namespace for kernels, but note that we will be inserting a native:: to the last level of namespace. So custom::add_out in the kernel_name will point to custom::native::add_out. Some examples of operator entry: - op: add.out kernels: - arg_meta: null kernel_name: torch::executor::add_out An out variant of a core ATen operator with a default kernel ATen operator with a dtype/dim order specialized kernel (works for Double dtype and dim order needs to be (0, 1, 2, 3)) - op: add.out type_alias: T0: [Double] dim_order_alias: D0: [[0, 1, 2, 3]] kernels: - arg_meta: self: [T0, D0] other: [T0 , D0] out: [T0, D0] kernel_name: torch::executor::add_out YAML Entry API for Custom Ops# As mentioned above, this option provides more support in terms of selective build and features such as merging operator libraries. First we need to specify the operator schema as well as a kernel section. So instead of op we use func with the operator schema. As an example, here\u2019s a yaml entry for a custom op: - func: allclose.out(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False, bool dummy_param=False, *, Tensor(a!) out) -\u003e Tensor(a!) kernels: - arg_meta: null kernel_name: torch::executor::allclose_out The kernel section is the same as the one defined in core ATen ops. For operator schema, we are reusing the DSL defined in this README.md, with a few differences: Out variants only# ExecuTorch only supports out-style operators, where: The caller provides the output Tensor or Tensor list in the final position with the name out. The C++ function modifies and returns the same out argument. If the return type in the YAML file is () (which maps to void), the C++ function should still modify out but does not need to return anything. The out argument must be keyword-only, which means it needs to follow an argument named * like in the add.out example below. Conventionally, these out operators are named using the pattern \u003cname\u003e.out or \u003cname\u003e.\u003coverload\u003e_out. Since all output values are returned via an out parameter, ExecuTorch ignores the actual C++ function return value. But, to be consistent, functions should always return out when the return type is non-void. Can only return Tensor or ()# ExecuTorch only supports operators that return a single Tensor, or the unit type () (which maps to void). It does not support returning any other types, including lists, optionals, tuples, or scalars like bool. Supported argument types# ExecuTorch does not support all of the argument types that core PyTorch supports. Here\u2019s a list of the argument types we currently support: Tensor int bool float str Scalar ScalarType MemoryFormat Device Optional List List\u003cOptional\u003e Optional\u003cList\u003e CMake Macros# We provide build time macros to help users to build their kernel registration library. The macro takes the yaml file describing the kernel library as well as model operator metadata, and packages the generated C++ bindings into a C++ library. The macro is available on CMake. generate_bindings_for_kernels(FUNCTIONS_YAML functions_yaml CUSTOM_OPS_YAML custom_ops_yaml) takes a yaml file for core ATen op out variants and also a yaml file for custom ops, generate C++ bindings for kernel registration. It also depends on the selective build artifact generated by gen_selected_ops(), see selective build doc for more information. Then gen_operators_lib will package those bindings to be a C++ library. As an example: # SELECT_OPS_LIST: aten::add.out,aten::mm.out gen_selected_ops(\"\" \"${SELECT_OPS_LIST}\" \"\") # Look for functions.yaml associated with portable libs and generate C++ bindings generate_bindings_for_kernels(FUNCTIONS_YAML ${EXECUTORCH_ROOT}/kernels/portable/functions.yaml) # Prepare a C++ library called \"generated_lib\" with _kernel_lib being the portable library, executorch is a dependency of it. gen_operators_lib(\"generated_lib\" KERNEL_LIBS ${_kernel_lib} DEPS executorch) # Link \"generated_lib\" into the application: target_link_libraries(executorch_binary generated_lib) We also provide the ability to merge two yaml files, given a precedence. merge_yaml(FUNCTIONS_YAML functions_yaml FALLBACK_YAML fallback_yaml OUTPUT_DIR out_dir) merges functions_yaml and fallback_yaml into a single yaml, if there\u2019s duplicate entries in functions_yaml and fallback_yaml, this macro will always take the one in functions_yaml. Example: # functions.yaml - op: add.out kernels: - arg_meta: null kernel_name: torch::executor::opt_add_out And out fallback: # fallback.yaml - op: add.out kernels: - arg_meta: null kernel_name: torch::executor::add_out The merged yaml will have the entry in functions.yaml. C++ API for Custom Ops# Unlike the YAML entry API, the C++ API only uses C++ macros EXECUTORCH_LIBRARY and WRAP_TO_ATEN for kernel registration, also without selective build support. It makes this API faster in terms of development speed, since users don\u2019t have to do YAML authoring and build system tweaking. Please refer to Custom Ops Best Practices on which API to use. Similar to TORCH_LIBRARY in PyTorch, EXECUTORCH_LIBRARY takes the operator name and the C++ function name and register them into ExecuTorch runtime. Prepare custom kernel implementation# Define your custom operator schema for both functional variant (used in AOT compilation) and out variant (used in ExecuTorch runtime). The schema needs to follow PyTorch ATen convention (see native_functions.yaml). For example: custom_linear(Tensor weight, Tensor input, Tensor(?) bias) -\u003e Tensor custom_linear.out(Tensor weight, Tensor input, Tensor(?) bias, *, Tensor(a!) out) -\u003e Tensor(a!) Then write your custom kernel according to the schema using ExecuTorch types, along with APIs to register to ExecuTorch runtime: // custom_linear.h/custom_linear.cpp #include \u003cexecutorch/runtime/kernel/kernel_includes.h\u003e Tensor\u0026 custom_linear_out(const Tensor\u0026 weight, const Tensor\u0026 input, optional\u003cTensor\u003e bias, Tensor\u0026 out) { // calculation return out; } Use a C++ macro to register it into ExecuTorch# Append the following line in the example above: // custom_linear.h/custom_linear.cpp // opset namespace myop EXECUTORCH_LIBRARY(myop, \"custom_linear.out\", custom_linear_out); Now we need to write some wrapper for this op to show up in PyTorch, but don\u2019t worry we don\u2019t need to rewrite the kernel. Create a separate .cpp for this purpose: // custom_linear_pytorch.cpp #include \"custom_linear.h\" #include \u003ctorch/library.h\u003e at::Tensor custom_linear(const at::Tensor\u0026 weight, const at::Tensor\u0026 input, std::optional\u003cat::Tensor\u003e bias) { // initialize out at::Tensor out = at::empty({weight.size(1), input.size(1)}); // wrap kernel in custom_linear.cpp into ATen kernel WRAP_TO_ATEN(custom_linear_out, 3)(weight, input, bias, out); return out; } // standard API to register ops into PyTorch TORCH_LIBRARY(myop, m) { m.def(\"custom_linear(Tensor weight, Tensor input, Tensor(?) bias) -\u003e Tensor\", custom_linear); m.def(\"custom_linear.out(Tensor weight, Tensor input, Tensor(?) bias, *, Tensor(a!) out) -\u003e Tensor(a!)\", WRAP_TO_ATEN(custom_linear_out, 3)); } Compile and link the custom kernel# Link it into ExecuTorch runtime: In our CMakeLists.txt that builds the binary/application, we need to add custom_linear.h/cpp into the binary target. We can build a dynamically loaded library (.so or .dylib) and link it as well. Here\u2019s an example to do it: # For executorch_target_link_options_shared_lib include(${EXECUTORCH_ROOT}/tools/cmake/Utils.cmake) # Add a custom op library add_library(custom_op_lib SHARED ${CMAKE_CURRENT_SOURCE_DIR}/custom_op.cpp) # Include the header target_include_directory(custom_op_lib PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include) # Link ExecuTorch library target_link_libraries(custom_op_lib PUBLIC executorch) # Define a binary target add_executable(custom_op_runner PUBLIC main.cpp) # Link this library with --whole-archive !! IMPORTANT !! this is to avoid the operators being stripped by linker executorch_target_link_options_shared_lib(custom_op_lib) # Link custom op lib target_link_libraries(custom_op_runner PUBLIC custom_op_lib) Link it into the PyTorch runtime: We need to package custom_linear.h, custom_linear.cpp and custom_linear_pytorch.cpp into a dynamically loaded library (.so or .dylib) and load it into our python environment. One way of doing this is: import torch torch.ops.load_library(\"libcustom_linear.so/dylib\") # Now we have access to the custom op, backed by kernel implemented in custom_linear.cpp. op = torch.ops.myop.custom_linear.default Using a Custom Operator in a Model# The custom operator can explicitly used in the PyTorch model, or you can write a transformation to replace instances of a core operator with the custom variant. For this example, you could find all instances of torch.nn.Linear and replace them with CustomLinear. def replace_linear_with_custom_linear(module): for name, child in module.named_children(): if isinstance(child, nn.Linear): setattr( module, name, CustomLinear(child.in_features, child.out_features, child.bias), ) else: replace_linear_with_custom_linear(child) The remaining steps are the same as the normal flow. Now you can run this module in eager mode as well as export to ExecuTorch. Custom Ops API Best Practices# Given that we have 2 kernel registration APIs for custom ops, which API should we use? Here are some pros and cons for each API: C++ API: Pros: Only C++ code changes are needed Resembles PyTorch custom ops C++ API Low maintenance cost Cons: No selective build support No centralized bookkeepping Yaml entry API: Pros: Has selective build support Provides a centralized place for custom ops It shows what ops are being registered and what kernels are bound to these ops, for an application Cons: User needs to create and maintain yaml files Relatively inflexible to change the op definition Overall if we are building an application and it uses custom ops, during the development phase it\u2019s recommended to use the C++ API since it\u2019s low-cost to use and flexible to change. Once the application moves to production phase where the custom ops definitions and the build systems are quite stable and binary size is to be considered, it is recommended to use the Yaml entry API.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/kernel-library-custom-aten-kernel.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>