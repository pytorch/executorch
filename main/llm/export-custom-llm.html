
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Exporting custom LLMs &#8212; ExecuTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'llm/export-custom-llm';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.pytorch.org/executorch/executorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://docs.pytorch.org/executorch/llm/export-custom-llm.html" />
    <link rel="icon" href="../_static/ExecuTorch-Logo-cropped.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Running LLMs with C++" href="run-with-c-plus-plus.html" />
    <link rel="prev" title="Exporting LLMs" href="export-llm.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

<!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
<meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/executorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/et-logo.png" class="logo__image only-light" alt="ExecuTorch main documentation - Home"/>
    <script>document.write(`<img src="../_static/et-logo.png" class="logo__image only-dark" alt="ExecuTorch main documentation - Home"/>`);</script>
  
  
</a></div>
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../usage.html">
    Usage
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../examples.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../backends.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../developer-tools.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../runtime.html">
    Runtime
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../quantization.html">
    Quantization
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../kernel-library.html">
    Kernel Library
  </a>
</li>


<li class=" current active">
  <a class="nav-link dropdown-item nav-internal" href="working-with-llms.html">
    Working with LLMs
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../backend-development.html">
    Backend Development
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../ir-specification.html">
    IR Specification
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../compiler-entry-points.html">
    Compiler Entry Points
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../contributing.html">
    Contributing to ExecuTorch
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../usage.html">
    Usage
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../examples.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../backends.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../developer-tools.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../runtime.html">
    Runtime
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../quantization.html">
    Quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../kernel-library.html">
    Kernel Library
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="working-with-llms.html">
    Working with LLMs
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../backend-development.html">
    Backend Development
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ir-specification.html">
    IR Specification
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compiler-entry-points.html">
    Compiler Entry Points
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributing.html">
    Contributing to ExecuTorch
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Deploying LLMs to ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="export-llm.html">Exporting LLMs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Exporting custom LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="run-with-c-plus-plus.html">Running LLMs with C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="build-run-llama3-qualcomm-ai-engine-direct-backend.html">Building and Running Llama 3 8B Instruct with Qualcomm AI Engine Direct Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="run-on-ios.html">Running LLMs on iOS</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="working-with-llms.html" class="nav-link">Working with LLMs</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Exporting...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="working-with-llms.html">
        <meta itemprop="name" content="Working with LLMs">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Exporting custom LLMs">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="exporting-custom-llms">
<h1>Exporting custom LLMs<a class="headerlink" href="#exporting-custom-llms" title="Link to this heading">#</a></h1>
<p>If you have your own PyTorch model that is an LLM, this guide will show you how to manually export and lower to ExecuTorch, with many of the same optimizations as covered in the previous <code class="docutils literal notranslate"><span class="pre">export_llm</span></code> guide.</p>
<p>This example uses Karpathy’s <a class="reference external" href="https://github.com/karpathy/nanoGPT">nanoGPT</a>, which is a minimal implementation of
GPT-2 124M. This guide is applicable to other language models, as ExecuTorch is model-invariant.</p>
<section id="exporting-to-executorch-basic">
<h2>Exporting to ExecuTorch (basic)<a class="headerlink" href="#exporting-to-executorch-basic" title="Link to this heading">#</a></h2>
<p>Exporting takes a PyTorch model and converts it into a format that can run efficiently on consumer devices.</p>
<p>For this example, you will need the nanoGPT model and the corresponding tokenizer vocabulary.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
curl</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="o">.</span><span class="n">githubusercontent</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">karpathy</span><span class="o">/</span><span class="n">nanoGPT</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">model</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">O</span>
<span class="n">curl</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="o">.</span><span class="n">co</span><span class="o">/</span><span class="n">openai</span><span class="o">-</span><span class="n">community</span><span class="o">/</span><span class="n">gpt2</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">vocab</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">O</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
wget</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="o">.</span><span class="n">githubusercontent</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">karpathy</span><span class="o">/</span><span class="n">nanoGPT</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">model</span><span class="o">.</span><span class="n">py</span>
<span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="o">.</span><span class="n">co</span><span class="o">/</span><span class="n">openai</span><span class="o">-</span><span class="n">community</span><span class="o">/</span><span class="n">gpt2</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">vocab</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
</div>
</div>
<p>To convert the model into a format optimized for standalone execution, there are two steps. First, use the PyTorch
<code class="docutils literal notranslate"><span class="pre">export</span></code> function to convert the PyTorch model into an intermediate, platform-independent intermediate representation. Then
use the ExecuTorch <code class="docutils literal notranslate"><span class="pre">to_edge</span></code> and <code class="docutils literal notranslate"><span class="pre">to_executorch</span></code> methods to prepare the model for on-device execution. This creates a .pte
file which can be loaded by a desktop or mobile application at runtime.</p>
<p>Create a file called export_nanogpt.py with the following contents:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># export_nanogpt.py</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir</span><span class="w"> </span><span class="kn">import</span> <span class="n">EdgeCompileConfig</span><span class="p">,</span> <span class="n">to_edge</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.attention</span><span class="w"> </span><span class="kn">import</span> <span class="n">sdpa_kernel</span><span class="p">,</span> <span class="n">SDPBackend</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">export</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">model</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT</span>

<span class="c1"># Load the model.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="c1"># Create example inputs. This is used in the export process to provide</span>
<span class="c1"># hints on the expected shape of the model input.</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span> <span class="p">)</span>

<span class="c1"># Set up dynamic shape configuration. This allows the sizes of the input tensors</span>
<span class="c1"># to differ from the sizes of the tensors in `example_inputs` during runtime, as</span>
<span class="c1"># long as they adhere to the rules specified in the dynamic shape configuration.</span>
<span class="c1"># Here we set the range of 0th model input&#39;s 1st dimension as</span>
<span class="c1"># [0, model.config.block_size].</span>
<span class="c1"># See https://pytorch.org/executorch/main/concepts#dynamic-shapes</span>
<span class="c1"># for details about creating dynamic shapes.</span>
<span class="n">dynamic_shape</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">Dim</span><span class="p">(</span><span class="s2">&quot;token_dim&quot;</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span><span class="p">)},</span>
<span class="p">)</span>

<span class="c1"># Trace the model, converting it to a portable intermediate representation.</span>
<span class="c1"># The torch.no_grad() call tells PyTorch to exclude training-specific logic.</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">sdpa_kernel</span><span class="p">([</span><span class="n">SDPBackend</span><span class="o">.</span><span class="n">MATH</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">,</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shape</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">,</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shape</span><span class="p">)</span>

<span class="c1"># Convert the model into a runnable ExecuTorch program.</span>
<span class="n">edge_config</span> <span class="o">=</span> <span class="n">EdgeCompileConfig</span><span class="p">(</span><span class="n">_check_ir_validity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">edge_manager</span> <span class="o">=</span> <span class="n">to_edge</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span>  <span class="n">compile_config</span><span class="o">=</span><span class="n">edge_config</span><span class="p">)</span>
<span class="n">et_program</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">to_executorch</span><span class="p">()</span>

<span class="c1"># Save the ExecuTorch program to a file.</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;nanogpt.pte&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">et_program</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
</pre></div>
</div>
<p>To export, run the script with <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">export_nanogpt.py</span></code> (or python3, as appropriate for your environment). It will generate a <code class="docutils literal notranslate"><span class="pre">nanogpt.pte</span></code> file in the current directory.</p>
<p>For more information, see <a class="reference external" href="https://pytorch.org/executorch/main/tutorials/export-to-executorch-tutorial">Exporting to ExecuTorch</a> and
<a class="reference external" href="https://pytorch.org/docs/stable/export.html">torch.export</a>.</p>
</section>
<section id="backend-delegation">
<h2>Backend delegation<a class="headerlink" href="#backend-delegation" title="Link to this heading">#</a></h2>
<p>While ExecuTorch provides a portable, cross-platform implementation for all
operators, it also provides specialized backends for a number of different
targets. These include, but are not limited to, x86 and ARM CPU acceleration via
the XNNPACK backend, Apple acceleration via the Core ML backend and Metal
Performance Shader (MPS) backend, and GPU acceleration via the Vulkan backend.</p>
<p>Because optimizations are specific to a given backend, each pte file is specific
to the backend(s) targeted at export. To support multiple devices, such as
XNNPACK acceleration for Android and Core ML for iOS, export a separate PTE file
for each backend.</p>
<p>To delegate a model to a specific backend during export, ExecuTorch uses the
<code class="docutils literal notranslate"><span class="pre">to_edge_transform_and_lower()</span></code> function. This function takes the exported program
from <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> and a backend-specific partitioner object. The partitioner
identifies parts of the computation graph that can be optimized by the target
backend. Within <code class="docutils literal notranslate"><span class="pre">to_edge_transform_and_lower()</span></code>, the exported program is
converted to an edge dialect program. The partitioner then delegates compatible
graph sections to the backend for acceleration and optimization. Any graph parts
not delegated are executed by ExecuTorch’s default operator implementations.</p>
<p>To delegate the exported model to a specific backend, we need to import its
partitioner as well as edge compile config from ExecuTorch codebase first, then
call <code class="docutils literal notranslate"><span class="pre">to_edge_transform_and_lower</span></code>.</p>
<p>Here’s an example of how to delegate nanoGPT to XNNPACK (if you’re deploying to an Android phone for instance):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># export_nanogpt.py</span>

<span class="c1"># Load partitioner for Xnnpack backend</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.backends.xnnpack.partition.xnnpack_partitioner</span><span class="w"> </span><span class="kn">import</span> <span class="n">XnnpackPartitioner</span>

<span class="c1"># Model to be delegated to specific backend should use specific edge compile config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.backends.xnnpack.utils.configs</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_xnnpack_edge_compile_config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir</span><span class="w"> </span><span class="kn">import</span> <span class="n">EdgeCompileConfig</span><span class="p">,</span> <span class="n">to_edge_transform_and_lower</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">export</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.attention</span><span class="w"> </span><span class="kn">import</span> <span class="n">sdpa_kernel</span><span class="p">,</span> <span class="n">SDPBackend</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">export</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">model</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT</span>

<span class="c1"># Load the nanoGPT model.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="c1"># Create example inputs. This is used in the export process to provide</span>
<span class="c1"># hints on the expected shape of the model input.</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>
    <span class="p">)</span>

<span class="c1"># Set up dynamic shape configuration. This allows the sizes of the input tensors</span>
<span class="c1"># to differ from the sizes of the tensors in `example_inputs` during runtime, as</span>
<span class="c1"># long as they adhere to the rules specified in the dynamic shape configuration.</span>
<span class="c1"># Here we set the range of 0th model input&#39;s 1st dimension as</span>
<span class="c1"># [0, model.config.block_size].</span>
<span class="c1"># See https://pytorch.org/executorch/main/concepts.html#dynamic-shapes</span>
<span class="c1"># for details about creating dynamic shapes.</span>
<span class="n">dynamic_shape</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">Dim</span><span class="p">(</span><span class="s2">&quot;token_dim&quot;</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)},</span>
<span class="p">)</span>

<span class="c1"># Trace the model, converting it to a portable intermediate representation.</span>
<span class="c1"># The torch.no_grad() call tells PyTorch to exclude training-specific logic.</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">sdpa_kernel</span><span class="p">([</span><span class="n">SDPBackend</span><span class="o">.</span><span class="n">MATH</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">,</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shape</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">,</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shape</span><span class="p">)</span>

<span class="c1"># Convert the model into a runnable ExecuTorch program.</span>
<span class="c1"># To be further lowered to Xnnpack backend, `traced_model` needs xnnpack-specific edge compile config</span>
<span class="n">edge_config</span> <span class="o">=</span> <span class="n">get_xnnpack_edge_compile_config</span><span class="p">()</span>
<span class="c1"># Converted to edge program and then delegate exported model to Xnnpack backend</span>
<span class="c1"># by invoking `to` function with Xnnpack partitioner.</span>
<span class="n">edge_manager</span> <span class="o">=</span> <span class="n">to_edge_transform_and_lower</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span> <span class="n">partitioner</span> <span class="o">=</span> <span class="p">[</span><span class="n">XnnpackPartitioner</span><span class="p">()],</span> <span class="n">compile_config</span> <span class="o">=</span> <span class="n">edge_config</span><span class="p">)</span>
<span class="n">et_program</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">to_executorch</span><span class="p">()</span>

<span class="c1"># Save the Xnnpack-delegated ExecuTorch program to a file.</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;nanogpt.pte&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">et_program</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="quantization">
<h2>Quantization<a class="headerlink" href="#quantization" title="Link to this heading">#</a></h2>
<p>Quantization refers to a set of techniques for running calculations and storing tensors using lower precision types.
Compared to 32-bit floating point, using 8-bit integers can provide both a significant speedup and reduction in
memory usage. There are many approaches to quantizing a model, varying in amount of pre-processing required, data
types used, and impact on model accuracy and performance.</p>
<p>Because compute and memory are highly constrained on mobile devices, some form of quantization is necessary to ship
large models on consumer electronics. In particular, large language models, such as Llama2, may require quantizing
model weights to 4 bits or less.</p>
<p>Leveraging quantization requires transforming the model before export. PyTorch provides the pt2e (PyTorch 2 Export)
API for this purpose. This example targets CPU acceleration using the XNNPACK delegate. As such, it needs to use the
XNNPACK-specific quantizer. Targeting a different backend will require use of the corresponding quantizer.</p>
<p>To use 8-bit integer dynamic quantization with the XNNPACK delegate, call <code class="docutils literal notranslate"><span class="pre">prepare_pt2e</span></code>, calibrate the model by
running with a representative input, and then call <code class="docutils literal notranslate"><span class="pre">convert_pt2e</span></code>. This updates the computational graph to use
quantized operators where available.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># export_nanogpt.py</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.backends.transforms.duplicate_dynamic_quant_chain</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">DuplicateDynamicQuantChainPass</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.backends.xnnpack.quantizer.xnnpack_quantizer</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_symmetric_quantization_config</span><span class="p">,</span>
    <span class="n">XNNPACKQuantizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchao.quantization.pt2e.quantize_pt2e</span><span class="w"> </span><span class="kn">import</span> <span class="n">convert_pt2e</span><span class="p">,</span> <span class="n">prepare_pt2e</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use dynamic, per-channel quantization.</span>
<span class="n">xnnpack_quant_config</span> <span class="o">=</span> <span class="n">get_symmetric_quantization_config</span><span class="p">(</span>
    <span class="n">is_per_channel</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">xnnpack_quantizer</span> <span class="o">=</span> <span class="n">XNNPACKQuantizer</span><span class="p">()</span>
<span class="n">xnnpack_quantizer</span><span class="o">.</span><span class="n">set_global</span><span class="p">(</span><span class="n">xnnpack_quant_config</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>

<span class="c1"># Annotate the model for quantization. This prepares the model for calibration.</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">prepare_pt2e</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">xnnpack_quantizer</span><span class="p">)</span>

<span class="c1"># Calibrate the model using representative inputs. This allows the quantization</span>
<span class="c1"># logic to determine the expected range of values in each tensor.</span>
<span class="n">m</span><span class="p">(</span><span class="o">*</span><span class="n">example_inputs</span><span class="p">)</span>

<span class="c1"># Perform the actual quantization.</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">convert_pt2e</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">fold_quantize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">DuplicateDynamicQuantChainPass</span><span class="p">()(</span><span class="n">m</span><span class="p">)</span>

<span class="n">traced_model</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>
</pre></div>
</div>
<p>Additionally, add or update the <code class="docutils literal notranslate"><span class="pre">to_edge_transform_and_lower()</span></code> call to use <code class="docutils literal notranslate"><span class="pre">XnnpackPartitioner</span></code>. This
instructs ExecuTorch to optimize the model for CPU execution via the XNNPACK backend.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">executorch.backends.xnnpack.partition.xnnpack_partitioner</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">XnnpackPartitioner</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">edge_config</span> <span class="o">=</span> <span class="n">get_xnnpack_edge_compile_config</span><span class="p">()</span>
<span class="c1"># Convert to edge dialect and lower to XNNPack.</span>
<span class="n">edge_manager</span> <span class="o">=</span> <span class="n">to_edge_transform_and_lower</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span> <span class="n">partitioner</span> <span class="o">=</span> <span class="p">[</span><span class="n">XnnpackPartitioner</span><span class="p">()],</span> <span class="n">compile_config</span> <span class="o">=</span> <span class="n">edge_config</span><span class="p">)</span>
<span class="n">et_program</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">to_executorch</span><span class="p">()</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;nanogpt.pte&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">et_program</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, see <a class="reference internal" href="../quantization-overview.html"><span class="std std-doc">Quantization in ExecuTorch</span></a>.</p>
</section>
<section id="profiling-and-debugging">
<h2>Profiling and Debugging<a class="headerlink" href="#profiling-and-debugging" title="Link to this heading">#</a></h2>
<p>After lowering a model by calling <code class="docutils literal notranslate"><span class="pre">to_edge_transform_and_lower()</span></code>, you may want to see what got delegated and what didn’t. ExecuTorch
provides utility methods to give insight on the delegation. You can use this information to gain visibility into
the underlying computation and diagnose potential performance issues. Model authors can use this information to
structure the model in a way that is compatible with the target backend.</p>
<section id="visualizing-the-delegation">
<h3>Visualizing the Delegation<a class="headerlink" href="#visualizing-the-delegation" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">get_delegation_info()</span></code> method provides a summary of what happened to the model after the <code class="docutils literal notranslate"><span class="pre">to_edge_transform_and_lower()</span></code> call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">executorch.devtools.backend_debug</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_delegation_info</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabulate</span><span class="w"> </span><span class="kn">import</span> <span class="n">tabulate</span>

<span class="c1"># ... After call to to_edge_transform_and_lower(), but before to_executorch()</span>
<span class="n">graph_module</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">exported_program</span><span class="p">()</span><span class="o">.</span><span class="n">graph_module</span>
<span class="n">delegation_info</span> <span class="o">=</span> <span class="n">get_delegation_info</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">delegation_info</span><span class="o">.</span><span class="n">get_summary</span><span class="p">())</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">delegation_info</span><span class="o">.</span><span class="n">get_operator_delegation_dataframe</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s2">&quot;keys&quot;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">&quot;fancy_grid&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>For nanoGPT targeting the XNNPACK backend, you might see the following (note that the numbers below are for illustration purposes only and actual values may vary):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Total</span>  <span class="n">delegated</span>  <span class="n">subgraphs</span><span class="p">:</span>  <span class="mi">145</span>
<span class="n">Number</span>  <span class="n">of</span>  <span class="n">delegated</span>  <span class="n">nodes</span><span class="p">:</span>  <span class="mi">350</span>
<span class="n">Number</span>  <span class="n">of</span>  <span class="n">non</span><span class="o">-</span><span class="n">delegated</span>  <span class="n">nodes</span><span class="p">:</span>  <span class="mi">760</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>op_type</p></th>
<th class="head"><p># in_delegated_graphs</p></th>
<th class="head"><p># in_non_delegated_graphs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>aten__softmax_default</p></td>
<td><p>12</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>aten_add_tensor</p></td>
<td><p>37</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>aten_addmm_default</p></td>
<td><p>48</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>aten_any_dim</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>…</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>25</p></td>
<td><p>aten_view_copy_default</p></td>
<td><p>96</p></td>
<td><p>122</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>…</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>30</p></td>
<td><p>Total</p></td>
<td><p>350</p></td>
<td><p>760</p></td>
</tr>
</tbody>
</table>
</div>
<p>From the table, the operator <code class="docutils literal notranslate"><span class="pre">aten_view_copy_default</span></code> appears 96 times in delegate graphs and 122 times in non-delegated graphs.
To see a more detailed view, use the <code class="docutils literal notranslate"><span class="pre">format_delegated_graph()</span></code> method to get a formatted str of printout of the whole graph or use <code class="docutils literal notranslate"><span class="pre">print_delegated_graph()</span></code> to print directly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.backend.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">format_delegated_graph</span>
<span class="n">graph_module</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">exported_program</span><span class="p">()</span><span class="o">.</span><span class="n">graph_module</span>
<span class="nb">print</span><span class="p">(</span><span class="n">format_delegated_graph</span><span class="p">(</span><span class="n">graph_module</span><span class="p">))</span>
</pre></div>
</div>
<p>This may generate a large amount of output for large models. Consider using “Control+F” or “Command+F” to locate the operator you’re interested in
(e.g. “aten_view_copy_default”). Observe which instances are not under lowered graphs.</p>
<p>In the fragment of the output for nanoGPT below, observe that a transformer module has been delegated to XNNPACK while the where operator is not.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">aten_where_self_22</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">executorch</span><span class="o">.</span><span class="n">exir</span><span class="o">.</span><span class="n">dialects</span><span class="o">.</span><span class="n">edge</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">where</span><span class="o">.</span><span class="n">self</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">aten_logical_not_default_33</span><span class="p">,</span> <span class="o">%</span><span class="n">scalar_tensor_23</span><span class="p">,</span> <span class="o">%</span><span class="n">scalar_tensor_22</span><span class="p">),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
<span class="o">%</span><span class="n">lowered_module_144</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_attr</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">lowered_module_144</span><span class="p">]</span>
<span class="n">backend_id</span><span class="p">:</span> <span class="n">XnnpackBackend</span>
<span class="n">lowered</span> <span class="n">graph</span><span class="p">():</span>
    <span class="o">%</span><span class="n">p_transformer_h_0_attn_c_attn_weight</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">p_transformer_h_0_attn_c_attn_weight</span><span class="p">]</span>
    <span class="o">%</span><span class="n">p_transformer_h_0_attn_c_attn_bias</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">p_transformer_h_0_attn_c_attn_bias</span><span class="p">]</span>
    <span class="o">%</span><span class="n">getitem</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">getitem</span><span class="p">]</span>
    <span class="o">%</span><span class="n">sym_size</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">sym_size</span><span class="p">]</span>
    <span class="o">%</span><span class="n">aten_view_copy_default</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">executorch</span><span class="o">.</span><span class="n">exir</span><span class="o">.</span><span class="n">dialects</span><span class="o">.</span><span class="n">edge</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">view_copy</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">getitem</span><span class="p">,</span> <span class="p">[</span><span class="o">%</span><span class="n">sym_size</span><span class="p">,</span> <span class="mi">768</span><span class="p">]),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="o">%</span><span class="n">aten_permute_copy_default</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">executorch</span><span class="o">.</span><span class="n">exir</span><span class="o">.</span><span class="n">dialects</span><span class="o">.</span><span class="n">edge</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">permute_copy</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">p_transformer_h_0_attn_c_attn_weight</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="o">%</span><span class="n">aten_addmm_default</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">executorch</span><span class="o">.</span><span class="n">exir</span><span class="o">.</span><span class="n">dialects</span><span class="o">.</span><span class="n">edge</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">addmm</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">p_transformer_h_0_attn_c_attn_bias</span><span class="p">,</span> <span class="o">%</span><span class="n">aten_view_copy_default</span><span class="p">,</span> <span class="o">%</span><span class="n">aten_permute_copy_default</span><span class="p">),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="o">%</span><span class="n">aten_view_copy_default_1</span> <span class="p">:</span> <span class="p">[</span><span class="n">num_users</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">executorch</span><span class="o">.</span><span class="n">exir</span><span class="o">.</span><span class="n">dialects</span><span class="o">.</span><span class="n">edge</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">view_copy</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">aten_addmm_default</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">%</span><span class="n">sym_size</span><span class="p">,</span> <span class="mi">2304</span><span class="p">]),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">aten_view_copy_default_1</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="further-model-analysis-and-debugging">
<h3>Further Model Analysis and Debugging<a class="headerlink" href="#further-model-analysis-and-debugging" title="Link to this heading">#</a></h3>
<p>Through the <a class="reference internal" href="getting-started.html#performance-analysis"><span class="std std-ref">ExecuTorch’s Developer Tools</span></a>, users are able to profile model execution, giving timing information for each operator in the model, doing model numeric debugging, etc.</p>
<p>An ETRecord is an artifact generated at the time of export that contains model graphs and source-level metadata linking the ExecuTorch program to the original PyTorch model. You can view all profiling events without an ETRecord, though with an ETRecord, you will also be able to link each event to the types of operators being executed, module hierarchy, and stack traces of the original PyTorch source code. For more information, see <a class="reference internal" href="../etrecord.html"><span class="std std-doc">the ETRecord docs</span></a>.</p>
<p>In your export script, after calling <code class="docutils literal notranslate"><span class="pre">to_edge()</span></code> and <code class="docutils literal notranslate"><span class="pre">to_executorch()</span></code>, call <code class="docutils literal notranslate"><span class="pre">generate_etrecord()</span></code> with the <code class="docutils literal notranslate"><span class="pre">EdgeProgramManager</span></code> from <code class="docutils literal notranslate"><span class="pre">to_edge()</span></code> and the <code class="docutils literal notranslate"><span class="pre">ExecuTorchProgramManager</span></code> from <code class="docutils literal notranslate"><span class="pre">to_executorch()</span></code>. Make sure to copy the <code class="docutils literal notranslate"><span class="pre">EdgeProgramManager</span></code>, as the call to <code class="docutils literal notranslate"><span class="pre">to_edge_transform_and_lower()</span></code> mutates the graph in-place.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># export_nanogpt.py</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.devtools</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate_etrecord</span>

<span class="c1"># Make the deep copy immediately after to to_edge()</span>
<span class="n">edge_manager_copy</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">edge_manager</span><span class="p">)</span>

<span class="c1"># ...</span>
<span class="c1"># Generate ETRecord right after to_executorch()</span>
<span class="n">etrecord_path</span> <span class="o">=</span> <span class="s2">&quot;etrecord.bin&quot;</span>
<span class="n">generate_etrecord</span><span class="p">(</span><span class="n">etrecord_path</span><span class="p">,</span> <span class="n">edge_manager_copy</span><span class="p">,</span> <span class="n">et_program</span><span class="p">)</span>
</pre></div>
</div>
<p>Run the export script and the ETRecord will be generated as <code class="docutils literal notranslate"><span class="pre">etrecord.bin</span></code>.</p>
<p>To learn more about ExecuTorch’s Developer Tools, see the <a class="reference internal" href="../devtools-overview.html"><span class="std std-doc">Introduction to the ExecuTorch Developer Tools</span></a>.</p>
</section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="export-llm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exporting LLMs</p>
      </div>
    </a>
    <a class="right-next"
       href="run-with-c-plus-plus.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Running LLMs with C++</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2024, ExecuTorch.
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="export-llm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exporting LLMs</p>
      </div>
    </a>
    <a class="right-next"
       href="run-with-c-plus-plus.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Running LLMs with C++</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exporting-to-executorch-basic">Exporting to ExecuTorch (basic)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backend-delegation">Backend delegation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization">Quantization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#profiling-and-debugging">Profiling and Debugging</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-delegation">Visualizing the Delegation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-model-analysis-and-debugging">Further Model Analysis and Debugging</a></li>
</ul>
</li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/executorch/edit/main/docs/source/llm/export-custom-llm.md">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/llm/export-custom-llm.md.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    
    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>
    

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, ExecuTorch.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Exporting custom LLMs",
       "headline": "Exporting custom LLMs",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/llm/export-custom-llm.html",
       "articleBody": "Exporting custom LLMs# If you have your own PyTorch model that is an LLM, this guide will show you how to manually export and lower to ExecuTorch, with many of the same optimizations as covered in the previous export_llm guide. This example uses Karpathy\u2019s nanoGPT, which is a minimal implementation of GPT-2 124M. This guide is applicable to other language models, as ExecuTorch is model-invariant. Exporting to ExecuTorch (basic)# Exporting takes a PyTorch model and converts it into a format that can run efficiently on consumer devices. For this example, you will need the nanoGPT model and the corresponding tokenizer vocabulary. curl curl https://raw.githubusercontent.com/karpathy/nanoGPT/master/model.py -O curl https://huggingface.co/openai-community/gpt2/resolve/main/vocab.json -O wget wget https://raw.githubusercontent.com/karpathy/nanoGPT/master/model.py wget https://huggingface.co/openai-community/gpt2/resolve/main/vocab.json To convert the model into a format optimized for standalone execution, there are two steps. First, use the PyTorch export function to convert the PyTorch model into an intermediate, platform-independent intermediate representation. Then use the ExecuTorch to_edge and to_executorch methods to prepare the model for on-device execution. This creates a .pte file which can be loaded by a desktop or mobile application at runtime. Create a file called export_nanogpt.py with the following contents: # export_nanogpt.py import torch from executorch.exir import EdgeCompileConfig, to_edge from torch.nn.attention import sdpa_kernel, SDPBackend from torch.export import export from model import GPT # Load the model. model = GPT.from_pretrained(\u0027gpt2\u0027) # Create example inputs. This is used in the export process to provide # hints on the expected shape of the model input. example_inputs = (torch.randint(0, 100, (1, model.config.block_size), dtype=torch.long), ) # Set up dynamic shape configuration. This allows the sizes of the input tensors # to differ from the sizes of the tensors in `example_inputs` during runtime, as # long as they adhere to the rules specified in the dynamic shape configuration. # Here we set the range of 0th model input\u0027s 1st dimension as # [0, model.config.block_size]. # See https://pytorch.org/executorch/main/concepts#dynamic-shapes # for details about creating dynamic shapes. dynamic_shape = ( {1: torch.export.Dim(\"token_dim\", max=model.config.block_size)}, ) # Trace the model, converting it to a portable intermediate representation. # The torch.no_grad() call tells PyTorch to exclude training-specific logic. with torch.nn.attention.sdpa_kernel([SDPBackend.MATH]), torch.no_grad(): m = export(model, example_inputs, dynamic_shapes=dynamic_shape).module() traced_model = export(m, example_inputs, dynamic_shapes=dynamic_shape) # Convert the model into a runnable ExecuTorch program. edge_config = EdgeCompileConfig(_check_ir_validity=False) edge_manager = to_edge(traced_model, compile_config=edge_config) et_program = edge_manager.to_executorch() # Save the ExecuTorch program to a file. with open(\"nanogpt.pte\", \"wb\") as file: file.write(et_program.buffer) To export, run the script with python export_nanogpt.py (or python3, as appropriate for your environment). It will generate a nanogpt.pte file in the current directory. For more information, see Exporting to ExecuTorch and torch.export. Backend delegation# While ExecuTorch provides a portable, cross-platform implementation for all operators, it also provides specialized backends for a number of different targets. These include, but are not limited to, x86 and ARM CPU acceleration via the XNNPACK backend, Apple acceleration via the Core ML backend and Metal Performance Shader (MPS) backend, and GPU acceleration via the Vulkan backend. Because optimizations are specific to a given backend, each pte file is specific to the backend(s) targeted at export. To support multiple devices, such as XNNPACK acceleration for Android and Core ML for iOS, export a separate PTE file for each backend. To delegate a model to a specific backend during export, ExecuTorch uses the to_edge_transform_and_lower() function. This function takes the exported program from torch.export and a backend-specific partitioner object. The partitioner identifies parts of the computation graph that can be optimized by the target backend. Within to_edge_transform_and_lower(), the exported program is converted to an edge dialect program. The partitioner then delegates compatible graph sections to the backend for acceleration and optimization. Any graph parts not delegated are executed by ExecuTorch\u2019s default operator implementations. To delegate the exported model to a specific backend, we need to import its partitioner as well as edge compile config from ExecuTorch codebase first, then call to_edge_transform_and_lower. Here\u2019s an example of how to delegate nanoGPT to XNNPACK (if you\u2019re deploying to an Android phone for instance): # export_nanogpt.py # Load partitioner for Xnnpack backend from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner # Model to be delegated to specific backend should use specific edge compile config from executorch.backends.xnnpack.utils.configs import get_xnnpack_edge_compile_config from executorch.exir import EdgeCompileConfig, to_edge_transform_and_lower import torch from torch.export import export from torch.nn.attention import sdpa_kernel, SDPBackend from torch.export import export from model import GPT # Load the nanoGPT model. model = GPT.from_pretrained(\u0027gpt2\u0027) # Create example inputs. This is used in the export process to provide # hints on the expected shape of the model input. example_inputs = ( torch.randint(0, 100, (1, model.config.block_size - 1), dtype=torch.long), ) # Set up dynamic shape configuration. This allows the sizes of the input tensors # to differ from the sizes of the tensors in `example_inputs` during runtime, as # long as they adhere to the rules specified in the dynamic shape configuration. # Here we set the range of 0th model input\u0027s 1st dimension as # [0, model.config.block_size]. # See https://pytorch.org/executorch/main/concepts.html#dynamic-shapes # for details about creating dynamic shapes. dynamic_shape = ( {1: torch.export.Dim(\"token_dim\", max=model.config.block_size - 1)}, ) # Trace the model, converting it to a portable intermediate representation. # The torch.no_grad() call tells PyTorch to exclude training-specific logic. with torch.nn.attention.sdpa_kernel([SDPBackend.MATH]), torch.no_grad(): m = export(model, example_inputs, dynamic_shapes=dynamic_shape).module() traced_model = export(m, example_inputs, dynamic_shapes=dynamic_shape) # Convert the model into a runnable ExecuTorch program. # To be further lowered to Xnnpack backend, `traced_model` needs xnnpack-specific edge compile config edge_config = get_xnnpack_edge_compile_config() # Converted to edge program and then delegate exported model to Xnnpack backend # by invoking `to` function with Xnnpack partitioner. edge_manager = to_edge_transform_and_lower(traced_model, partitioner = [XnnpackPartitioner()], compile_config = edge_config) et_program = edge_manager.to_executorch() # Save the Xnnpack-delegated ExecuTorch program to a file. with open(\"nanogpt.pte\", \"wb\") as file: file.write(et_program.buffer) Quantization# Quantization refers to a set of techniques for running calculations and storing tensors using lower precision types. Compared to 32-bit floating point, using 8-bit integers can provide both a significant speedup and reduction in memory usage. There are many approaches to quantizing a model, varying in amount of pre-processing required, data types used, and impact on model accuracy and performance. Because compute and memory are highly constrained on mobile devices, some form of quantization is necessary to ship large models on consumer electronics. In particular, large language models, such as Llama2, may require quantizing model weights to 4 bits or less. Leveraging quantization requires transforming the model before export. PyTorch provides the pt2e (PyTorch 2 Export) API for this purpose. This example targets CPU acceleration using the XNNPACK delegate. As such, it needs to use the XNNPACK-specific quantizer. Targeting a different backend will require use of the corresponding quantizer. To use 8-bit integer dynamic quantization with the XNNPACK delegate, call prepare_pt2e, calibrate the model by running with a representative input, and then call convert_pt2e. This updates the computational graph to use quantized operators where available. # export_nanogpt.py from executorch.backends.transforms.duplicate_dynamic_quant_chain import ( DuplicateDynamicQuantChainPass, ) from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import ( get_symmetric_quantization_config, XNNPACKQuantizer, ) from torchao.quantization.pt2e.quantize_pt2e import convert_pt2e, prepare_pt2e # Use dynamic, per-channel quantization. xnnpack_quant_config = get_symmetric_quantization_config( is_per_channel=True, is_dynamic=True ) xnnpack_quantizer = XNNPACKQuantizer() xnnpack_quantizer.set_global(xnnpack_quant_config) m = export(model, example_inputs).module() # Annotate the model for quantization. This prepares the model for calibration. m = prepare_pt2e(m, xnnpack_quantizer) # Calibrate the model using representative inputs. This allows the quantization # logic to determine the expected range of values in each tensor. m(*example_inputs) # Perform the actual quantization. m = convert_pt2e(m, fold_quantize=False) DuplicateDynamicQuantChainPass()(m) traced_model = export(m, example_inputs) Additionally, add or update the to_edge_transform_and_lower() call to use XnnpackPartitioner. This instructs ExecuTorch to optimize the model for CPU execution via the XNNPACK backend. from executorch.backends.xnnpack.partition.xnnpack_partitioner import ( XnnpackPartitioner, ) edge_config = get_xnnpack_edge_compile_config() # Convert to edge dialect and lower to XNNPack. edge_manager = to_edge_transform_and_lower(traced_model, partitioner = [XnnpackPartitioner()], compile_config = edge_config) et_program = edge_manager.to_executorch() with open(\"nanogpt.pte\", \"wb\") as file: file.write(et_program.buffer) For more information, see Quantization in ExecuTorch. Profiling and Debugging# After lowering a model by calling to_edge_transform_and_lower(), you may want to see what got delegated and what didn\u2019t. ExecuTorch provides utility methods to give insight on the delegation. You can use this information to gain visibility into the underlying computation and diagnose potential performance issues. Model authors can use this information to structure the model in a way that is compatible with the target backend. Visualizing the Delegation# The get_delegation_info() method provides a summary of what happened to the model after the to_edge_transform_and_lower() call: from executorch.devtools.backend_debug import get_delegation_info from tabulate import tabulate # ... After call to to_edge_transform_and_lower(), but before to_executorch() graph_module = edge_manager.exported_program().graph_module delegation_info = get_delegation_info(graph_module) print(delegation_info.get_summary()) df = delegation_info.get_operator_delegation_dataframe() print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\")) For nanoGPT targeting the XNNPACK backend, you might see the following (note that the numbers below are for illustration purposes only and actual values may vary): Total delegated subgraphs: 145 Number of delegated nodes: 350 Number of non-delegated nodes: 760 op_type # in_delegated_graphs # in_non_delegated_graphs 0 aten__softmax_default 12 0 1 aten_add_tensor 37 0 2 aten_addmm_default 48 0 3 aten_any_dim 0 12 \u2026 25 aten_view_copy_default 96 122 \u2026 30 Total 350 760 From the table, the operator aten_view_copy_default appears 96 times in delegate graphs and 122 times in non-delegated graphs. To see a more detailed view, use the format_delegated_graph() method to get a formatted str of printout of the whole graph or use print_delegated_graph() to print directly: from executorch.exir.backend.utils import format_delegated_graph graph_module = edge_manager.exported_program().graph_module print(format_delegated_graph(graph_module)) This may generate a large amount of output for large models. Consider using \u201cControl+F\u201d or \u201cCommand+F\u201d to locate the operator you\u2019re interested in (e.g. \u201caten_view_copy_default\u201d). Observe which instances are not under lowered graphs. In the fragment of the output for nanoGPT below, observe that a transformer module has been delegated to XNNPACK while the where operator is not. %aten_where_self_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.where.self](args = (%aten_logical_not_default_33, %scalar_tensor_23, %scalar_tensor_22), kwargs = {}) %lowered_module_144 : [num_users=1] = get_attr[target=lowered_module_144] backend_id: XnnpackBackend lowered graph(): %p_transformer_h_0_attn_c_attn_weight : [num_users=1] = placeholder[target=p_transformer_h_0_attn_c_attn_weight] %p_transformer_h_0_attn_c_attn_bias : [num_users=1] = placeholder[target=p_transformer_h_0_attn_c_attn_bias] %getitem : [num_users=1] = placeholder[target=getitem] %sym_size : [num_users=2] = placeholder[target=sym_size] %aten_view_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%getitem, [%sym_size, 768]), kwargs = {}) %aten_permute_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%p_transformer_h_0_attn_c_attn_weight, [1, 0]), kwargs = {}) %aten_addmm_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.addmm.default](args = (%p_transformer_h_0_attn_c_attn_bias, %aten_view_copy_default, %aten_permute_copy_default), kwargs = {}) %aten_view_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_addmm_default, [1, %sym_size, 2304]), kwargs = {}) return [aten_view_copy_default_1] Further Model Analysis and Debugging# Through the ExecuTorch\u2019s Developer Tools, users are able to profile model execution, giving timing information for each operator in the model, doing model numeric debugging, etc. An ETRecord is an artifact generated at the time of export that contains model graphs and source-level metadata linking the ExecuTorch program to the original PyTorch model. You can view all profiling events without an ETRecord, though with an ETRecord, you will also be able to link each event to the types of operators being executed, module hierarchy, and stack traces of the original PyTorch source code. For more information, see the ETRecord docs. In your export script, after calling to_edge() and to_executorch(), call generate_etrecord() with the EdgeProgramManager from to_edge() and the ExecuTorchProgramManager from to_executorch(). Make sure to copy the EdgeProgramManager, as the call to to_edge_transform_and_lower() mutates the graph in-place. # export_nanogpt.py import copy from executorch.devtools import generate_etrecord # Make the deep copy immediately after to to_edge() edge_manager_copy = copy.deepcopy(edge_manager) # ... # Generate ETRecord right after to_executorch() etrecord_path = \"etrecord.bin\" generate_etrecord(etrecord_path, edge_manager_copy, et_program) Run the export script and the ETRecord will be generated as etrecord.bin. To learn more about ExecuTorch\u2019s Developer Tools, see the Introduction to the ExecuTorch Developer Tools.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/llm/export-custom-llm.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>