
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Custom Compiler Passes and Partitioners &#8212; ExecuTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=a8da1a53"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'compiler-custom-compiler-passes';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.pytorch.org/executorch/executorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://docs.pytorch.org/executorch/compiler-custom-compiler-passes.html" />
    <link rel="icon" href="_static/ExecuTorch-Logo-cropped.svg"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Memory Planning" href="compiler-memory-planning.html" />
    <link rel="prev" title="Backend Dialect" href="compiler-backend-dialect.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

<!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
<meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/executorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/et-logo.png" class="logo__image only-light" alt="ExecuTorch main documentation - Home"/>
    <script>document.write(`<img src="_static/et-logo.png" class="logo__image only-dark" alt="ExecuTorch main documentation - Home"/>`);</script>
  
  
</a></div>
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="usage.html">
    Usage
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="examples.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="backends.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="developer-tools.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="runtime.html">
    Runtime
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="quantization.html">
    Quantization
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="kernel-library.html">
    Kernel Library
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="llm/working-with-llms.html">
    Working with LLMs
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="backend-development.html">
    Backend Development
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="ir-specification.html">
    IR Specification
  </a>
</li>


<li class=" current active">
  <a class="nav-link dropdown-item nav-internal" href="compiler-entry-points.html">
    Compiler Entry Points
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="contributing.html">
    Contributing to ExecuTorch
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="usage.html">
    Usage
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="examples.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="backends.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="developer-tools.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="runtime.html">
    Runtime
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="quantization.html">
    Quantization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="kernel-library.html">
    Kernel Library
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="llm/working-with-llms.html">
    Working with LLMs
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="backend-development.html">
    Backend Development
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="ir-specification.html">
    IR Specification
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="compiler-entry-points.html">
    Compiler Entry Points
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="contributing.html">
    Contributing to ExecuTorch
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="compiler-backend-dialect.html">Backend Dialect</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Custom Compiler Passes and Partitioners</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiler-memory-planning.html">Memory Planning</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="compiler-entry-points.html" class="nav-link">Compiler Entry Points</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Custom...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="compiler-entry-points.html">
        <meta itemprop="name" content="Compiler Entry Points">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Custom Compiler Passes and Partitioners">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="custom-compiler-passes-and-partitioners">
<h1>Custom Compiler Passes and Partitioners<a class="headerlink" href="#custom-compiler-passes-and-partitioners" title="Link to this heading">#</a></h1>
<section id="passes">
<h2>Passes<a class="headerlink" href="#passes" title="Link to this heading">#</a></h2>
<p>Passes can be roughly categorized into a couple of axes:</p>
<p>Axis A:</p>
<ol class="arabic simple">
<li><p>Creating one-to-X mapping (for example, decomposition)</p></li>
<li><p>Creating many-to-one mapping (for example, fusion)</p></li>
</ol>
<p>Axis B:</p>
<ol class="arabic simple">
<li><p>Performing forwards iteration (for example, shape propagation)</p></li>
<li><p>Performing backwards iteration (for example, dead code elimination)</p></li>
</ol>
<p>Axis C:</p>
<ol class="arabic simple">
<li><p>Dependent on local node information (eg. out-variant conversion)</p></li>
<li><p>Dependent on global graph information (eg. memory planning)</p></li>
</ol>
<p>Our projection on the frequency of these use cases are:</p>
<ol class="arabic simple">
<li><p>A.1, B.1, C.1</p></li>
<li><p>A.2</p></li>
<li><p>B.2, C.2</p></li>
</ol>
<section id="level-1">
<h3>Level 1<a class="headerlink" href="#level-1" title="Link to this heading">#</a></h3>
<p>For level 1 uses cases (creating one-to-X mappings, performing forwards iterations,
and looking at local node information), we can utilize a helper class called
<a class="reference external" href="https://github.com/pytorch/executorch/blob/d9eef24bb720804aa7b400b05241487510ae0dc2/exir/pass_base.py#L44"><code class="docutils literal notranslate"><span class="pre">ExportPass</span></code></a>.
This is an
<a class="reference external" href="https://pytorch.org/docs/stable/fx.html#the-interpreter-pattern">interpreter-based</a>
way where we execute each node and recreate the graph except with
transformations specified. This allows us to preserve the IR Spec by ensuring
that all nodes created while in the pass meet the IR Spec including ensuring that
metadata such as stack trace, FakeTensor values, and torch.nn.Module hierarchy
are preserved and updated depending on the transformations made.</p>
<p>To implement this pass, we can create a subclass of
<a class="reference external" href="https://github.com/pytorch/executorch/blob/d9eef24bb720804aa7b400b05241487510ae0dc2/exir/pass_base.py#L44"><code class="docutils literal notranslate"><span class="pre">ExportPass</span></code></a>
and implement the exposed functions.  When called with a graph module, it will
run the graph module and create a new graph containing the changes specified by
the pass. This means that the graph module passed in must be runnable on CPU,
and this invariant will be maintained after the pass is run.</p>
<section id="one-to-one-pass">
<h4>One-to-One Pass<a class="headerlink" href="#one-to-one-pass" title="Link to this heading">#</a></h4>
<p>An example for one-to-one mappings, if we wanted to replace an op A with another op B,
we can run the given
<code class="docutils literal notranslate"><span class="pre">fx.GraphModule</span></code>, and every time we see op A, return op B.</p>
<p>Consider the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ReplaceInPlaceReluWithOutOfPlaceReluPass</span><span class="p">(</span><span class="n">ExportPass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    relu_ is the in-place version. Replace it with relu, which is the</span>
<span class="sd">    out-of-place version</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">call_operator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">meta</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">op</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">relu_</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_operator</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">meta</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_operator</span><span class="p">(</span><span class="n">Op</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">default</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">meta</span><span class="p">)</span>

<span class="c1"># To create a pass</span>
<span class="n">replace_pass</span> <span class="o">=</span> <span class="n">ReplaceInPlaceReluWithOutOfPlaceReluPass</span><span class="p">()</span>
<span class="c1"># To run a pass</span>
<span class="n">new_graph_module</span> <span class="o">=</span> <span class="n">replace_pass</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span><span class="o">.</span><span class="n">graph_module</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">super().call_operator(op,</span> <span class="pre">args,</span> <span class="pre">kwargs,</span> <span class="pre">meta)</span></code> call creates a
<code class="docutils literal notranslate"><span class="pre">call_function</span></code> FX node, and returns the result of running the operator with the
given arguments.</p>
</section>
<section id="one-to-x-pass">
<h4>One-to-X Pass<a class="headerlink" href="#one-to-x-pass" title="Link to this heading">#</a></h4>
<p>If we wanted to do one-to-X mappings, like replacing op A with 2 other ops B and
C, we would then make 2 calls to <code class="docutils literal notranslate"><span class="pre">super().call_operator</span></code> to create 2 FX nodes,
one with op B and another with op C, and return the result of running op C.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ReplaceAddWithMulSub</span><span class="p">(</span><span class="n">ExportPass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Original:</span>
<span class="sd">        def f(x, y):</span>
<span class="sd">            return x + y</span>

<span class="sd">    After pass:</span>
<span class="sd">        def f(x, y):</span>
<span class="sd">            z = x * y</span>
<span class="sd">            return z - y</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">call_operator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">meta</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">op</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_operator</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">meta</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">args</span>

        <span class="n">mul_res</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_operator</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
            <span class="n">args</span><span class="p">,</span>
            <span class="p">{},</span>
            <span class="n">meta</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_operator</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sub</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
            <span class="p">(</span><span class="n">mul_res</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
            <span class="p">{},</span>
            <span class="n">meta</span>
        <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="one-to-none-pass">
<h4>One-to-None Pass<a class="headerlink" href="#one-to-none-pass" title="Link to this heading">#</a></h4>
<p>If we wanted to remove an op, we can just return the value passed into the
function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">RemoveDetachPass</span><span class="p">(</span><span class="n">ExportPass</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">call_operator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">meta</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">detach_copy</span><span class="o">.</span><span class="n">default</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_operator</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">meta</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="utilizing-local-information">
<h4>Utilizing Local Information<a class="headerlink" href="#utilizing-local-information" title="Link to this heading">#</a></h4>
<p>An example of utilizing local node information is, if we wanted to convert all the
scalars within the graph to tensors, we
can run the given <code class="docutils literal notranslate"><span class="pre">fx.GraphModule</span></code>, and for every argument that contains a scalar,
we convert it to a tensor. It might look something like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">args_map</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kwargs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># Update the argument based on the function passed</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">schema</span><span class="p">):</span>
        <span class="n">args</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">schema</span><span class="p">)</span>

    <span class="c1"># Update each argument in the schema</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">schema</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">arguments</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">schema</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">update</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">schema</span><span class="o">.</span><span class="n">kwarg_only</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="n">update</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ScalarToTensorPass</span><span class="p">(</span><span class="n">ExportPass</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">call_operator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">try_coerce</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">arg</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">))</span>
                <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">type</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">TensorType</span>
                <span class="k">else</span> <span class="n">value</span>
            <span class="p">)</span>

        <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">args_map</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">try_coerce</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_operator</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="level-2">
<h3>Level 2<a class="headerlink" href="#level-2" title="Link to this heading">#</a></h3>
<p>For creating many-to-one mappings, we can utilize FX’s <a class="reference external" href="https://github.com/pytorch/pytorch/blob/8597d37536ef11bdf6b0a539ab79af876e1c92f6/torch/fx/subgraph_rewriter.py#L77">subgraph
rewriter</a>.
Given a <code class="docutils literal notranslate"><span class="pre">pattern</span></code>, it creates a subgraph of operators matching to the pattern,
and then replaces each matched subgraph with the <code class="docutils literal notranslate"><span class="pre">replacement</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>This is an inplace operation.
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">pattern</span></code> and <code class="docutils literal notranslate"><span class="pre">replacement</span></code> inputs must be callable functions written with
the same ops that are used in the EXIR graph you are matching with (ATen ops)
so that the subgraph rewriter can find the correct pattern in the graph. Inputs
to the pattern/replacement callables will be treated as wildcards.</p>
<p>Consider the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.fx</span><span class="w"> </span><span class="kn">import</span> <span class="n">subgraph_rewriter</span>

<span class="k">def</span><span class="w"> </span><span class="nf">replace_patterns</span><span class="p">(</span><span class="n">graph_module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">pattern</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">replacement</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sub</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">replaced_patterns</span> <span class="o">=</span> <span class="n">subgraph_rewriter</span><span class="o">.</span><span class="n">replace_pattern_with_filters</span><span class="p">(</span>
    <span class="n">traced_module</span><span class="p">,</span> <span class="n">pattern</span><span class="p">,</span> <span class="n">replacement</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The subgraph rewriter returns a list of <code class="docutils literal notranslate"><span class="pre">ReplacedPatterns</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ReplacedPatterns</span><span class="p">:</span>
    <span class="c1"># Node from which the match was found</span>
    <span class="n">anchor</span><span class="p">:</span> <span class="n">Node</span>
    <span class="c1"># Maps nodes in the pattern subgraph to nodes in the larger graph</span>
    <span class="n">nodes_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Node</span><span class="p">,</span> <span class="n">Node</span><span class="p">]</span>
    <span class="c1"># List of nodes that were added into the graph</span>
    <span class="n">replacements</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Node</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The nodes created by the subgraph rewriter will not have the metadata that
is normally in EXIR nodes (`stack_trace`, `val`, `nn_module_stack`).
</pre></div>
</div>
</div>
</section>
<section id="level-3">
<h3>Level 3<a class="headerlink" href="#level-3" title="Link to this heading">#</a></h3>
<p>For the third way of creating a pass, we can utilize the most basic
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/8597d37536ef11bdf6b0a539ab79af876e1c92f6/torch/fx/passes/infra/pass_base.py#L22"><code class="docutils literal notranslate"><span class="pre">PassBase</span></code></a>.
To create a pass, we can subclass this and implement the function <code class="docutils literal notranslate"><span class="pre">call</span></code> with
the pass contents. Additionally, we can implement the functions <code class="docutils literal notranslate"><span class="pre">requires</span></code> and
<code class="docutils literal notranslate"><span class="pre">ensures</span></code> which will be called before and after the function <code class="docutils literal notranslate"><span class="pre">call</span></code>. Note that
these functions can also be overridden in <code class="docutils literal notranslate"><span class="pre">ExportPass</span></code>. To run a pass on a graph
module, we can pass the graph module directly to an instance of the class.</p>
<p>Consider the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ReplaceAddPass</span><span class="p">(</span><span class="n">PassBase</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replace_op</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replace_op</span> <span class="o">=</span> <span class="n">replace_op</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_module</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">:</span>
                <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replace_op</span>

    <span class="c1"># Optional to implement, will be called before call()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">requires</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">:</span>
                <span class="k">return</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No torch.add ops!&quot;</span><span class="p">)</span>

    <span class="c1"># Optional to implement, will be called after call()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">ensures</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

<span class="c1"># To create a pass</span>
<span class="n">replace_add_with_div</span> <span class="o">=</span> <span class="n">ReplaceAddPass</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">)</span>
<span class="c1"># To run a pass</span>
<span class="n">replace_add_with_div</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="pass-manager">
<h2>Pass Manager<a class="headerlink" href="#pass-manager" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">PassManager</span></code> is a class used to run multiple passes on a given graph
module. When initializing a <code class="docutils literal notranslate"><span class="pre">PassManager</span></code> instance, we pass in a list of passes
that we want to run and set a couple of flags. To run the collection of passes
on a graph module, we can pass the graph module directly to the <code class="docutils literal notranslate"><span class="pre">PassManager</span></code>
instance.</p>
<p>An example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.pass_manager</span><span class="w"> </span><span class="kn">import</span> <span class="n">PassManager</span>

<span class="n">pm</span> <span class="o">=</span> <span class="n">PassManager</span><span class="p">(</span>
    <span class="n">passes</span><span class="o">=</span><span class="p">[</span><span class="n">replace_add_with_div</span><span class="p">,</span> <span class="n">replace_div_with_mul</span><span class="p">],</span>
    <span class="n">run_checks_after_each_pass</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">suppress_check_failures</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">graph_module_out</span> <span class="o">=</span> <span class="n">pm</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>
</pre></div>
</div>
<p>To add a common set of checks that are run after each pass, we can call the
function <code class="docutils literal notranslate"><span class="pre">set_checks(check:</span> <span class="pre">Callable)</span></code> which takes in a callable function as
input. If the <code class="docutils literal notranslate"><span class="pre">run_checks_after_each_pass</span></code> flag is set, the <code class="docutils literal notranslate"><span class="pre">check</span></code> will be
called after each pass is run on the graph module.</p>
<p>An example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pm</span> <span class="o">=</span> <span class="n">PassManager</span><span class="p">(</span><span class="n">passes</span><span class="o">=</span><span class="p">[</span><span class="n">replace_add_with_div</span><span class="p">,</span> <span class="n">replace_div_with_mul</span><span class="p">])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">check_div_target</span><span class="p">(</span><span class="n">graph_module</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target should be div!&quot;</span><span class="p">)</span>

<span class="n">pm</span><span class="o">.</span><span class="n">add_checks</span><span class="p">(</span><span class="n">check_div_target</span><span class="p">)</span>

<span class="n">pm</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>    <span class="c1"># raises ValueError after replace_div_with_mul pass</span>
</pre></div>
</div>
</section>
<section id="partitioner">
<h2>Partitioner<a class="headerlink" href="#partitioner" title="Link to this heading">#</a></h2>
<p>There are a couple of common FX-graph based partitioners we can use to partition
the graph. However, these do not necessarily produce a graph that is compliant
with IR Spec, so be careful when using them.</p>
<section id="subgraph-matcher">
<h3>Subgraph Matcher<a class="headerlink" href="#subgraph-matcher" title="Link to this heading">#</a></h3>
<p>For finding subgraphs within a graph that match a specific pattern, we can
utilize FX’s
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/8597d37536ef11bdf6b0a539ab79af876e1c92f6/torch/fx/passes/utils/matcher_utils.py#L51"><code class="docutils literal notranslate"><span class="pre">SubgraphMatcher</span></code></a>.</p>
<p>Class Attributes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pattern</span> <span class="pre">(Graph)</span></code>: The targeted matching pattern. Placeholder nodes in the
graph will be treated as wildcards when matching.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">match_output</span> <span class="pre">(bool)</span></code>: If True, output node in the pattern graph will be
treated as a part of the targeted pattern.  If False, output node is ignored
during match.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">match_placeholder</span> <span class="pre">(bool)</span></code>: If True, placeholder node in the pattern graph
will be treated as a part of the targeted pattern. If False, placeholder
nodes will be used a wildcard.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">remove_overlapping_matches</span> <span class="pre">(bool)</span></code>: If True, in the case of overlapping
matches, only the first match will be returned.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ignore_literals</span> <span class="pre">(bool)</span></code>: If True, will not check if literals are equal and
will instead treat them as wildcards.</p></li>
</ul>
<p>Consider the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.fx.passes.utils.matcher_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">SubgraphMatcher</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LargeModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">addmm</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">)</span>

<span class="n">large_model_graph</span> <span class="o">=</span> <span class="n">to_edge</span><span class="p">(</span><span class="n">export</span><span class="p">(</span><span class="n">LargeModel</span><span class="p">(),</span> <span class="n">large_inputs</span><span class="p">))</span><span class="o">.</span><span class="n">exported_program</span><span class="p">()</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">graph</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PatternModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weight_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">addmm</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias_1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_1</span><span class="p">)</span>

<span class="n">pattern_graph</span> <span class="o">=</span> <span class="n">to_edge</span><span class="p">(</span><span class="n">export</span><span class="p">(</span><span class="n">PatternModel</span><span class="p">(),</span> <span class="n">pattern_inputs</span><span class="p">))</span><span class="o">.</span><span class="n">exported_program</span><span class="p">()</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">graph</span>

<span class="n">subgraph_matcher</span> <span class="o">=</span> <span class="n">SubgraphMatcher</span><span class="p">(</span><span class="n">pattern_graph</span><span class="p">)</span>
<span class="n">match_result</span> <span class="o">=</span> <span class="n">subgraph_matcher</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">large_model_graph</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">match</span></code> function returns a list of <code class="docutils literal notranslate"><span class="pre">InternalMatch</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">InternalMatch</span><span class="p">():</span>
    <span class="c1"># Nodes from which the match was found</span>
    <span class="n">anchors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Node</span><span class="p">]</span>
    <span class="c1"># Maps nodes in the pattern subgraph to nodes in the larger graph</span>
    <span class="n">nodes_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Node</span><span class="p">,</span> <span class="n">Node</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
    <span class="c1"># Nodes in target graph that are matched placeholder in pattern</span>
    <span class="n">placeholder_nodes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Node</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
    <span class="c1"># Nodes in matched subgraph returned by output</span>
    <span class="n">returning_nodes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Node</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="capability-based-partitioner">
<h3>Capability Based Partitioner<a class="headerlink" href="#capability-based-partitioner" title="Link to this heading">#</a></h3>
<p>To find the largest subgraphs of nodes that support a specific invariant, we can
utilize FX’s
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/8597d37536ef11bdf6b0a539ab79af876e1c92f6/torch/fx/passes/infra/partitioner.py#L34C1-L34C1"><code class="docutils literal notranslate"><span class="pre">CapabilityBasedPartitioner</span></code></a>.</p>
<p>Class Attributes</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">graph_module</span> <span class="pre">(torch.fx.GraphModule)</span></code>: The graph module we are partitioning on.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">operator_support</span> <span class="pre">(OperatorSupportBase)</span></code>: The object used to determine if a
node in the graph is supported in the partition.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">allows_single_node_partition</span> <span class="pre">(bool)</span></code>: If True, allows single node
partitions to be formed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">non_compute_ops</span> <span class="pre">(Optional[Sequence[str]])</span></code>: A set of ops that are
considered to be “non-compute” (ex <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.view</span></code> and
<code class="docutils literal notranslate"><span class="pre">_operator.getitem</span></code>, so that the partitioner will not create graphs that only
contain these non-compute ops</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">allowed_single_node_partition_ops</span> <span class="pre">(Optional[Sequence[str]])</span></code>: A set of ops
that are allowed to be in a single node partition.</p></li>
</ul>
<p>The
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/8597d37536ef11bdf6b0a539ab79af876e1c92f6/torch/fx/passes/operator_support.py#L28"><code class="docutils literal notranslate"><span class="pre">OperatorSupportBase</span></code></a>
class is used by
the partitioner to determine if a specific node in the graph belongs in the
partition. This is done by overriding the <code class="docutils literal notranslate"><span class="pre">is_node_supported</span></code> function. You can
chain multiple <code class="docutils literal notranslate"><span class="pre">OperatorSuppportBase</span></code> by using
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/8597d37536ef11bdf6b0a539ab79af876e1c92f6/torch/fx/passes/operator_support.py#L150"><code class="docutils literal notranslate"><span class="pre">chain</span></code></a>(which
returns False if any of the OperatorSupportBase return False) and
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/8597d37536ef11bdf6b0a539ab79af876e1c92f6/torch/fx/passes/operator_support.py#L164"><code class="docutils literal notranslate"><span class="pre">any_chain</span></code></a>
(which returns True if any of the OperatorSupportBase returns True).</p>
<p>Consider the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.fx.passes.infra.partitioner</span><span class="w"> </span><span class="kn">import</span> <span class="n">CapabilityBasedPartitioner</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.fx.passes.operator_support</span><span class="w"> </span><span class="kn">import</span> <span class="n">any_chain</span><span class="p">,</span> <span class="n">OperatorSupportBase</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AddMulOperatorSupport</span><span class="p">(</span><span class="n">OperatorSupportBase</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_node_supported</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">submodules</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span>

<span class="n">capability_partitioner</span> <span class="o">=</span> <span class="n">CapabilityBasedPartitioner</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">,</span>
    <span class="n">op_support</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Returns a list of partitions (list of nodes that belong in each partition)</span>
<span class="n">partition_list</span> <span class="o">=</span> <span class="n">capability_partitioner</span><span class="o">.</span><span class="n">propose_partitions</span><span class="p">()</span>
</pre></div>
</div>
<p>If you look at the capability based partitioner, you may also find a
<code class="docutils literal notranslate"><span class="pre">fuse_partition</span></code> function which will return a modified graph with the partitions
as submodules, and calls to these submodules in the toplevel graph through
<code class="docutils literal notranslate"><span class="pre">call_module</span></code> nodes. However, this is not compliant to the IR Spec because we do
not allow <code class="docutils literal notranslate"><span class="pre">call_module</span></code> nodes.</p>
</section>
<section id="combined">
<h3>Combined<a class="headerlink" href="#combined" title="Link to this heading">#</a></h3>
<p>We also provide a combined helper function:
<a class="reference external" href="https://github.com/pytorch/executorch/blob/d9eef24bb720804aa7b400b05241487510ae0dc2/exir/backend/canonical_partitioners/pattern_op_partitioner.py#L59"><code class="docutils literal notranslate"><span class="pre">generate_pattern_op_partitions</span></code></a></p>
<p>Args:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">graph_module</span> <span class="pre">(fx.GraphModule)</span></code>: Module that we want to partition</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">patterns</span> <span class="pre">(List[torch.fx.Graph])</span></code>: A list of patterns in the form of
torch.fx.Graph. These graphs can be obtained through the <code class="docutils literal notranslate"><span class="pre">graph</span></code> field from a
GraphModule obtained by exir.capture (recommended) or symbolic tracing (which
might not result in an accurate edge dialect graph), or by manual crafting a
graph module.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">op_support</span> <span class="pre">(OperatorSupportBase)</span></code>: A OperatorSupportBase that can be created
in the following ways:</p>
<ul>
<li><p>Subclassing it directly and implementing <code class="docutils literal notranslate"><span class="pre">is_node_supported()</span></code></p></li>
<li><p>Getting the result of <code class="docutils literal notranslate"><span class="pre">create_op_support()</span></code></p></li>
<li><p>Getting the result of <code class="docutils literal notranslate"><span class="pre">create_pattern_support()</span></code></p></li>
<li><p>Multiple OperatorSupportBase classes chained together with <code class="docutils literal notranslate"><span class="pre">chain()</span></code> or <code class="docutils literal notranslate"><span class="pre">any_chain()</span></code></p></li>
</ul>
</li>
</ul>
<p>Returns</p>
<ul class="simple">
<li><p>A list of partitions (largest possible subgraphs) containing nodes are
supported by the union of the given OperatorSupportBase object and the
given pattern graphs.</p></li>
</ul>
</section>
<section id="source-partitioner">
<h3>Source Partitioner<a class="headerlink" href="#source-partitioner" title="Link to this heading">#</a></h3>
<p>For more complicated use cases in which users want to partition based on higher
level modules (<code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.Linear</span></code>) which are now
decomposed into their operators (<code class="docutils literal notranslate"><span class="pre">aten.permute</span></code>, <code class="docutils literal notranslate"><span class="pre">aten.addmm</span></code>), we have the
following <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/utils/source_matcher_utils.py#L51">helper function</a>:</p>
<p><code class="docutils literal notranslate"><span class="pre">get_source_partitions(graph:</span> <span class="pre">torch.fx.Graph,</span> <span class="pre">wanted_sources:</span> <span class="pre">List[Any])</span> <span class="pre">-&gt;</span> <span class="pre">Dict[Any,</span> <span class="pre">SourcePartition]</span></code></p>
<p>Args:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">graph</span></code>: The graph we want to partition</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wanted_sources</span></code>: List of sources of nodes that were decomposed from this
source. This can be a function (ex. <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.linear</span></code>) or a leaf
module type (ex. <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code>)</p></li>
</ul>
<p>Returns:</p>
<ul class="simple">
<li><p>Dictionary mapping sources (ex. <code class="docutils literal notranslate"><span class="pre">torch.nn.modules.linear.Linear</span></code>) to a list of
<code class="docutils literal notranslate"><span class="pre">SourcePartitions</span></code> that correspond to the list of nodes that were flattened from
a module of that type.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SourcePartition</span><span class="p">():</span>
    <span class="c1"># Nodes in a particular partition</span>
    <span class="n">nodes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Node</span><span class="p">]</span>
    <span class="c1"># Module type</span>
    <span class="n">module_type</span><span class="p">:</span> <span class="n">Type</span>
    <span class="c1"># Nodes in the graph that are needed as inputs to the partition</span>
    <span class="n">input_nodes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Node</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
    <span class="c1"># Nodes in the partition that are being used by nodes outside of the partition</span>
    <span class="n">output_nodes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Node</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
    <span class="c1"># Parameters that are being used</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
</pre></div>
</div>
<p>An example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),)</span>
<span class="n">edge_graph</span> <span class="o">=</span> <span class="n">to_edge</span><span class="p">(</span><span class="n">export</span><span class="p">(</span><span class="n">M</span><span class="p">(),</span> <span class="n">inputs</span><span class="p">))</span><span class="o">.</span><span class="n">exported_program</span><span class="p">()</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">graph</span>
<span class="nb">print</span><span class="p">(</span><span class="n">edge_graph</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">graph():</span>
<span class="sd">    %arg0 : [#users=1] = placeholder[target=arg0]</span>
<span class="sd">    %_param_constant0 : [#users=1] = get_attr[target=_param_constant0]</span>
<span class="sd">    %permute_default : [#users=1] = call_function[target=torch.ops.aten.permute_copy.default](args = (%_param_constant0,), kwargs = {})</span>
<span class="sd">    %_param_constant1 : [#users=1] = get_attr[target=_param_constant1]</span>
<span class="sd">    %addmm_default : [#users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %arg0, %t_default), kwargs = {})</span>
<span class="sd">    %_param_constant0_1 : [#users=1] = get_attr[target=_param_constant0]</span>
<span class="sd">    %permute_default_1 : [#users=1] = call_function[target=torch.ops.aten.permute_copy.default](args = (%_param_constant0_1,), kwargs = {})</span>
<span class="sd">    %_param_constant1_1 : [#users=1] = get_attr[target=_param_constant1]</span>
<span class="sd">    %addmm_default_1 : [#users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1_1, %addmm_default, %t_default_1), kwargs = {})</span>
<span class="sd">    %relu_default : [#users=1] = call_function[target=torch.ops.aten.relu.default](args = (%addmm_default_1,), kwargs = {})</span>
<span class="sd">    %_param_constant2 : [#users=1] = get_attr[target=_param_constant2]</span>
<span class="sd">    %permute_default_2 : [#users=1] = call_function[target=torch.ops.aten.permute_copy.default](args = (%_param_constant2,), kwargs = {})</span>
<span class="sd">    %_param_constant3 : [#users=1] = get_attr[target=_param_constant3]</span>
<span class="sd">    %addmm_default_2 : [#users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant3, %relu_default, %t_default_2), kwargs = {})</span>
<span class="sd">    return [addmm_default_2]</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">module_partitions</span> <span class="o">=</span> <span class="n">get_source_partitions</span><span class="p">(</span><span class="n">edge_graph</span><span class="p">,</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">module_partitions</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">{&lt;class &#39;torch.nn.modules.linear.Linear&#39;&gt;: [</span>
<span class="sd">    ModulePartition(nodes=[_param_constant0, t_default, _param_constant1, addmm_default], module_type=&lt;class &#39;torch.nn.modules.linear.Linear&#39;&gt;, input_nodes=[arg0], output_nodes=[addmm_default], params=[&quot;_param_constant0&quot;, &quot;_param_constant1&quot;]),</span>
<span class="sd">    ModulePartition(nodes=[_param_constant0_1, t_default_1, _param_constant1_1, addmm_default_1], module_type=&lt;class &#39;torch.nn.modules.linear.Linear&#39;&gt;, input_nodes=[addmm_default], output_nodes=[addmm_default_1], params=[&quot;_param_constant0_1&quot;, &quot;_param_constant1_1&quot;]),</span>
<span class="sd">    ModulePartition(nodes=[_param_constant2, t_default_2, _param_constant3, addmm_default_2], module_type=&lt;class &#39;torch.nn.modules.linear.Linear&#39;&gt;, input_nodes=[relu_default], output_nodes=[addmm_default_2], params=[&quot;_param_constant2&quot;, &quot;_param_constant3&quot;])],</span>

<span class="sd"> &lt;class &#39;torch.nn.modules.activation.ReLU&#39;&gt;: [</span>
<span class="sd">    ModulePartition(nodes=[relu_default], module_type=&lt;class &#39;torch.nn.modules.activation.ReLU&#39;&gt;, input_nodes=[addmm_default_1], output_nodes=[relu_default], params=[])]}</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
</section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="compiler-backend-dialect.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Backend Dialect</p>
      </div>
    </a>
    <a class="right-next"
       href="compiler-memory-planning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Memory Planning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2024, ExecuTorch.
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="compiler-backend-dialect.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Backend Dialect</p>
      </div>
    </a>
    <a class="right-next"
       href="compiler-memory-planning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Memory Planning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#passes">Passes</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#level-1">Level 1</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-to-one-pass">One-to-One Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-to-x-pass">One-to-X Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-to-none-pass">One-to-None Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#utilizing-local-information">Utilizing Local Information</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#level-2">Level 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#level-3">Level 3</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pass-manager">Pass Manager</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partitioner">Partitioner</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subgraph-matcher">Subgraph Matcher</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#capability-based-partitioner">Capability Based Partitioner</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#combined">Combined</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#source-partitioner">Source Partitioner</a></li>
</ul>
</li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/executorch/edit/main/docs/source/compiler-custom-compiler-passes.md">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/compiler-custom-compiler-passes.md.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    
    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>
    

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, ExecuTorch.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Custom Compiler Passes and Partitioners",
       "headline": "Custom Compiler Passes and Partitioners",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/compiler-custom-compiler-passes.html",
       "articleBody": "Custom Compiler Passes and Partitioners# Passes# Passes can be roughly categorized into a couple of axes: Axis A: Creating one-to-X mapping (for example, decomposition) Creating many-to-one mapping (for example, fusion) Axis B: Performing forwards iteration (for example, shape propagation) Performing backwards iteration (for example, dead code elimination) Axis C: Dependent on local node information (eg. out-variant conversion) Dependent on global graph information (eg. memory planning) Our projection on the frequency of these use cases are: A.1, B.1, C.1 A.2 B.2, C.2 Level 1# For level 1 uses cases (creating one-to-X mappings, performing forwards iterations, and looking at local node information), we can utilize a helper class called ExportPass. This is an interpreter-based way where we execute each node and recreate the graph except with transformations specified. This allows us to preserve the IR Spec by ensuring that all nodes created while in the pass meet the IR Spec including ensuring that metadata such as stack trace, FakeTensor values, and torch.nn.Module hierarchy are preserved and updated depending on the transformations made. To implement this pass, we can create a subclass of ExportPass and implement the exposed functions. When called with a graph module, it will run the graph module and create a new graph containing the changes specified by the pass. This means that the graph module passed in must be runnable on CPU, and this invariant will be maintained after the pass is run. One-to-One Pass# An example for one-to-one mappings, if we wanted to replace an op A with another op B, we can run the given fx.GraphModule, and every time we see op A, return op B. Consider the following example: class ReplaceInPlaceReluWithOutOfPlaceReluPass(ExportPass): \"\"\" relu_ is the in-place version. Replace it with relu, which is the out-of-place version \"\"\" def call_operator(self, op, args, kwargs, meta): if op != torch.ops.aten.relu_.default: return super().call_operator(op, args, kwargs, meta) return super().call_operator(Op(torch.ops.aten.relu.default), args, kwargs, meta) # To create a pass replace_pass = ReplaceInPlaceReluWithOutOfPlaceReluPass() # To run a pass new_graph_module = replace_pass(graph_module).graph_module The super().call_operator(op, args, kwargs, meta) call creates a call_function FX node, and returns the result of running the operator with the given arguments. One-to-X Pass# If we wanted to do one-to-X mappings, like replacing op A with 2 other ops B and C, we would then make 2 calls to super().call_operator to create 2 FX nodes, one with op B and another with op C, and return the result of running op C. For example: class ReplaceAddWithMulSub(ExportPass): \"\"\" Original: def f(x, y): return x + y After pass: def f(x, y): z = x * y return z - y \"\"\" def call_operator(self, op, args, kwargs, meta): if op != torch.ops.aten.add.default: return super().call_operator(op, args, kwargs, meta) x, y = args mul_res = super().call_operator( torch.ops.aten.mul.default, args, {}, meta ) return super().call_operator( torch.ops.aten.sub.default, (mul_res, y), {}, meta ) One-to-None Pass# If we wanted to remove an op, we can just return the value passed into the function: class RemoveDetachPass(ExportPass): def call_operator(self, op, args, kwargs, meta): if op not in ( torch.ops.aten.detach.default, torch.ops.aten.detach_copy.default, ): return super().call_operator(op, args, kwargs, meta) assert len(args) == 1 return args[0] Utilizing Local Information# An example of utilizing local node information is, if we wanted to convert all the scalars within the graph to tensors, we can run the given fx.GraphModule, and for every argument that contains a scalar, we convert it to a tensor. It might look something like: def args_map(op, fn, args, kwargs): assert isinstance(args, tuple) assert isinstance(kwargs, dict) args = list(args) kwargs = kwargs.copy() # Update the argument based on the function passed def update(key, args, schema): args[key] = fn(args[key], schema) # Update each argument in the schema for i, schema in enumerate(self.op._schema.arguments): if schema.name in kwargs: update(schema.name, kwargs, schema) elif not schema.kwarg_only and i \u003c len(args): update(i, args, schema) class ScalarToTensorPass(ExportPass): def call_operator(self, op, args, kwargs): def try_coerce(value, arg): return ( torch.tensor(value) if isinstance(value, (float, int, bool)) and type(arg.type) == torch.TensorType else value ) args, kwargs = args_map(op, try_coerce, args, kwargs) return super().call_operator(op, args, kwargs) Level 2# For creating many-to-one mappings, we can utilize FX\u2019s subgraph rewriter. Given a pattern, it creates a subgraph of operators matching to the pattern, and then replaces each matched subgraph with the replacement. Note This is an inplace operation. The pattern and replacement inputs must be callable functions written with the same ops that are used in the EXIR graph you are matching with (ATen ops) so that the subgraph rewriter can find the correct pattern in the graph. Inputs to the pattern/replacement callables will be treated as wildcards. Consider the following example: from torch.fx import subgraph_rewriter def replace_patterns(graph_module): def pattern(x, y): x = torch.ops.aten.add.Tensor(x, y) x = torch.ops.aten.mul.Tensor(x, y) return x def replacement(x, y): return torch.ops.aten.sub.Tensor(x, y) replaced_patterns = subgraph_rewriter.replace_pattern_with_filters( traced_module, pattern, replacement ) The subgraph rewriter returns a list of ReplacedPatterns: @dataclass class ReplacedPatterns: # Node from which the match was found anchor: Node # Maps nodes in the pattern subgraph to nodes in the larger graph nodes_map: Dict[Node, Node] # List of nodes that were added into the graph replacements: List[Node] Note The nodes created by the subgraph rewriter will not have the metadata that is normally in EXIR nodes (`stack_trace`, `val`, `nn_module_stack`). Level 3# For the third way of creating a pass, we can utilize the most basic PassBase. To create a pass, we can subclass this and implement the function call with the pass contents. Additionally, we can implement the functions requires and ensures which will be called before and after the function call. Note that these functions can also be overridden in ExportPass. To run a pass on a graph module, we can pass the graph module directly to an instance of the class. Consider the following example: class ReplaceAddPass(PassBase): def __init__(self, replace_op): self.replace_op = replace_op def call(self, graph_module): for node in gm.graph.nodes: if node.op == \"call_function\" and node.target == torch.add: node.target = self.replace_op # Optional to implement, will be called before call() def requires(self, graph_module) -\u003e None: for node in graph_module.graph.nodes: if node.op == \"call_function\" and node.target == torch.add: return raise ValueError(\"No torch.add ops!\") # Optional to implement, will be called after call() def ensures(self, graph_module: torch.fx.GraphModule) -\u003e None: pass # To create a pass replace_add_with_div = ReplaceAddPass(torch.div) # To run a pass replace_add_with_div(graph_module) Pass Manager# The PassManager is a class used to run multiple passes on a given graph module. When initializing a PassManager instance, we pass in a list of passes that we want to run and set a couple of flags. To run the collection of passes on a graph module, we can pass the graph module directly to the PassManager instance. An example: from executorch.exir.pass_manager import PassManager pm = PassManager( passes=[replace_add_with_div, replace_div_with_mul], run_checks_after_each_pass=True, suppress_check_failures=False, ) graph_module_out = pm(graph_module) To add a common set of checks that are run after each pass, we can call the function set_checks(check: Callable) which takes in a callable function as input. If the run_checks_after_each_pass flag is set, the check will be called after each pass is run on the graph module. An example: pm = PassManager(passes=[replace_add_with_div, replace_div_with_mul]) def check_div_target(graph_module): for node in graph_module.graph.nodes: if node.op == \"call_function\" and node.target != torch.div: raise ValueError(\"Target should be div!\") pm.add_checks(check_div_target) pm(graph_module) # raises ValueError after replace_div_with_mul pass Partitioner# There are a couple of common FX-graph based partitioners we can use to partition the graph. However, these do not necessarily produce a graph that is compliant with IR Spec, so be careful when using them. Subgraph Matcher# For finding subgraphs within a graph that match a specific pattern, we can utilize FX\u2019s SubgraphMatcher. Class Attributes: pattern (Graph): The targeted matching pattern. Placeholder nodes in the graph will be treated as wildcards when matching. match_output (bool): If True, output node in the pattern graph will be treated as a part of the targeted pattern. If False, output node is ignored during match. match_placeholder (bool): If True, placeholder node in the pattern graph will be treated as a part of the targeted pattern. If False, placeholder nodes will be used a wildcard. remove_overlapping_matches (bool): If True, in the case of overlapping matches, only the first match will be returned. ignore_literals (bool): If True, will not check if literals are equal and will instead treat them as wildcards. Consider the following example: from torch.fx.passes.utils.matcher_utils import SubgraphMatcher class LargeModel(torch.nn.Module): def __init__(self): super().__init__() self._weight = torch.nn.Parameter(torch.ones(3, 3)) self._bias = torch.nn.Parameter(torch.ones(3, 3)) def forward(self, x): return torch.ops.aten.addmm.default(self._bias, x, self._weight) large_model_graph = to_edge(export(LargeModel(), large_inputs)).exported_program().graph_module.graph class PatternModel(torch.nn.Module): def __init__(self): super().__init__() self._weight_1 = torch.nn.Parameter(torch.ones(5, 5)) self._bias_1 = torch.nn.Parameter(torch.ones(5, 5)) def forward(self, x): return torch.ops.aten.addmm.default(self._bias_1, x, self._weight_1) pattern_graph = to_edge(export(PatternModel(), pattern_inputs)).exported_program().graph_module.graph subgraph_matcher = SubgraphMatcher(pattern_graph) match_result = subgraph_matcher.match(large_model_graph) The match function returns a list of InternalMatch: @dataclass class InternalMatch(): # Nodes from which the match was found anchors: List[Node] # Maps nodes in the pattern subgraph to nodes in the larger graph nodes_map: Dict[Node, Node] = field(default_factory=dict) # Nodes in target graph that are matched placeholder in pattern placeholder_nodes: List[Node] = field(default_factory=list) # Nodes in matched subgraph returned by output returning_nodes: List[Node] = field(default_factory=list) Capability Based Partitioner# To find the largest subgraphs of nodes that support a specific invariant, we can utilize FX\u2019s CapabilityBasedPartitioner. Class Attributes graph_module (torch.fx.GraphModule): The graph module we are partitioning on. operator_support (OperatorSupportBase): The object used to determine if a node in the graph is supported in the partition. allows_single_node_partition (bool): If True, allows single node partitions to be formed. non_compute_ops (Optional[Sequence[str]]): A set of ops that are considered to be \u201cnon-compute\u201d (ex torch.ops.aten.view and _operator.getitem, so that the partitioner will not create graphs that only contain these non-compute ops allowed_single_node_partition_ops (Optional[Sequence[str]]): A set of ops that are allowed to be in a single node partition. The OperatorSupportBase class is used by the partitioner to determine if a specific node in the graph belongs in the partition. This is done by overriding the is_node_supported function. You can chain multiple OperatorSuppportBase by using chain(which returns False if any of the OperatorSupportBase return False) and any_chain (which returns True if any of the OperatorSupportBase returns True). Consider the following example: from torch.fx.passes.infra.partitioner import CapabilityBasedPartitioner from torch.fx.passes.operator_support import any_chain, OperatorSupportBase class AddMulOperatorSupport(OperatorSupportBase): def is_node_supported(self, submodules, node: torch.fx.Node) -\u003e bool: return node.op == \"call_function\" and node.target in [ torch.ops.aten.add.Tensor, torch.ops.aten.mul.Tensor, ] capability_partitioner = CapabilityBasedPartitioner( graph_module, op_support, ) # Returns a list of partitions (list of nodes that belong in each partition) partition_list = capability_partitioner.propose_partitions() If you look at the capability based partitioner, you may also find a fuse_partition function which will return a modified graph with the partitions as submodules, and calls to these submodules in the toplevel graph through call_module nodes. However, this is not compliant to the IR Spec because we do not allow call_module nodes. Combined# We also provide a combined helper function: generate_pattern_op_partitions Args: graph_module (fx.GraphModule): Module that we want to partition patterns (List[torch.fx.Graph]): A list of patterns in the form of torch.fx.Graph. These graphs can be obtained through the graph field from a GraphModule obtained by exir.capture (recommended) or symbolic tracing (which might not result in an accurate edge dialect graph), or by manual crafting a graph module. op_support (OperatorSupportBase): A OperatorSupportBase that can be created in the following ways: Subclassing it directly and implementing is_node_supported() Getting the result of create_op_support() Getting the result of create_pattern_support() Multiple OperatorSupportBase classes chained together with chain() or any_chain() Returns A list of partitions (largest possible subgraphs) containing nodes are supported by the union of the given OperatorSupportBase object and the given pattern graphs. Source Partitioner# For more complicated use cases in which users want to partition based on higher level modules (torch.nn.Linear or torch.nn.functional.Linear) which are now decomposed into their operators (aten.permute, aten.addmm), we have the following helper function: get_source_partitions(graph: torch.fx.Graph, wanted_sources: List[Any]) -\u003e Dict[Any, SourcePartition] Args: graph: The graph we want to partition wanted_sources: List of sources of nodes that were decomposed from this source. This can be a function (ex. torch.nn.functional.linear) or a leaf module type (ex. torch.nn.Linear) Returns: Dictionary mapping sources (ex. torch.nn.modules.linear.Linear) to a list of SourcePartitions that correspond to the list of nodes that were flattened from a module of that type. @dataclass class SourcePartition(): # Nodes in a particular partition nodes: List[Node] # Module type module_type: Type # Nodes in the graph that are needed as inputs to the partition input_nodes: List[Node] = field(default_factory=list) # Nodes in the partition that are being used by nodes outside of the partition output_nodes: List[Node] = field(default_factory=list) # Parameters that are being used params: List[str] = field(default_factory=list) An example: class M(torch.nn.Module): def __init__(self): super().__init__() self.linear1 = torch.nn.Linear(3, 3) self.relu = torch.nn.ReLU() self.linear2 = torch.nn.Linear(3, 5) def forward(self, x): x = self.linear1(x) x = self.linear1(x) x = self.relu(x) x = self.linear2(x) return x inputs = (torch.randn(3, 3),) edge_graph = to_edge(export(M(), inputs)).exported_program().graph_module.graph print(edge_graph) \"\"\" graph(): %arg0 : [#users=1] = placeholder[target=arg0] %_param_constant0 : [#users=1] = get_attr[target=_param_constant0] %permute_default : [#users=1] = call_function[target=torch.ops.aten.permute_copy.default](args = (%_param_constant0,), kwargs = {}) %_param_constant1 : [#users=1] = get_attr[target=_param_constant1] %addmm_default : [#users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %arg0, %t_default), kwargs = {}) %_param_constant0_1 : [#users=1] = get_attr[target=_param_constant0] %permute_default_1 : [#users=1] = call_function[target=torch.ops.aten.permute_copy.default](args = (%_param_constant0_1,), kwargs = {}) %_param_constant1_1 : [#users=1] = get_attr[target=_param_constant1] %addmm_default_1 : [#users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1_1, %addmm_default, %t_default_1), kwargs = {}) %relu_default : [#users=1] = call_function[target=torch.ops.aten.relu.default](args = (%addmm_default_1,), kwargs = {}) %_param_constant2 : [#users=1] = get_attr[target=_param_constant2] %permute_default_2 : [#users=1] = call_function[target=torch.ops.aten.permute_copy.default](args = (%_param_constant2,), kwargs = {}) %_param_constant3 : [#users=1] = get_attr[target=_param_constant3] %addmm_default_2 : [#users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant3, %relu_default, %t_default_2), kwargs = {}) return [addmm_default_2] \"\"\" module_partitions = get_source_partitions(edge_graph, [torch.nn.Linear, torch.nn.ReLU]) print(module_partitions) \"\"\" {\u003cclass \u0027torch.nn.modules.linear.Linear\u0027\u003e: [ ModulePartition(nodes=[_param_constant0, t_default, _param_constant1, addmm_default], module_type=\u003cclass \u0027torch.nn.modules.linear.Linear\u0027\u003e, input_nodes=[arg0], output_nodes=[addmm_default], params=[\"_param_constant0\", \"_param_constant1\"]), ModulePartition(nodes=[_param_constant0_1, t_default_1, _param_constant1_1, addmm_default_1], module_type=\u003cclass \u0027torch.nn.modules.linear.Linear\u0027\u003e, input_nodes=[addmm_default], output_nodes=[addmm_default_1], params=[\"_param_constant0_1\", \"_param_constant1_1\"]), ModulePartition(nodes=[_param_constant2, t_default_2, _param_constant3, addmm_default_2], module_type=\u003cclass \u0027torch.nn.modules.linear.Linear\u0027\u003e, input_nodes=[relu_default], output_nodes=[addmm_default_2], params=[\"_param_constant2\", \"_param_constant3\"])], \u003cclass \u0027torch.nn.modules.activation.ReLU\u0027\u003e: [ ModulePartition(nodes=[relu_default], module_type=\u003cclass \u0027torch.nn.modules.activation.ReLU\u0027\u003e, input_nodes=[addmm_default_1], output_nodes=[relu_default], params=[])]} \"\"\"",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/compiler-custom-compiler-passes.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>