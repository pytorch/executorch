{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Exporting to ExecuTorch Tutorial\n\n**Author:** [Angela Yi](https://github.com/angelayi)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ExecuTorch is a unified ML stack for lowering PyTorch models to edge devices.\nIt introduces improved entry points to perform model, device, and/or use-case\nspecific optimizations such as backend delegation, user-defined compiler\ntransformations, default or user-defined memory planning, and more.\n\nAt a high level, the workflow looks as follows:\n\n<img src=\"file://../executorch_stack.png\" width=\"560\">\n\nIn this tutorial, we will cover the APIs in the \"Program preparation\" steps to\nlower a PyTorch model to a format which can be loaded to device and run on the\nExecuTorch runtime.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\nTo run this tutorial, you\u2019ll first need to\n[Set up your ExecuTorch environment](../getting-started-setup.html)_.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting a Model\n\nNote: The Export APIs are still undergoing changes to align better with the\nlonger term state of export. Please refer to this\n[issue](https://github.com/pytorch/executorch/issues/290)_ for more details.\n\nThe first step of lowering to ExecuTorch is to export the given model (any\ncallable or ``torch.nn.Module``) to a graph representation. This is done via\n``torch.export``, which takes in an ``torch.nn.Module``, a tuple of\npositional arguments, optionally a dictionary of keyword arguments (not shown\nin the example), and a list of dynamic shapes (covered later).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.export import export, ExportedProgram\n\n\nclass SimpleConv(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels=3, out_channels=16, kernel_size=3, padding=1\n        )\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        a = self.conv(x)\n        return self.relu(a)\n\n\nexample_args: tuple[torch.Tensor] = (torch.randn(1, 3, 256, 256),)\naten_dialect: ExportedProgram = export(SimpleConv(), example_args, strict=True)\nprint(aten_dialect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output of ``torch.export.export`` is a fully flattened graph (meaning the\ngraph does not contain any module hierarchy, except in the case of control\nflow operators). Additionally, the graph is purely functional, meaning it does\nnot contain operations with side effects such as mutations or aliasing.\n\nMore specifications about the result of ``torch.export`` can be found\n[here](https://pytorch.org/docs/main/export.html)_ .\n\nThe graph returned by ``torch.export`` only contains functional ATen operators\n(~2000 ops), which we will call the ``ATen Dialect``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expressing Dynamism\n\nBy default, the exporting flow will trace the program assuming that all input\nshapes are static, so if we run the program with inputs shapes that are\ndifferent than the ones we used while tracing, we will run into an error:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import traceback as tb\n\n\nclass Basic(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        return x + y\n\n\nexample_args_2: tuple[torch.Tensor, torch.Tensor] = (\n    torch.randn(3, 3),\n    torch.randn(3, 3),\n)\naten_dialect = export(Basic(), example_args_2, strict=True)\n\n# Works correctly\nprint(aten_dialect.module()(torch.ones(3, 3), torch.ones(3, 3)))\n\n# Errors\ntry:\n    print(aten_dialect.module()(torch.ones(3, 2), torch.ones(3, 2)))\nexcept Exception:\n    tb.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To express that some input shapes are dynamic, we can insert dynamic\n shapes to the exporting flow. This is done through the ``Dim`` API:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.export import Dim\n\nexample_args_2 = (torch.randn(3, 3), torch.randn(3, 3))\ndim1_x = Dim(\"dim1_x\", min=1, max=10)\ndynamic_shapes = {\"x\": {1: dim1_x}, \"y\": {1: dim1_x}}\naten_dialect = export(\n    Basic(), example_args_2, dynamic_shapes=dynamic_shapes, strict=True\n)\nprint(aten_dialect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that that the inputs ``arg0_1`` and ``arg1_1`` now have shapes (3, s0),\nwith ``s0`` being a symbol representing that this dimension can be a range\nof values.\n\nAdditionally, we can see in the **Range constraints** that value of ``s0`` has\nthe range [1, 10], which was specified by our dynamic shapes.\n\nNow let's try running the model with different shapes:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Works correctly\nprint(aten_dialect.module()(torch.ones(3, 3), torch.ones(3, 3)))\nprint(aten_dialect.module()(torch.ones(3, 2), torch.ones(3, 2)))\n\n# Errors because it violates our constraint that input 0, dim 1 <= 10\ntry:\n    print(aten_dialect.module()(torch.ones(3, 15), torch.ones(3, 15)))\nexcept Exception:\n    tb.print_exc()\n\n# Errors because it violates our constraint that input 0, dim 1 == input 1, dim 1\ntry:\n    print(aten_dialect.module()(torch.ones(3, 3), torch.ones(3, 2)))\nexcept Exception:\n    tb.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Addressing Untraceable Code\n\nAs our goal is to capture the entire computational graph from a PyTorch\nprogram, we might ultimately run into untraceable parts of programs. To\naddress these issues, the\n[torch.export documentation](https://pytorch.org/docs/main/export.html#limitations-of-torch-export)_,\nor the\n[torch.export tutorial](https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html)_\nwould be the best place to look.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performing Quantization\n\nTo quantize a model, we first need to capture the graph with\n``torch.export.export``, perform quantization, and then\ncall ``torch.export``. ``torch.export.export`` returns a\ngraph which contains ATen operators which are Autograd safe, meaning they are\nsafe for eager-mode training, which is needed for quantization. We will call\nthe graph at this level, the ``Pre-Autograd ATen Dialect`` graph.\n\nCompared to\n[FX Graph Mode Quantization](https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html)_,\nwe will need to call two new APIs: ``prepare_pt2e`` and ``convert_pt2e``\ninstead of ``prepare_fx`` and ``convert_fx``. It differs in that\n``prepare_pt2e`` takes a backend-specific ``Quantizer`` as an argument, which\nwill annotate the nodes in the graph with information needed to quantize the\nmodel properly for a specific backend.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.export import export\n\nexample_args = (torch.randn(1, 3, 256, 256),)\npre_autograd_aten_dialect = export(SimpleConv(), example_args, strict=True).module()\nprint(\"Pre-Autograd ATen Dialect Graph\")\nprint(pre_autograd_aten_dialect)\n\nfrom executorch.backends.xnnpack.quantizer.xnnpack_quantizer import (\n    get_symmetric_quantization_config,\n    XNNPACKQuantizer,\n)\nfrom torchao.quantization.pt2e.quantize_pt2e import convert_pt2e, prepare_pt2e\n\nquantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())\nprepared_graph = prepare_pt2e(pre_autograd_aten_dialect, quantizer)  # type: ignore[arg-type]\n# calibrate with a sample dataset\nconverted_graph = convert_pt2e(prepared_graph)\nprint(\"Quantized Graph\")\nprint(converted_graph)\n\naten_dialect = export(converted_graph, example_args, strict=True)\nprint(\"ATen Dialect Graph\")\nprint(aten_dialect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More information on how to quantize a model, and how a backend can implement a\n``Quantizer`` can be found\n[here](https://pytorch.org/docs/main/quantization.html#prototype-pytorch-2-export-quantization)_.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lowering to Edge Dialect\n\nAfter exporting and lowering the graph to the ``ATen Dialect``, the next step\nis to lower to the ``Edge Dialect``, in which specializations that are useful\nfor edge devices but not necessary for general (server) environments will be\napplied.\nSome of these specializations include:\n\n- DType specialization\n- Scalar to tensor conversion\n- Converting all ops to the ``executorch.exir.dialects.edge`` namespace.\n\nNote that this dialect is still backend (or target) agnostic.\n\nThe lowering is done through the ``to_edge`` API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from executorch.exir import EdgeProgramManager, to_edge\n\nexample_args = (torch.randn(1, 3, 256, 256),)\naten_dialect = export(SimpleConv(), example_args, strict=True)\n\nedge_program: EdgeProgramManager = to_edge(aten_dialect)\nprint(\"Edge Dialect Graph\")\nprint(edge_program.exported_program())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``to_edge()`` returns an ``EdgeProgramManager`` object, which contains the\nexported programs which will be placed on this device. This data structure\nallows users to export multiple programs and combine them into one binary. If\nthere is only one program, it will by default be saved to the name \"forward\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Encode(torch.nn.Module):\n    def forward(self, x):\n        return torch.nn.functional.linear(x, torch.randn(5, 10))\n\n\nclass Decode(torch.nn.Module):\n    def forward(self, x):\n        return torch.nn.functional.linear(x, torch.randn(10, 5))\n\n\nencode_args = (torch.randn(1, 10),)\naten_encode: ExportedProgram = export(Encode(), encode_args, strict=True)\n\ndecode_args = (torch.randn(1, 5),)\naten_decode: ExportedProgram = export(Decode(), decode_args, strict=True)\n\nedge_program = to_edge({\"encode\": aten_encode, \"decode\": aten_decode})\nfor method in edge_program.methods:\n    print(f\"Edge Dialect graph of {method}\")\n    print(edge_program.exported_program(method))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also run additional passes on the exported program through\nthe ``transform`` API. An in-depth documentation on how to write\ntransformations can be found\n[here](../compiler-custom-compiler-passes.html)_.\n\nNote that since the graph is now in the Edge Dialect, all passes must also\nresult in a valid Edge Dialect graph (specifically one thing to point out is\nthat the operators are now in the ``executorch.exir.dialects.edge`` namespace,\nrather than the ``torch.ops.aten`` namespace.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "example_args = (torch.randn(1, 3, 256, 256),)\naten_dialect = export(SimpleConv(), example_args, strict=True)\nedge_program = to_edge(aten_dialect)\nprint(\"Edge Dialect Graph\")\nprint(edge_program.exported_program())\n\nfrom executorch.exir.dialects._ops import ops as exir_ops\nfrom executorch.exir.pass_base import ExportPass\n\n\nclass ConvertReluToSigmoid(ExportPass):\n    def call_operator(self, op, args, kwargs, meta):\n        if op == exir_ops.edge.aten.relu.default:\n            return super().call_operator(\n                exir_ops.edge.aten.sigmoid.default, args, kwargs, meta\n            )\n        else:\n            return super().call_operator(op, args, kwargs, meta)\n\n\ntransformed_edge_program = edge_program.transform((ConvertReluToSigmoid(),))\nprint(\"Transformed Edge Dialect Graph\")\nprint(transformed_edge_program.exported_program())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: if you see error like ``torch._export.verifier.SpecViolationError:\nOperator torch._ops.aten._native_batch_norm_legit_functional.default is not\nAten Canonical``,\nplease file an issue in https://github.com/pytorch/executorch/issues and we're happy to help!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delegating to a Backend\n\nWe can now delegate parts of the graph or the whole graph to a third-party\nbackend through the ``to_backend`` API.  An in-depth documentation on the\nspecifics of backend delegation, including how to delegate to a backend and\nhow to implement a backend, can be found\n[here](../compiler-delegate-and-partitioner.html)_.\n\nThere are three ways for using this API:\n\n1. We can lower the whole module.\n2. We can take the lowered module, and insert it in another larger module.\n3. We can partition the module into subgraphs that are lowerable, and then\n   lower those subgraphs to a backend.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lowering the Whole Module\n\nTo lower an entire module, we can pass ``to_backend`` the backend name, the\nmodule to be lowered, and a list of compile specs to help the backend with the\nlowering process.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LowerableModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return torch.sin(x)\n\n\n# Export and lower the module to Edge Dialect\nexample_args = (torch.ones(1),)\naten_dialect = export(LowerableModule(), example_args, strict=True)\nedge_program = to_edge(aten_dialect)\nto_be_lowered_module = edge_program.exported_program()\n\nfrom executorch.exir.backend.backend_api import LoweredBackendModule, to_backend\n\n# Import the backend\nfrom executorch.exir.backend.test.backend_with_compiler_demo import (  # noqa\n    BackendWithCompilerDemo,\n)\n\n# Lower the module\nlowered_module: LoweredBackendModule = to_backend(  # type: ignore[call-arg]\n    \"BackendWithCompilerDemo\", to_be_lowered_module, []\n)\nprint(lowered_module)\nprint(lowered_module.backend_id)\nprint(lowered_module.processed_bytes)\nprint(lowered_module.original_module)\n\n# Serialize and save it to a file\nsave_path = \"delegate.pte\"\nwith open(save_path, \"wb\") as f:\n    f.write(lowered_module.buffer())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this call, ``to_backend`` will return a ``LoweredBackendModule``. Some\nimportant attributes of the ``LoweredBackendModule`` are:\n\n- ``backend_id``: The name of the backend this lowered module will run on in\n  the runtime\n- ``processed_bytes``: a binary blob which will tell the backend how to run\n  this program in the runtime\n- ``original_module``: the original exported module\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compose the Lowered Module into Another Module\n\nIn cases where we want to reuse this lowered module in multiple programs, we\ncan compose this lowered module with another module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class NotLowerableModule(torch.nn.Module):\n    def __init__(self, bias):\n        super().__init__()\n        self.bias = bias\n\n    def forward(self, a, b):\n        return torch.add(torch.add(a, b), self.bias)\n\n\nclass ComposedModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.non_lowerable = NotLowerableModule(torch.ones(1) * 0.3)\n        self.lowerable = lowered_module\n\n    def forward(self, x):\n        a = self.lowerable(x)\n        b = self.lowerable(a)\n        ret = self.non_lowerable(a, b)\n        return a, b, ret\n\n\nexample_args = (torch.ones(1),)\naten_dialect = export(ComposedModule(), example_args, strict=True)\nedge_program = to_edge(aten_dialect)\nexported_program = edge_program.exported_program()\nprint(\"Edge Dialect graph\")\nprint(exported_program)\nprint(\"Lowered Module within the graph\")\nprint(exported_program.graph_module.lowered_module_0.backend_id)\nprint(exported_program.graph_module.lowered_module_0.processed_bytes)\nprint(exported_program.graph_module.lowered_module_0.original_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that there is now a ``torch.ops.higher_order.executorch_call_delegate`` node in the\ngraph, which is calling ``lowered_module_0``. Additionally, the contents of\n``lowered_module_0`` are the same as the ``lowered_module`` we created\npreviously.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Partition and Lower Parts of a Module\n\nA separate lowering flow is to pass ``to_backend`` the module that we want to\nlower, and a backend-specific partitioner. ``to_backend`` will use the\nbackend-specific partitioner to tag nodes in the module which are lowerable,\npartition those nodes into subgraphs, and then create a\n``LoweredBackendModule`` for each of those subgraphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Foo(torch.nn.Module):\n    def forward(self, a, x, b):\n        y = torch.mm(a, x)\n        z = y + b\n        a = z - a\n        y = torch.mm(a, x)\n        z = y + b\n        return z\n\n\nexample_args_3 = (torch.randn(2, 2), torch.randn(2, 2), torch.randn(2, 2))\naten_dialect = export(Foo(), example_args_3, strict=True)\nedge_program = to_edge(aten_dialect)\nexported_program = edge_program.exported_program()\nprint(\"Edge Dialect graph\")\nprint(exported_program)\n\nfrom executorch.exir.backend.test.op_partitioner_demo import AddMulPartitionerDemo\n\ndelegated_program = to_backend(exported_program, AddMulPartitionerDemo())  # type: ignore[call-arg]\nprint(\"Delegated program\")\nprint(delegated_program)\nprint(delegated_program.graph_module.lowered_module_0.original_module)\nprint(delegated_program.graph_module.lowered_module_1.original_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that there are now 2 ``torch.ops.higher_order.executorch_call_delegate`` nodes in the\ngraph, one containing the operations `add, mul` and the other containing the\noperations `mul, add`.\n\nAlternatively, a more cohesive API to lower parts of a module is to directly\ncall ``to_backend`` on it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "example_args_3 = (torch.randn(2, 2), torch.randn(2, 2), torch.randn(2, 2))\naten_dialect = export(Foo(), example_args_3, strict=True)\nedge_program = to_edge(aten_dialect)\nexported_program = edge_program.exported_program()\ndelegated_program = edge_program.to_backend(AddMulPartitionerDemo())\n\nprint(\"Delegated program\")\nprint(delegated_program.exported_program())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running User-Defined Passes and Memory Planning\n\nAs a final step of lowering, we can use the ``to_executorch()`` API to pass in\nbackend-specific passes, such as replacing sets of operators with a custom\nbackend operator, and a memory planning pass, to tell the runtime how to\nallocate memory ahead of time when running the program.\n\nA default memory planning pass is provided, but we can also choose a\nbackend-specific memory planning pass if it exists. More information on\nwriting a custom memory planning pass can be found\n[here](../compiler-memory-planning.html)_\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from executorch.exir import ExecutorchBackendConfig, ExecutorchProgramManager\nfrom executorch.exir.passes import MemoryPlanningPass\n\nexecutorch_program: ExecutorchProgramManager = edge_program.to_executorch(\n    ExecutorchBackendConfig(\n        passes=[],  # User-defined passes\n        memory_planning_pass=MemoryPlanningPass(),  # Default memory planning pass\n    )\n)\n\nprint(\"ExecuTorch Dialect\")\nprint(executorch_program.exported_program())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that in the graph we now see operators like ``torch.ops.aten.sub.out``\nand ``torch.ops.aten.div.out`` rather than ``torch.ops.aten.sub.Tensor`` and\n``torch.ops.aten.div.Tensor``.\n\nThis is because between running the backend passes and memory planning passes,\nto prepare the graph for memory planning, an out-variant pass is run on\nthe graph to convert all of the operators to their out variants. Instead of\nallocating returned tensors in the kernel implementations, an operator's\n``out`` variant will take in a prealloacated tensor to its out kwarg, and\nstore the result there, making it easier for memory planners to do tensor\nlifetime analysis.\n\nWe also insert ``alloc`` nodes into the graph containing calls to a special\n``executorch.exir.memory.alloc`` operator. This tells us how much memory is\nneeded to allocate each tensor output by the out-variant operator.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving to a File\n\nFinally, we can save the ExecuTorch Program to a file and load it to a device\nto be run.\n\nHere is an example for an entire end-to-end workflow:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.export import export, export, ExportedProgram\n\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\n\nexample_args = (torch.randn(3, 4),)\npre_autograd_aten_dialect = export(M(), example_args, strict=True).module()\n# Optionally do quantization:\n# pre_autograd_aten_dialect = convert_pt2e(prepare_pt2e(pre_autograd_aten_dialect, CustomBackendQuantizer))\naten_dialect = export(pre_autograd_aten_dialect, example_args, strict=True)\nedge_program = to_edge(aten_dialect)\n# Optionally do delegation:\n# edge_program = edge_program.to_backend(CustomBackendPartitioner)\nexecutorch_program = edge_program.to_executorch(\n    ExecutorchBackendConfig(\n        passes=[],  # User-defined passes\n    )\n)\n\nwith open(\"model.pte\", \"wb\") as file:\n    file.write(executorch_program.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we went over the APIs and steps required to lower a PyTorch\nprogram to a file that can be run on the ExecuTorch runtime.\n\n### Links Mentioned\n\n- [torch.export Documentation](https://pytorch.org/docs/2.1/export.html)_\n- [Quantization Documentation](https://pytorch.org/docs/main/quantization.html#prototype-pytorch-2-export-quantization)_\n- [IR Spec](../ir-exir.html)_\n- [Writing Compiler Passes + Partitioner Documentation](../compiler-custom-compiler-passes.html)_\n- [Backend Delegation Documentation](../compiler-delegate-and-partitioner.html)_\n- [Memory Planning Documentation](../compiler-memory-planning.html)_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}