from abc import ABC, abstractmethod
from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union

import torch
from executorch.devtools.backend_debug import get_delegation_info
from executorch.exir._warnings import experimental
from executorch.exir.program import (
    EdgeProgramManager,
    ExecutorchProgramManager,
    to_edge_transform_and_lower,
)
from executorch.exir.schema import Program
from executorch.runtime import Runtime, Verification
from tabulate import tabulate
from torch import nn
from torch.ao.quantization import allow_exported_model_train_eval
from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e
from torch.export import export_for_training, ExportedProgram
from torchao.quantization import quantize_
from torchao.utils import unwrap_tensor_subclass

from .recipe import ExportRecipe


class Stage(ABC):
    """
    Interface for a Stage in the ExecuTorch export pipeline.

    Each stage can be connected to other stages to form a pipeline.
    Stages have clear run and get_outputs functions to make the data flow explicit.
    Each stage implements its own run method with specific parameter names.
    """

    @property
    @abstractmethod
    def name(self) -> str:
        """
        Returns the name of this stage.
        """
        pass

    @abstractmethod
    def run(self, **kwargs) -> None:
        """
        Executes this stage with the given inputs.

        Each concrete stage class implements this method with specific parameter names.
        """
        pass

    @abstractmethod
    def get_outputs(self) -> Any:
        """
        Returns the outputs generated by this stage.

        Returns:
            The outputs of this stage, to be used as inputs for the next stage
        """
        pass


class ExportStage(Stage):
    """
    First stage: Export PyTorch model to ExportedProgram.
    """

    def __init__(
        self,
        pre_edge_transform_passes: Optional[
            Callable[[ExportedProgram], ExportedProgram]
        ] = None,
    ) -> None:
        self._exported_program: Dict[str, ExportedProgram] = {}
        self._pre_edge_transform_passes = pre_edge_transform_passes
        self._model_dict: Dict[str, nn.Module] = {}
        self._example_inputs_dict: Dict[str, List[tuple[torch.Tensor, ...]]] = {}
        self._dynamic_shapes_dict: Dict[str, Any] = {}

    @property
    def name(self) -> str:
        return "export"

    def run(
        self,
        models: Dict[str, Any],
        export_config: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> None:
        """
        Export PyTorch model to ExportedProgram.

        Args:
            models: Dictionary mapping method names to PyTorch models
            export_config: Configuration containing example inputs and dynamic shapes
            **kwargs: Additional keyword arguments (not used)
        """
        # Store inputs
        self._model_dict = models.get("model", {})

        if export_config is not None:
            self._example_inputs_dict = export_config.get("example_inputs", {})
            self._dynamic_shapes_dict = export_config.get("dynamic_shapes", {})

        # Check if we need to do export (if _exported_program is empty)
        if not self._exported_program:
            # Process inputs
            with torch.no_grad():
                for method_name, model in self._model_dict.items():
                    # Check if method_name exists in example_inputs
                    if method_name not in self._example_inputs_dict:
                        raise ValueError(
                            f"Example inputs for method {method_name} not found."
                        )

                    # Get dynamic shapes if available
                    dynamic_shapes = None
                    if method_name in self._dynamic_shapes_dict:
                        dynamic_shapes = self._dynamic_shapes_dict[method_name]

                    # Export the model
                    self._exported_program[method_name] = torch.export.export(
                        model,
                        self._example_inputs_dict[method_name][0],
                        dynamic_shapes=dynamic_shapes,
                    )

                    # Apply pre-edge transform passes if available
                    if self._pre_edge_transform_passes is not None:
                        self._exported_program[method_name] = (
                            self._pre_edge_transform_passes(
                                self._exported_program[method_name]
                            )
                        )

    def get_outputs(self) -> Dict[str, ExportedProgram]:
        """
        Returns the exported program dictionary.

        Returns:
            Dictionary mapping method names to exported programs
        """
        return self._exported_program


class EdgeTransformAndLowerStage(Stage):
    """
    Second stage: Transform and lower to EdgeProgramManager.
    """

    def __init__(
        self,
        partitioners: Optional[List[Any]] = None,
        transform_passes: Optional[Sequence[Callable[[Any], Optional[Any]]]] = None,
        compile_config: Optional[Any] = None,
    ) -> None:
        self._partitioners = partitioners
        self._transform_passes = transform_passes
        self._compile_config = compile_config
        self._edge_program_manager: Optional[EdgeProgramManager] = None
        self._delegation_info = None
        self._exported_program: Dict[str, ExportedProgram] = {}
        self._constant_methods = None

    @property
    def name(self) -> str:
        return "edge_transform_and_lower"

    def run(
        self,
        exported_programs: Dict[str, ExportedProgram],
        transform_config: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> None:
        """
        Transform and lower to EdgeProgramManager.

        Args:
            exported_programs: Dictionary mapping method names to exported programs
            transform_config: Configuration containing constant methods
            **kwargs: Additional keyword arguments (not used)
        """
        # Store inputs
        self._exported_program = exported_programs

        self._constant_methods = None
        if transform_config is not None:
            self._constant_methods = transform_config.get("constant_methods", None)

        # Process inputs
        self._edge_program_manager = to_edge_transform_and_lower(
            self._exported_program,
            partitioner=self._partitioners,
            transform_passes=self._transform_passes,
            constant_methods=self._constant_methods,
            compile_config=self._compile_config,
        )
        self._delegation_info = get_delegation_info(
            self._edge_program_manager.exported_program().graph_module
        )

    def get_outputs(self) -> EdgeProgramManager:
        """
        Returns the edge program manager.

        Returns:
            The edge program manager

        Raises:
            RuntimeError: If the edge program manager is not initialized
        """
        if self._edge_program_manager is None:
            raise RuntimeError("Edge program manager is not initialized.")
        return self._edge_program_manager

    @property
    def delegation_info(self) -> Any:
        """
        Returns the delegation info.
        """
        return self._delegation_info


class ExecutorchStage(Stage):
    """
    Third stage: Convert to ExecutorchProgramManager.
    """

    def __init__(self, backend_config: Any) -> None:
        self._backend_config = backend_config
        self._executorch_program_manager: Optional[ExecutorchProgramManager] = None
        self._edge_program_manager: Optional[EdgeProgramManager] = None

    @property
    def name(self) -> str:
        return "executorch"

    def run(
        self,
        edge_program: EdgeProgramManager,
        backend_options: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> None:
        """
        Convert to ExecutorchProgramManager.

        Args:
            edge_program: Edge program manager containing the lowered program
            backend_options: Additional backend-specific options (not used in this stage)
            **kwargs: Additional keyword arguments (not used)
        """
        # Store inputs
        self._edge_program_manager = edge_program

        # Process inputs
        if self._edge_program_manager is None:
            raise RuntimeError("Edge program manager is not set.")

        self._executorch_program_manager = self._edge_program_manager.to_executorch(
            self._backend_config
        )

    def get_outputs(self) -> ExecutorchProgramManager:
        """
        Returns the executorch program manager.

        Returns:
            The executorch program manager

        Raises:
            RuntimeError: If the executorch program manager is not initialized
        """
        if self._executorch_program_manager is None:
            raise RuntimeError("Executorch program manager is not initialized.")
        return self._executorch_program_manager


class SourceTransformStage(Stage):
    """
    Source transform stage: Apply source transformations to the model.
    """

    def __init__(self, quantization_recipe: Any) -> None:
        self._quantization_recipe = quantization_recipe
        self._transformed_models: Dict[str, nn.Module] = {}

    @property
    def name(self) -> str:
        return "source_transform"

    def run(self, models: Dict[str, nn.Module], *args, **kwargs) -> None:
        """
        Apply source transformations to the model.

        Args:
            models: Dictionary mapping method names to PyTorch models
            **kwargs: Additional keyword arguments (not used)
        """
        # Store the original models
        self._transformed_models = models

        # Check if there's a quantization recipe with ao_base_config
        if self._quantization_recipe and self._quantization_recipe.ao_base_config:
            # Apply torchao quantize_ to each model
            for method_name, model in models.items():
                for config in self._quantization_recipe.ao_base_config:
                    quantize_(model, config)
                    unwrap_tensor_subclass(model)
                    self._transformed_models[method_name] = model

    def get_outputs(self) -> Dict[str, nn.Module]:
        """
        Returns the transformed models.

        Returns:
            Dictionary mapping method names to transformed models
        """
        return self._transformed_models


class QuantizeStage(Stage):
    """
    Optional stage: Perform post-training quantization on the model.
    """

    def __init__(self, quantizer: Any) -> None:
        self._quantizer = quantizer
        self._quantized_models: Dict[str, nn.Module] = {}
        self._model_dict: Dict[str, nn.Module] = {}
        self._exported_models_dict: Dict[str, nn.Module] = {}
        self._example_inputs_dict: Dict[str, List[tuple[torch.Tensor, ...]]] = {}

    @property
    def name(self) -> str:
        return "quantize"

    def run(
        self,
        model_data: Dict[str, Any],
        calibration_config: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> None:
        """
        Perform post-training quantization on the model.

        Args:
            model_data: Dictionary containing models and exported models
            calibration_config: Configuration containing example inputs for calibration
            **kwargs: Additional keyword arguments (not used)
        """
        # Store inputs
        self._model_dict = model_data["model"]
        self._exported_models_dict = {}

        # Initialize with empty dictionaries
        self._example_inputs_dict = {}
        dynamic_shapes_dict = {}

        if calibration_config is not None:
            self._example_inputs_dict = calibration_config.get("example_inputs", {})
            dynamic_shapes_dict = calibration_config.get("dynamic_shapes", {})

        # Export models for training to enable quantization
        for method_name, model in self._model_dict.items():
            dynamic_shapes = None
            if method_name in dynamic_shapes_dict:
                dynamic_shapes = dynamic_shapes_dict[method_name]

            # Check if method_name exists in example_inputs and has at least one element
            if (
                method_name not in self._example_inputs_dict
                or not self._example_inputs_dict[method_name]
            ):
                raise ValueError(
                    f"Example inputs for method {method_name} not found or empty."
                )

            # Set model to evaluation mode for quantization
            model.eval()

            # Export the model for training
            kwargs = {}
            if dynamic_shapes is not None:
                kwargs["dynamic_shapes"] = dynamic_shapes

            self._exported_models_dict[method_name] = export_for_training(
                model, self._example_inputs_dict[method_name][0], **kwargs
            ).module()

        # Process inputs
        for method_name, _ in self._model_dict.items():
            # Use the pre-exported model
            captured_model = self._exported_models_dict[method_name]

            # Prepare the model for quantization
            prepared_model = prepare_pt2e(captured_model, self._quantizer)  # type: ignore

            # Allow the exported model to switch between train and eval modes
            allow_exported_model_train_eval(prepared_model)

            # Calibrate the model with the provided calibration data
            for calibration_input in self._example_inputs_dict[method_name]:  # type: ignore
                prepared_model(*calibration_input)

            # Convert the prepared model to a quantized model
            quantized_model = convert_pt2e(prepared_model)
            self._quantized_models[method_name] = quantized_model  # type: ignore

    def get_outputs(self) -> Dict[str, nn.Module]:
        """
        Returns the quantized models.

        Returns:
            Dictionary mapping method names to quantized models
        """
        return self._quantized_models


@experimental(
    "This API and all of its related functionality such as ExportSession and ExportRecipe are experimental."
)
def export(
    model: Union[nn.Module, Dict[str, nn.Module]],
    example_inputs: Union[
        List[tuple[torch.Tensor, ...]], Dict[str, List[tuple[torch.Tensor, ...]]]
    ],
    export_recipe: ExportRecipe,
    name: Optional[str] = None,
    dynamic_shapes: Optional[Union[Any, Dict[str, Any]]] = None,
    constant_methods: Optional[Union[Dict[str, Callable]]] = None,
    artifact_dir: Optional[str] = None,
) -> "ExportSession":
    """
    Create and configure an ExportSession with the given parameters.

    This function provides a convenient way to create an ExportSession and
    optionally run the export process in one step.

    Args:
        model: The PyTorch model(s) to export, either a single model or a dictionary
              mapping method names to models
        example_inputs: Example inputs for the model(s), either a list of input tuples
                      or a dictionary mapping method names to lists of input tuples
        export_recipe: Contains the configuration for the export process
        name: Optional name for the export
        dynamic_shapes: Optional dynamic shape specifications
        constant_methods: Optional dictionary of constant methods
        artifact_dir: Optional directory to store artifacts

    Returns:
        A configured ExportSession instance with the export process completed if requested
    """
    session = ExportSession(
        model=model,
        example_inputs=example_inputs,
        export_recipe=export_recipe,
        name=name,
        dynamic_shapes=dynamic_shapes,
        constant_methods=constant_methods,
        artifact_dir=artifact_dir,
    )
    session.export()

    return session


@experimental(
    "This API and all of its related functionality such as ExportSession and ExportRecipe are experimental."
)
class ExportSession:
    """
    Manages the export process for ExecuTorch models.

    This class handles the export process through a pipeline of stages:
    1. (Optional) Quantize - Apply post-training quantization to the model
    2. Export - Export PyTorch model to ExportedProgram
    3. EdgeTransformAndLower - Transform and lower to EdgeProgramManager
    4. Executorch - Convert to ExecutorchProgramManager for final execution
    """

    def __init__(
        self,
        model: Union[nn.Module, Dict[str, nn.Module]],
        example_inputs: Union[
            List[tuple[torch.Tensor, ...]], Dict[str, List[tuple[torch.Tensor, ...]]]
        ],
        export_recipe: ExportRecipe,
        name: Optional[str] = None,
        dynamic_shapes: Optional[Union[Any, Dict[str, Any]]] = None,
        constant_methods: Optional[Union[Dict[str, Callable]]] = None,
        artifact_dir: Optional[str] = None,
    ) -> None:
        """
        Initialize the ExportSession with model, inputs, and recipe.

        Args:
            model: The PyTorch model(s) to export, either a single model or a dictionary
                  mapping method names to models
            example_inputs: Example inputs for the model(s), either a list of input tuples
                          or a dictionary mapping method names to lists of input tuples
            export_recipe: Contains the configuration for the export process
            name: Optional name for the export
            dynamic_shapes: Optional dynamic shape specifications
            constant_methods: Optional dictionary of constant methods
            artifact_dir: Optional directory to store artifacts
        """
        # Standardize model to dictionary format
        self._model = model if isinstance(model, dict) else {"forward": model}

        # Standardize example_inputs to dictionary format
        self._example_inputs = (
            example_inputs
            if isinstance(example_inputs, dict)
            else {"forward": example_inputs}
        )

        # Standardize dynamic_shapes to dictionary format
        self._dynamic_shapes = {}
        if dynamic_shapes is not None:
            if isinstance(dynamic_shapes, dict):
                self._dynamic_shapes = dynamic_shapes
            else:
                self._dynamic_shapes = {"forward": dynamic_shapes}

        self._name = name
        self._constant_methods = constant_methods
        self._artifact_dir = artifact_dir
        self._export_recipe = export_recipe

        # Initialize stages
        self._stages = {}

        # Create the source transform stage if a quantization recipe is provided
        if self._export_recipe.quantization_recipe is not None:
            self._stages["source_transform"] = SourceTransformStage(
                quantization_recipe=self._export_recipe.quantization_recipe
            )

            # Create the quantize stage if a quantizer is provided
            if self._export_recipe.quantization_recipe is not None:
                quantizer = self._export_recipe.quantization_recipe.get_quantizer()
                if quantizer is not None:
                    self._stages["quantize"] = QuantizeStage(quantizer=quantizer)

        # Create the export stage
        self._stages["export"] = ExportStage(
            pre_edge_transform_passes=self._export_recipe.pre_edge_transform_passes
        )

        # Create the edge transform and lower stage
        self._stages["edge_transform_and_lower"] = EdgeTransformAndLowerStage(
            partitioners=self._export_recipe.partitioners,
            transform_passes=self._export_recipe.edge_transform_passes,
            compile_config=self._export_recipe.edge_compile_config,
        )

        # Create the executorch stage
        self._stages["executorch"] = ExecutorchStage(
            backend_config=self._export_recipe.executorch_backend_config
        )

        # Define the pipeline as a list of stages in execution order
        self._pipeline = ["export", "edge_transform_and_lower", "executorch"]

        # If quantization is available, add it after source transform
        if "quantize" in self._stages:
            self._pipeline.insert(0, "quantize")

        # If source transform is available, add it at the beginning
        if "source_transform" in self._stages:
            self._pipeline.insert(0, "source_transform")

        # Current stage index
        self._current_stage_index = -1

        # Initialize stage artifacts
        self._exported_models: Dict[str, nn.Module] = {}

        # Initialize stage artifacts
        self._exported_program: Dict[str, ExportedProgram] = {}
        self._edge_program_manager: Optional[EdgeProgramManager] = None
        self._executorch_program_manager: Optional[ExecutorchProgramManager] = None
        self._delegation_info = None

    def _run_stage(
        self,
        stage_name: str,
        primary_input: Any,
        config_params: Optional[Dict[str, Any]] = None,
    ) -> Any:
        """
        Run a specific stage in the pipeline.

        Args:
            stage_name: Name of the stage to run
            primary_input: Primary input for the stage (passed to the stage's specific first parameter)
            config_params: Configuration parameters for the stage (passed to the stage's specific second parameter)

        Returns:
            The outputs produced by the stage

        Raises:
            ValueError: If the stage is not found
        """
        if stage_name not in self._stages:
            raise ValueError(f"Stage '{stage_name}' not found")

        # Run the stage with the inputs
        # Note: Each stage has specific parameter names in its run method:
        # - export: models, export_config
        # - edge_transform_and_lower: exported_programs, transform_config
        # - executorch: edge_program, backend_options
        # - quantize: model_data, calibration_config
        # But we use generic parameter names here for simplicity
        self._stages[stage_name].run(primary_input, config_params)

        # Return the outputs
        return self._stages[stage_name].get_outputs()

    def _get_stage_inputs(self, stage_name: str) -> tuple[Any, Dict[str, Any]]:
        """
        Get all inputs for a specific stage.

        Args:
            stage_name: Name of the stage to get inputs for

        Returns:
            A tuple containing:
            - The primary input for the stage
            - Dictionary of configuration parameters for the stage
        """
        if stage_name == "source_transform":
            # For SourceTransformStage.run(models, **kwargs)
            return self._model, {}

        elif stage_name == "quantize":
            # For QuantizeStage.run(model_data, calibration_config)
            model_data = {
                "model": self._model,
            }
            calibration_config = {
                "example_inputs": self._example_inputs,
            }
            return model_data, calibration_config

        elif stage_name == "export":
            # For ExportStage.run(models, export_config)
            models = {
                "model": self._model,
            }
            export_config = {
                "example_inputs": self._example_inputs,
                "dynamic_shapes": self._dynamic_shapes,
            }
            return models, export_config

        elif stage_name == "edge_transform_and_lower":
            # For EdgeTransformAndLowerStage.run(exported_programs, transform_config)
            exported_programs = self._exported_program
            transform_config = {
                "constant_methods": self._constant_methods,
            }
            return exported_programs, transform_config

        elif stage_name == "executorch":
            # For ExecutorchStage.run(edge_program, backend_options)
            edge_program = self._edge_program_manager
            backend_options = {}
            return edge_program, backend_options

        else:
            return None, {}

    def _store_stage_result(self, stage_name: str, result: Any) -> None:
        """
        Store the result of a stage.

        Args:
            stage_name: Name of the stage
            result: Result of the stage
        """
        if stage_name == "source_transform":
            self._model = result
        elif stage_name == "quantize":
            self._model = result
        elif stage_name == "export":
            self._exported_program = result
        elif stage_name == "edge_transform_and_lower":
            self._edge_program_manager = result
            self._delegation_info = self._stages[stage_name].delegation_info
        elif stage_name == "executorch":
            self._executorch_program_manager = result

    def _run_pipeline(self, start_stage: str) -> None:
        """
        Run the pipeline starting from a specific stage.

        This method cascades through the pipeline of stages, starting from the specified stage.
        Each stage's artifact is passed as input to the next stage.

        Args:
            start_stage: Name of the stage to start from

        Raises:
            ValueError: If the stage is not found
        """
        if start_stage not in self._stages:
            raise ValueError(f"Stage '{start_stage}' not found")

        # Find the starting index in the pipeline
        try:
            start_index = self._pipeline.index(start_stage)
        except ValueError:
            raise ValueError(f"Stage '{start_stage}' not found in pipeline")

        # Set the current stage index
        self._current_stage_index = start_index

        # Process each stage in the pipeline
        for i in range(start_index, len(self._pipeline)):
            stage_name = self._pipeline[i]
            self._current_stage_index = i

            # Get the primary input and configuration parameters for this stage
            primary_input, config_params = self._get_stage_inputs(stage_name)

            # Run the stage
            result = self._run_stage(stage_name, primary_input, config_params)

            # Store the result
            self._store_stage_result(stage_name, result)

    def export(self) -> None:
        """
        Execute the full export process.

        This method orchestrates the export process with optional quantization:
        1. (Optional) Apply quantization to the model
        2. Export the PyTorch model to ExportedProgram
        3. Transform and lower to EdgeProgramManager
        4. Convert to ExecutorchProgramManager
        """
        # Start with source transform stage if available
        if "source_transform" in self._stages:
            self._run_pipeline("source_transform")
        # Otherwise start with quantize stage if available
        elif "quantize" in self._stages:
            self._run_pipeline("quantize")
        # Otherwise start with export stage
        else:
            self._run_pipeline("export")

    def save_pte_file(self, path: str) -> None:
        """
        Save the exported program to a PTE file.

        Args:
            path: Path where the PTE file will be saved

        Raises:
            RuntimeError: If the executorch program manager is not initialized
        """
        if self._executorch_program_manager is None:
            raise RuntimeError(
                "Executorch program manager is not initialized. Run export() first."
            )
        self._executorch_program_manager.save(path)

    def get_executorch_program(self) -> Program:
        """
        Get the ExecutorchProgram from the ExecutorchProgramManager.

        Returns:
            The ExecutorchProgram

        Raises:
            RuntimeError: If the executorch program manager is not initialized
        """
        if self._executorch_program_manager is None:
            raise RuntimeError(
                "Executorch program manager is not initialized. Run export() first."
            )
        return self._executorch_program_manager.executorch_program

    def get_pte_buffer(self) -> bytes:
        """
        Get the PTE buffer as bytes.

        Returns:
            The PTE buffer as bytes

        Raises:
            RuntimeError: If the executorch program manager is not initialized
        """
        if self._executorch_program_manager is None:
            raise RuntimeError(
                "Executorch program manager is not initialized. Run export() first."
            )
        return self._executorch_program_manager.buffer

    def get_example_input(
        self, method_name: str = "forward"
    ) -> Tuple[torch.Tensor, ...]:
        """
        Get the example input for a specific method.

        Args:
            method_name: Name of the method to get example input for, defaults to "forward"

        Returns:
            Tuple of tensors representing the example input

        Raises:
            KeyError: If the method name is not found in example inputs
            ValueError: If the example inputs list is empty
        """
        if method_name not in self._example_inputs:
            raise KeyError(f"Method name '{method_name}' not found in example inputs")

        # Access the first element of the list for this method
        example_inputs_list = self._example_inputs[method_name]
        if not example_inputs_list:
            raise ValueError(f"Example inputs list for method {method_name} is empty")

        # The original code expects this to be a tuple of tensors
        return self._example_inputs[method_name][0]

    def run_method(
        self,
        method_name: str = "forward",
        example_inputs: Optional[Tuple[torch.Tensor, ...]] = None,
    ) -> Sequence[Any]:
        """
        Run a specific method with the given inputs.

        Args:
            method_name: Name of the method to run, defaults to "forward"
            example_inputs: Optional inputs to use, defaults to the example inputs

        Returns:
            The outputs of the method execution

        Raises:
            RuntimeError: If the method cannot be loaded
        """
        et_runtime = Runtime.get()
        program = et_runtime.load_program(
            self.get_pte_buffer(), verification=Verification.Minimal
        )
        forward = program.load_method(method_name)

        if forward is None:
            raise RuntimeError(
                f"Failed to load method '{method_name}' from the program"
            )
        if example_inputs is None:
            example_inputs = self.get_example_input(method_name)

        return forward.execute(example_inputs)

    def print_delegation_info(self) -> None:
        """
        Print delegation information for the exported program.
        """
        print(self._delegation_info.get_summary())
        df = self._delegation_info.get_operator_delegation_dataframe()
        print(tabulate(df, headers="keys", tablefmt="fancy_grid"))
