
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>executorch.exir.lowered_backend_module &#8212; ExecuTorch 1.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/executorch/exir/lowered_backend_module';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.pytorch.org/executorch/executorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.0';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="canonical" href="https://docs.pytorch.org/executorch/_modules/executorch/exir/lowered_backend_module.html" />
    <link rel="icon" href="../../../_static/executorch-chip-logo.svg"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../../../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', '1.0');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

<!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
<meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/executorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/et-logo.png" class="logo__image only-light" alt="ExecuTorch 1.0 documentation - Home"/>
    <script>document.write(`<img src="../../../_static/et-logo.png" class="logo__image only-dark" alt="ExecuTorch 1.0 documentation - Home"/>`);</script>
  
  
</a></div>
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../intro-section.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quick-start-section.html">
    Quick Start
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../edge-platforms-section.html">
    Edge
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../backends-section.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../llm/working-with-llms.html">
    LLMs
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../advanced-topics-section.html">
    Advanced
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tools-section.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api-section.html">
    API
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../support-section.html">
    Support
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../intro-section.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quick-start-section.html">
    Quick Start
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../edge-platforms-section.html">
    Edge
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../backends-section.html">
    Backends
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../llm/working-with-llms.html">
    LLMs
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../advanced-topics-section.html">
    Advanced
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tools-section.html">
    Tools
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api-section.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../support-section.html">
    Support
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/executorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/executorch" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">executorch.e...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../index.html">
        <meta itemprop="name" content="Module code">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="executorch.exir.lowered_backend_module">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for executorch.exir.lowered_backend_module</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="c1"># pyre-strict</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">operator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils._pytree</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pytree</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir._serialize</span><span class="w"> </span><span class="kn">import</span> <span class="n">_serialize_pte_binary</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir._serialize._named_data_store</span><span class="w"> </span><span class="kn">import</span> <span class="n">NamedDataStoreOutput</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.backend.compile_spec_schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompileSpec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.delegate</span><span class="w"> </span><span class="kn">import</span> <span class="n">executorch_call_delegate</span><span class="p">,</span> <span class="n">get_lowered_module_name</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.emit</span><span class="w"> </span><span class="kn">import</span> <span class="n">emit_program</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.graph_module</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_submodule</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.passes.memory_planning_pass</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemoryPlanningPass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.passes.spec_prop_pass</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_spec</span><span class="p">,</span> <span class="n">SpecPropPass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">Program</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.tracer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Value</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._library.fake_class_registry</span><span class="w"> </span><span class="kn">import</span> <span class="n">FakeScriptObject</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch._subclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">FakeTensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.export.exported_program</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ConstantArgument</span><span class="p">,</span>
    <span class="n">ExportedProgram</span><span class="p">,</span>
    <span class="n">ExportGraphSignature</span><span class="p">,</span>
    <span class="n">InputKind</span><span class="p">,</span>
    <span class="n">InputSpec</span><span class="p">,</span>
    <span class="n">ModuleCallEntry</span><span class="p">,</span>
    <span class="n">ModuleCallSignature</span><span class="p">,</span>
    <span class="n">OutputKind</span><span class="p">,</span>
    <span class="n">OutputSpec</span><span class="p">,</span>
    <span class="n">TensorArgument</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.fx.passes.utils.fuser_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">erase_nodes</span><span class="p">,</span>
    <span class="n">fuse_as_graphmodule</span><span class="p">,</span>
    <span class="n">insert_subgm</span><span class="p">,</span>
    <span class="n">legalize_graph</span><span class="p">,</span>
    <span class="n">NodeList</span><span class="p">,</span>
    <span class="n">topo_sort</span><span class="p">,</span>
<span class="p">)</span>


<div class="viewcode-block" id="LoweredBackendModule">
<a class="viewcode-back" href="../../../export-to-executorch-api-reference.html#executorch.exir.backend.backend_api.LoweredBackendModule">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">LoweredBackendModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A subclass of nn.Module that is generated for modules containing</span>
<span class="sd">    delegated functions. This is can be created by calling `to_backend`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_backend_id</span><span class="p">:</span> <span class="nb">str</span>  <span class="c1"># The backend&#39;s name</span>
    <span class="n">_processed_bytes</span><span class="p">:</span> <span class="nb">bytes</span>  <span class="c1"># The delegate blobs created from backend.preprocess</span>
    <span class="n">_compile_specs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span>
        <span class="n">CompileSpec</span>
    <span class="p">]</span>  <span class="c1"># A list of backend-specific objects with static metadata to configure the &quot;compilation&quot; process.</span>
    <span class="n">_original_exported_program</span><span class="p">:</span> <span class="n">ExportedProgram</span>  <span class="c1"># The original EXIR module</span>
    <span class="n">_named_data_store_output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">NamedDataStoreOutput</span>
    <span class="p">]</span>  <span class="c1"># Named Data serialized by the backend</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span>  <span class="c1"># Metadata for the lowered module</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">edge_program</span><span class="p">:</span> <span class="n">ExportedProgram</span><span class="p">,</span>
        <span class="n">backend_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">processed_bytes</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">,</span>
        <span class="n">compile_specs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">CompileSpec</span><span class="p">],</span>
        <span class="n">named_data_store_output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">NamedDataStoreOutput</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span> <span class="o">=</span> <span class="n">edge_program</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backend_id</span> <span class="o">=</span> <span class="n">backend_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_processed_bytes</span> <span class="o">=</span> <span class="n">processed_bytes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compile_specs</span> <span class="o">=</span> <span class="n">compile_specs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_named_data_store_output</span> <span class="o">=</span> <span class="n">named_data_store_output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">meta</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># pyre-ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="s2">&quot;LoweredBackendModule&quot;</span><span class="p">:</span>
        <span class="c1"># Copy exported program</span>
        <span class="n">copied_program</span> <span class="o">=</span> <span class="n">ExportedProgram</span><span class="p">(</span>
            <span class="n">root</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="o">.</span><span class="n">graph_module</span><span class="p">),</span>
            <span class="n">graph</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="p">),</span>
            <span class="n">graph_signature</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="o">.</span><span class="n">graph_signature</span>
            <span class="p">),</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="o">.</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">range_constraints</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="o">.</span><span class="n">range_constraints</span>
            <span class="p">),</span>
            <span class="n">module_call_graph</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="o">.</span><span class="n">module_call_graph</span>
            <span class="p">),</span>
            <span class="n">constants</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="o">.</span><span class="n">constants</span><span class="p">,</span>
            <span class="n">verifiers</span><span class="o">=</span><span class="p">[</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="o">.</span><span class="n">verifier</span><span class="p">)],</span>
        <span class="p">)</span>

        <span class="n">res</span> <span class="o">=</span> <span class="n">LoweredBackendModule</span><span class="p">(</span>
            <span class="n">edge_program</span><span class="o">=</span><span class="n">copied_program</span><span class="p">,</span>
            <span class="n">backend_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_backend_id</span><span class="p">,</span>
            <span class="n">processed_bytes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_processed_bytes</span><span class="p">,</span>
            <span class="n">compile_specs</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_compile_specs</span><span class="p">,</span> <span class="n">memo</span><span class="p">),</span>
            <span class="n">named_data_store_output</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_named_data_store_output</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">res</span><span class="o">.</span><span class="n">meta</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">,</span> <span class="p">{}))</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backend_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the backends name.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_id</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">processed_bytes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bytes</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the delegate blob created from backend.preprocess</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processed_bytes</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">compile_specs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">CompileSpec</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of backend-specific objects with static metadata to configure the &quot;compilation&quot; process.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile_specs</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">original_module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ExportedProgram</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the original EXIR module</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">named_data_store_output</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">NamedDataStoreOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the Named Data Store Output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_data_store_output</span>

    <span class="c1"># TODO(chenlai): consolidate the seriailization config with serialize_to_flatbuffer api</span>
<div class="viewcode-block" id="LoweredBackendModule.buffer">
<a class="viewcode-back" href="../../../export-to-executorch-api-reference.html#executorch.exir.backend.backend_api.LoweredBackendModule.buffer">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">buffer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">extract_delegate_segments</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">segment_alignment</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">constant_tensor_alignment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">delegate_alignment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">memory_planning</span><span class="p">:</span> <span class="n">MemoryPlanningPass</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># pyre-fixme[9]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bytes</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a buffer containing the serialized ExecuTorch binary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO(T181463742): avoid calling bytes(..) which incurs large copies.</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nb">bytes</span><span class="p">(</span>
            <span class="n">_serialize_pte_binary</span><span class="p">(</span>
                <span class="n">program</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">program</span><span class="p">(</span><span class="n">memory_planning</span><span class="o">=</span><span class="n">memory_planning</span><span class="p">),</span>
                <span class="n">extract_delegate_segments</span><span class="o">=</span><span class="n">extract_delegate_segments</span><span class="p">,</span>
                <span class="n">segment_alignment</span><span class="o">=</span><span class="n">segment_alignment</span><span class="p">,</span>
                <span class="n">constant_tensor_alignment</span><span class="o">=</span><span class="n">constant_tensor_alignment</span><span class="p">,</span>
                <span class="n">delegate_alignment</span><span class="o">=</span><span class="n">delegate_alignment</span><span class="p">,</span>
                <span class="n">named_data</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">named_data_store_output</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></div>


    <span class="c1"># TODO(chenlai): re-consider recapture instead of manually constructing the program because</span>
    <span class="c1"># the meta data construction is done manually.</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">program</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">emit_stacktrace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">memory_planning</span><span class="p">:</span> <span class="n">MemoryPlanningPass</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># pyre-fixme[9]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Program</span><span class="p">:</span>
        <span class="c1"># Fix autodpes introuces cyclic dependencies:</span>
        <span class="c1"># program -&gt; verifier -&gt; lowered_backend_module -&gt; program</span>
        <span class="c1"># @manual</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">executorch.exir.program._program</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
            <span class="n">_get_updated_graph_signature</span><span class="p">,</span>
            <span class="n">_transform</span><span class="p">,</span>
        <span class="p">)</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the object that represents the ExecuTorch binary before serialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Creates a new module based on the original module. The original module will</span>
        <span class="c1"># look something like following:</span>
        <span class="c1">#</span>
        <span class="c1"># opcode         name                 target            args                                        kwargs</span>
        <span class="c1"># -------------  -------------------  ----------------  ------------------------------------------  --------</span>
        <span class="c1"># placeholder    arg0_1               arg0_1            ()                                          {}</span>
        <span class="c1"># placeholder    arg1_1               arg1_1            ()                                          {}</span>
        <span class="c1"># call_function  aten_repeat_default  *                 (arg1_1, [4, 1])                            {}</span>
        <span class="c1"># call_function  aten_mul_tensor      *                 (aten_repeat_default, aten_repeat_default)  {}</span>
        <span class="c1"># call_function  aten_add_tensor      *                 (arg1_1, arg1_1)                            {}</span>
        <span class="c1"># output         output               output            ([aten_mul_tensor, aten_add_tensor],)       {}</span>
        <span class="c1">#</span>
        <span class="c1"># if the whole module is lowered, the resulting lowered module look like</span>
        <span class="c1">#</span>
        <span class="c1"># opcode         name                      target                       args                                kwargs</span>
        <span class="c1"># -------------  ------------------------  ---------------------------  ----------------------------------  --------</span>
        <span class="c1"># placeholder    arg0_1                    arg0_1                       ()                                  {}</span>
        <span class="c1"># placeholder    arg1_1                    arg1_1                       ()                                  {}</span>
        <span class="c1"># get_attr       lowered_module_0          lowered_module_0             ()                                  {}</span>
        <span class="c1"># call_function  executorch_call_delegate  executorch_call_delegate     (lowered_module_0, arg0_1, arg1_1)  {}</span>
        <span class="c1"># call_function  getitem                   &lt;built-in function getitem&gt;  (executorch_call_delegate, 0)       {}</span>
        <span class="c1"># call_function  getitem_1                 &lt;built-in function getitem&gt;  (executorch_call_delegate, 1)       {}</span>
        <span class="c1"># output         output_1                  output                       ([getitem, getitem_1],)             {}</span>
        <span class="c1">#</span>
        <span class="c1"># We&#39;ll remove all call_function nodes, insert an call_delegate node, inserting getitems nodes to get the result for call_delegate node</span>
        <span class="c1"># and return the list of getitems as the output</span>

        <span class="n">lowered_exported_program</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="p">)</span>

        <span class="c1"># The real input nodes are the ones not buffer or parameter</span>
        <span class="n">all_input_nodes</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">node</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span>
                <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span>
                <span class="ow">not</span> <span class="ow">in</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">inputs_to_buffers</span>
                <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span>
                <span class="ow">not</span> <span class="ow">in</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">inputs_to_parameters</span>
            <span class="p">)</span>
        <span class="p">]</span>

        <span class="n">output_node</span> <span class="o">=</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output_node</span><span class="p">()</span>

        <span class="c1"># Step 1. Cleaning up the graph before inserting the call_delegate node</span>
        <span class="c1"># Remove the original output node</span>
        <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">erase_node</span><span class="p">(</span><span class="n">output_node</span><span class="p">)</span>

        <span class="c1"># Remove all the everything else except the input</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">!=</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
                <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">erase_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="c1"># Find placeholders that are parameters or buffers, remove them from the main graph</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">inputs_to_buffers</span>
                <span class="ow">or</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span>
                <span class="ow">in</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">inputs_to_parameters</span>
            <span class="p">):</span>
                <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">erase_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="c1"># Step 2. Start constructing the graph</span>
        <span class="n">lowered_name</span> <span class="o">=</span> <span class="n">get_lowered_module_name</span><span class="p">(</span>
            <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_module</span><span class="p">,</span> <span class="bp">self</span>
        <span class="p">)</span>
        <span class="c1"># Insert the lowered module to the graph module as an attibute</span>
        <span class="n">lowered_node</span> <span class="o">=</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="n">lowered_name</span><span class="p">)</span>

        <span class="c1"># Insert a call_delegate node to the graph module, with arguments from the arg list</span>
        <span class="n">delegate_node</span> <span class="o">=</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span>
            <span class="n">executorch_call_delegate</span><span class="p">,</span> <span class="p">(</span><span class="n">lowered_node</span><span class="p">,</span> <span class="o">*</span><span class="n">all_input_nodes</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Get the output list. Since the output node is a tuple of list, like ([aten_mul_tensor, aten_add_tensor],)</span>
        <span class="c1"># We add some handling logic to get the list `[aten_mul_tensor, aten_add_tensor]` properly</span>
        <span class="n">original_output_nodes</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_original_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output_node</span><span class="p">()</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">delegate_node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;spec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="p">[</span><span class="n">make_spec</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">original_output_nodes</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">delegate_node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">original_output_nodes</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># The getitem nodes that are going to be inserted to the lowered graph module</span>
        <span class="n">getitem_nodes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">original_output_nodes</span><span class="p">)):</span>
            <span class="n">getitem_node</span> <span class="o">=</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span>
                <span class="n">operator</span><span class="o">.</span><span class="n">getitem</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">delegate_node</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">getitem_node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">delegate_node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
            <span class="n">getitem_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">getitem_node</span><span class="p">)</span>
        <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">getitem_nodes</span><span class="p">)</span>

        <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>
        <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">lint</span><span class="p">()</span>

        <span class="c1"># Users output will be the get items nodes instead</span>
        <span class="n">output_specs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">OutputSpec</span><span class="p">(</span>
                <span class="n">kind</span><span class="o">=</span><span class="n">OutputKind</span><span class="o">.</span><span class="n">USER_OUTPUT</span><span class="p">,</span>
                <span class="n">arg</span><span class="o">=</span><span class="n">TensorArgument</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">getitem_node</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">getitem_node</span> <span class="ow">in</span> <span class="n">getitem_nodes</span>
        <span class="p">]</span>
        <span class="c1"># All data are consumed by the delegates so they should be removed from the state dict.</span>
        <span class="n">inputs_to_parameters</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">inputs_to_parameters</span>
        <span class="p">)</span>
        <span class="n">inputs_to_buffers</span> <span class="o">=</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">inputs_to_buffers</span>
        <span class="n">input_specs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">InputSpec</span><span class="p">(</span>
                <span class="n">kind</span><span class="o">=</span><span class="n">InputKind</span><span class="o">.</span><span class="n">USER_INPUT</span><span class="p">,</span>
                <span class="n">arg</span><span class="o">=</span><span class="n">TensorArgument</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">user_input</span> <span class="ow">in</span> <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">user_inputs</span>
            <span class="k">if</span> <span class="n">user_input</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inputs_to_parameters</span>
            <span class="ow">and</span> <span class="n">user_input</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inputs_to_buffers</span>
        <span class="p">]</span>

        <span class="c1"># Double check the ExportedProgram data(especially everything except graph) is good</span>
        <span class="n">exported_program</span> <span class="o">=</span> <span class="n">ExportedProgram</span><span class="p">(</span>
            <span class="n">root</span><span class="o">=</span><span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_module</span><span class="p">,</span>
            <span class="n">graph</span><span class="o">=</span><span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span>
            <span class="n">graph_signature</span><span class="o">=</span><span class="n">_get_updated_graph_signature</span><span class="p">(</span>
                <span class="n">ExportGraphSignature</span><span class="p">(</span>
                    <span class="n">input_specs</span><span class="o">=</span><span class="n">input_specs</span><span class="p">,</span> <span class="n">output_specs</span><span class="o">=</span><span class="n">output_specs</span>
                <span class="p">),</span>
                <span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">graph_module</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="c1"># TODO: May need to set lowered_exported_program.call_spec = CallSpec(None, None)</span>
            <span class="c1"># somewhere as we should pass it a list of tensors to the lowered module and output a</span>
            <span class="c1"># list of tensors. Putting call_spec=lowered_exported_program.call_spec is correct here as the</span>
            <span class="c1"># inputs/outputs to the toplevel program will be in the format of the eager module.</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="p">{},</span>  <span class="c1"># None because all data are consumed by delegate</span>
            <span class="n">range_constraints</span><span class="o">=</span><span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">range_constraints</span><span class="p">,</span>
            <span class="n">module_call_graph</span><span class="o">=</span><span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">module_call_graph</span><span class="p">,</span>
            <span class="n">example_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">verifiers</span><span class="o">=</span><span class="p">[</span><span class="n">lowered_exported_program</span><span class="o">.</span><span class="n">verifier</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">memory_planning</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">memory_planning</span> <span class="o">=</span> <span class="n">MemoryPlanningPass</span><span class="p">()</span>
        <span class="n">exported_program</span> <span class="o">=</span> <span class="n">_transform</span><span class="p">(</span><span class="n">exported_program</span><span class="p">,</span> <span class="n">SpecPropPass</span><span class="p">(),</span> <span class="n">memory_planning</span><span class="p">)</span>
        <span class="n">emitted_program</span> <span class="o">=</span> <span class="n">emit_program</span><span class="p">(</span>
            <span class="n">exported_program</span><span class="p">,</span> <span class="n">emit_stacktrace</span><span class="o">=</span><span class="n">emit_stacktrace</span>
        <span class="p">)</span><span class="o">.</span><span class="n">program</span>
        <span class="k">return</span> <span class="n">emitted_program</span>

    <span class="c1"># Used to patch each delegated function with a call_delegate call</span>
    <span class="c1"># @staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Value</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Value</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Value</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">executorch_call_delegate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span></div>



<span class="c1"># TODO(zhxchen17) Try ExportPass</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_fixup_output_node</span><span class="p">(</span><span class="n">gm</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">inserting_before</span><span class="p">(</span><span class="n">node</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">):</span>
                    <span class="n">val</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                        <span class="c1"># If a list is returned, in some cases it is represented as a</span>
                        <span class="c1"># singular node, like `split_copy_tensor` but EXIR will return a</span>
                        <span class="c1"># opened-up list like `[getitem1, getitem2]`</span>
                        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Proxy</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">node</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">val</span><span class="p">))</span>
                        <span class="p">]</span>
            <span class="n">returns</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
            <span class="n">node</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">returns</span><span class="p">,)</span>
            <span class="k">return</span>


<span class="k">def</span><span class="w"> </span><span class="nf">arrange_graph_placeholders</span><span class="p">(</span>
    <span class="n">gm</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">owning_program</span><span class="p">:</span> <span class="n">ExportedProgram</span><span class="p">,</span> <span class="n">tag</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Modifies the graph of the given graphmodule with one that contains the same nodes as the original,</span>
<span class="sd">    but with placeholders in order of (Params + Buffers) (User Inputs)</span>

<span class="sd">    This is used by the delegate api which disturbs the placeholder ordering when creating a submodule</span>
<span class="sd">    from partitioned nodes</span>

<span class="sd">    Args:</span>
<span class="sd">        gm: The graph module that we want arranged</span>
<span class="sd">        owning_program: ExportedProgram that the submodule (gm) belongs to</span>

<span class="sd">    Returns:</span>
<span class="sd">        The graph module in-placed arranged</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">new_graph</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

    <span class="n">node_map</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># mapping of nodes from old graph to new graph</span>

    <span class="n">graph_sign</span> <span class="o">=</span> <span class="n">owning_program</span><span class="o">.</span><span class="n">graph_signature</span>

    <span class="c1"># Add all placeholders into the graph first:</span>
    <span class="n">param_nodes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">buffer_nodes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">input_nodes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">!=</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">graph_sign</span><span class="o">.</span><span class="n">inputs_to_parameters</span>
            <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;delegation_tag&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="n">tag</span>
        <span class="p">):</span>
            <span class="n">param_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">graph_sign</span><span class="o">.</span><span class="n">inputs_to_buffers</span>
            <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;delegation_tag&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="n">tag</span>
        <span class="p">):</span>
            <span class="n">buffer_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">param_node</span> <span class="ow">in</span> <span class="n">param_nodes</span><span class="p">:</span>
        <span class="n">new_node</span> <span class="o">=</span> <span class="n">new_graph</span><span class="o">.</span><span class="n">node_copy</span><span class="p">(</span><span class="n">param_node</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">node_map</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
        <span class="n">node_map</span><span class="p">[</span><span class="n">param_node</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_node</span>
    <span class="k">for</span> <span class="n">buffer_node</span> <span class="ow">in</span> <span class="n">buffer_nodes</span><span class="p">:</span>
        <span class="n">new_node</span> <span class="o">=</span> <span class="n">new_graph</span><span class="o">.</span><span class="n">node_copy</span><span class="p">(</span><span class="n">buffer_node</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">node_map</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
        <span class="n">node_map</span><span class="p">[</span><span class="n">buffer_node</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_node</span>
    <span class="k">for</span> <span class="n">input_node</span> <span class="ow">in</span> <span class="n">input_nodes</span><span class="p">:</span>
        <span class="n">new_node</span> <span class="o">=</span> <span class="n">new_graph</span><span class="o">.</span><span class="n">node_copy</span><span class="p">(</span><span class="n">input_node</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">node_map</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
        <span class="n">node_map</span><span class="p">[</span><span class="n">input_node</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_node</span>

    <span class="c1"># Now add all the other nodes in order</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="n">new_node</span> <span class="o">=</span> <span class="n">new_graph</span><span class="o">.</span><span class="n">node_copy</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">node_map</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
        <span class="n">node_map</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_node</span>

    <span class="c1"># lint to ensure correctness</span>
    <span class="n">new_graph</span><span class="o">.</span><span class="n">lint</span><span class="p">()</span>

    <span class="n">new_graph</span><span class="o">.</span><span class="n">_codegen</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_codegen</span>
    <span class="n">gm</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">new_graph</span>

    <span class="k">return</span> <span class="n">gm</span>


<span class="c1"># TODO Don&#39;t regenerate new signature manually.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_get_new_signature</span><span class="p">(</span>  <span class="c1"># noqa: C901</span>
    <span class="n">original_program</span><span class="p">:</span> <span class="n">ExportedProgram</span><span class="p">,</span>
    <span class="n">gm</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">call_module_node</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">,</span>
    <span class="n">tag</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">is_submodule</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
    <span class="n">ExportGraphSignature</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]],</span>
    <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">FakeScriptObject</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ScriptObject</span><span class="p">]],</span>
    <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">InputSpec</span><span class="p">],</span>
    <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OutputSpec</span><span class="p">],</span>
<span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        original_program: The original program that we are paritioning</span>
<span class="sd">        gm: The partitioned graph module.</span>
<span class="sd">        call_module_node: The node in the original program that is calling the</span>
<span class="sd">            partitioned graph module.</span>
<span class="sd">        tag: The tag being used for this partitioned submodule. This is used to</span>
<span class="sd">            tell if a particular parameter/buffer/constant node is being tagged,</span>
<span class="sd">            aka consumed by the delegate.</span>
<span class="sd">        is_submodule: True if we are currently partitioning inside of a</span>
<span class="sd">            submodule (like cond&#39;s submodule). If we are inside of a submodule,</span>
<span class="sd">            we do not care about consuming params/buffers.</span>

<span class="sd">    Returns:</span>

<span class="sd">        new_signature (ExportGraphSignature): The new signature for the</span>
<span class="sd">            partitioned graph module.</span>
<span class="sd">        new_state_dict (Dict[str, Union[torch.Tensor, torch.nn.Parameter]]): The</span>
<span class="sd">            new state dict containing the consumed params/buffers.</span>
<span class="sd">        new_constants (Dict[str, Union[torch.Tensor, FakeScriptObject,</span>
<span class="sd">            torch.ScriptObject]]): The new constants table containing the</span>
<span class="sd">            consumed constants .</span>
<span class="sd">        input_specs_to_delete (Dict[str, InputSpec]): The input specs that have</span>
<span class="sd">            been consumed by the delegate (param/buffer input nodes) and should</span>
<span class="sd">            be removed from the toplevel ExportedProgram.</span>
<span class="sd">        output_specs_to_delete (Dict[str, InputSpec]): The output specs that have</span>
<span class="sd">            been consumed by the delegate (buffer mutation nodes) and should be</span>
<span class="sd">            removed from the toplevel ExportedProgram.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">old_signature</span> <span class="o">=</span> <span class="n">original_program</span><span class="o">.</span><span class="n">graph_signature</span>

    <span class="n">input_specs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">output_specs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">input_specs_to_delete</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">output_specs_to_delete</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">new_state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">new_constants</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># If we are within a submodule, we do not need to care about consuming</span>
    <span class="c1"># parameter/buffers</span>
    <span class="n">input_node_to_sig</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">InputSpec</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">{</span><span class="n">input_spec</span><span class="o">.</span><span class="n">arg</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">input_spec</span> <span class="k">for</span> <span class="n">input_spec</span> <span class="ow">in</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">input_specs</span><span class="p">}</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_submodule</span>
        <span class="k">else</span> <span class="p">{}</span>
    <span class="p">)</span>

    <span class="n">toplevel_output_node_to_sig</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputSpec</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_submodule</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">output_spec</span> <span class="ow">in</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">output_specs</span><span class="p">:</span>
            <span class="n">toplevel_output_node_to_sig</span><span class="p">[</span><span class="n">output_spec</span><span class="o">.</span><span class="n">arg</span><span class="o">.</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_spec</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>

            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">input_node_to_sig</span><span class="p">:</span>
                <span class="n">input_specs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">InputSpec</span><span class="p">(</span>
                        <span class="n">kind</span><span class="o">=</span><span class="n">InputKind</span><span class="o">.</span><span class="n">USER_INPUT</span><span class="p">,</span>
                        <span class="n">arg</span><span class="o">=</span><span class="n">TensorArgument</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                        <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="k">continue</span>

            <span class="n">orig_input_spec</span> <span class="o">=</span> <span class="n">input_node_to_sig</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">orig_input_spec</span><span class="o">.</span><span class="n">arg</span><span class="p">,</span> <span class="n">TensorArgument</span><span class="p">):</span>
                <span class="n">input_specs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">orig_input_spec</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;delegation_tag&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="n">tag</span><span class="p">:</span>
                <span class="n">input_specs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">orig_input_spec</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">orig_input_spec</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">InputKind</span><span class="o">.</span><span class="n">USER_INPUT</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="c1"># The following input specs are all attributes that should be</span>
                <span class="c1"># consumed by the delegate, so we want to remove it from the</span>
                <span class="c1"># toplevel module input/output</span>
                <span class="n">input_specs_to_delete</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">orig_input_spec</span>

                <span class="n">input_target</span> <span class="o">=</span> <span class="n">orig_input_spec</span><span class="o">.</span><span class="n">target</span>
                <span class="k">if</span> <span class="n">input_target</span> <span class="ow">in</span> <span class="n">original_program</span><span class="o">.</span><span class="n">state_dict</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">orig_input_spec</span><span class="o">.</span><span class="n">kind</span> <span class="ow">in</span> <span class="p">(</span>
                        <span class="n">InputKind</span><span class="o">.</span><span class="n">PARAMETER</span><span class="p">,</span>
                        <span class="n">InputKind</span><span class="o">.</span><span class="n">BUFFER</span><span class="p">,</span>
                    <span class="p">)</span>

                    <span class="n">new_state_dict</span><span class="p">[</span><span class="n">input_target</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_program</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span>
                        <span class="n">input_target</span>
                    <span class="p">]</span>
                <span class="k">elif</span> <span class="n">input_target</span> <span class="ow">in</span> <span class="n">original_program</span><span class="o">.</span><span class="n">constants</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">orig_input_spec</span><span class="o">.</span><span class="n">kind</span> <span class="ow">in</span> <span class="p">(</span>
                        <span class="n">InputKind</span><span class="o">.</span><span class="n">CONSTANT_TENSOR</span><span class="p">,</span>
                        <span class="n">InputKind</span><span class="o">.</span><span class="n">CUSTOM_OBJ</span><span class="p">,</span>
                        <span class="n">InputKind</span><span class="o">.</span><span class="n">BUFFER</span><span class="p">,</span>
                    <span class="p">)</span>

                    <span class="n">new_constants</span><span class="p">[</span><span class="n">input_target</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_program</span><span class="o">.</span><span class="n">constants</span><span class="p">[</span>
                        <span class="n">input_target</span>
                    <span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid input spec </span><span class="si">{</span><span class="n">orig_input_spec</span><span class="si">}</span><span class="s2"> received&quot;</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_specs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">InputSpec</span><span class="p">(</span>
                        <span class="n">kind</span><span class="o">=</span><span class="n">InputKind</span><span class="o">.</span><span class="n">USER_INPUT</span><span class="p">,</span>
                        <span class="n">arg</span><span class="o">=</span><span class="n">TensorArgument</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                        <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span>
            <span class="n">buffer_mutation_idxs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputSpec</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">user</span> <span class="ow">in</span> <span class="n">call_module_node</span><span class="o">.</span><span class="n">users</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">user</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">toplevel_output_node_to_sig</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="n">user</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="n">user</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">operator</span><span class="o">.</span><span class="n">getitem</span>
                    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Invalid user </span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">, node.op is </span><span class="si">{</span><span class="n">user</span><span class="o">.</span><span class="n">op</span><span class="si">}</span><span class="s2"> and node.target is </span><span class="si">{</span><span class="n">user</span><span class="o">.</span><span class="n">target</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="n">getitem_idx</span> <span class="o">=</span> <span class="n">user</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                        <span class="n">getitem_idx</span><span class="p">,</span> <span class="nb">int</span>
                    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Invalid getitem type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">getitem_idx</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="n">buffer_mutation_idxs</span><span class="p">[</span><span class="n">getitem_idx</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                        <span class="n">toplevel_output_node_to_sig</span><span class="p">[</span><span class="n">user</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
                    <span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">output_node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">buffer_mutation_idxs</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_node</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">)</span>
                    <span class="n">orig_output_specs</span> <span class="o">=</span> <span class="n">buffer_mutation_idxs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

                    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                        <span class="n">orig_output_spec</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">OutputKind</span><span class="o">.</span><span class="n">BUFFER_MUTATION</span>
                        <span class="ow">and</span> <span class="p">(</span>
                            <span class="n">orig_output_spec</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">new_state_dict</span>
                            <span class="ow">or</span> <span class="n">orig_output_spec</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">new_constants</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">orig_output_spec</span> <span class="ow">in</span> <span class="n">orig_output_specs</span>
                    <span class="p">):</span>
                        <span class="c1"># If the delegate wants to consume the buffer, then the</span>
                        <span class="c1"># delegate should also consume the buffer mutation</span>
                        <span class="c1"># (output spec would be a BUFFER_MUTATION).  Otherwise</span>
                        <span class="c1"># the delegate will just return the result of the</span>
                        <span class="c1"># mutation as a USER_OUTPUT.</span>

                        <span class="n">orig_output_spec</span> <span class="o">=</span> <span class="p">[</span>
                            <span class="n">orig_output_spec</span>
                            <span class="k">for</span> <span class="n">orig_output_spec</span> <span class="ow">in</span> <span class="n">orig_output_specs</span>
                            <span class="k">if</span> <span class="n">orig_output_spec</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">OutputKind</span><span class="o">.</span><span class="n">BUFFER_MUTATION</span>
                            <span class="ow">and</span> <span class="p">(</span>
                                <span class="n">orig_output_spec</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">new_state_dict</span>
                                <span class="ow">or</span> <span class="n">orig_output_spec</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">new_constants</span>
                            <span class="p">)</span>
                        <span class="p">][</span><span class="mi">0</span><span class="p">]</span>

                        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">orig_output_specs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Constant </span><span class="si">{</span><span class="n">orig_output_spec</span><span class="o">.</span><span class="n">target</span><span class="si">}</span><span class="s2"> was tagged to be &quot;</span>
                            <span class="s2">&quot;consumed by the buffer, and was found to also contain &quot;</span>
                            <span class="s2">&quot;a buffer mutation. However this buffer mutation node &quot;</span>
                            <span class="s2">&quot;was found to also be used as other types of outputs &quot;</span>
                            <span class="s2">&quot;which is currently not supported. Please file an &quot;</span>
                            <span class="s2">&quot;issue on Github. </span><span class="se">\n\n</span><span class="s2">&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;The toplevel program: </span><span class="si">{</span><span class="n">original_program</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="n">output_specs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                            <span class="n">OutputSpec</span><span class="p">(</span>
                                <span class="n">kind</span><span class="o">=</span><span class="n">OutputKind</span><span class="o">.</span><span class="n">BUFFER_MUTATION</span><span class="p">,</span>
                                <span class="n">arg</span><span class="o">=</span><span class="n">TensorArgument</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">output_node</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                                <span class="n">target</span><span class="o">=</span><span class="n">orig_output_spec</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                        <span class="n">output_specs_to_delete</span><span class="p">[</span><span class="n">orig_output_spec</span><span class="o">.</span><span class="n">arg</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="n">orig_output_spec</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">output_specs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                            <span class="n">OutputSpec</span><span class="p">(</span>
                                <span class="n">kind</span><span class="o">=</span><span class="n">OutputKind</span><span class="o">.</span><span class="n">USER_OUTPUT</span><span class="p">,</span>
                                <span class="n">arg</span><span class="o">=</span><span class="n">TensorArgument</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">output_node</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                                <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="p">)</span>
                        <span class="p">)</span>

                <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_node</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">):</span>
                    <span class="n">output_specs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">OutputSpec</span><span class="p">(</span>
                            <span class="n">kind</span><span class="o">=</span><span class="n">OutputKind</span><span class="o">.</span><span class="n">USER_OUTPUT</span><span class="p">,</span>
                            <span class="n">arg</span><span class="o">=</span><span class="n">ConstantArgument</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">output_node</span><span class="p">),</span>
                            <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_specs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">OutputSpec</span><span class="p">(</span>
                            <span class="n">kind</span><span class="o">=</span><span class="n">OutputKind</span><span class="o">.</span><span class="n">USER_OUTPUT</span><span class="p">,</span>
                            <span class="n">arg</span><span class="o">=</span><span class="n">TensorArgument</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">output_node</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                            <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

    <span class="n">new_signature</span> <span class="o">=</span> <span class="n">ExportGraphSignature</span><span class="p">(</span>
        <span class="n">input_specs</span><span class="o">=</span><span class="n">input_specs</span><span class="p">,</span> <span class="n">output_specs</span><span class="o">=</span><span class="n">output_specs</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">new_signature</span><span class="p">,</span>
        <span class="n">new_state_dict</span><span class="p">,</span>
        <span class="n">new_constants</span><span class="p">,</span>
        <span class="n">input_specs_to_delete</span><span class="p">,</span>
        <span class="n">output_specs_to_delete</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">create_exported_program_from_submodule</span><span class="p">(</span>
    <span class="n">submodule</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">owning_program</span><span class="p">:</span> <span class="n">ExportedProgram</span><span class="p">,</span>
    <span class="n">tag</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">call_module_node</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">,</span>
    <span class="n">is_submodule</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ExportedProgram</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">InputSpec</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OutputSpec</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates an ExportedProgram from the given submodule using the parameters and buffers</span>
<span class="sd">    from the top-level owning program</span>

<span class="sd">    Args:</span>
<span class="sd">        submodule: submodule to create and exported program from</span>
<span class="sd">        owning_program: exported program containing the parameters and buffers used within</span>
<span class="sd">            the submodule</span>

<span class="sd">    Returns:</span>
<span class="sd">        The ExportedProgram created from submodule</span>
<span class="sd">        input_specs_to_delete (Dict[str, InputSpec]): The input specs that have</span>
<span class="sd">            been consumed by the delegate (param/buffer input nodes) and should</span>
<span class="sd">            be removed from the toplevel ExportedProgram.</span>
<span class="sd">        output_specs_to_delete (Dict[str, InputSpec]): The output specs that have</span>
<span class="sd">            been consumed by the delegate (buffer mutation nodes) and should be</span>
<span class="sd">            removed from the toplevel ExportedProgram.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Arrange the submodule&#39;s placeholders in order</span>
    <span class="n">submodule</span> <span class="o">=</span> <span class="n">arrange_graph_placeholders</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">owning_program</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span>

    <span class="c1"># TODO: we probably need to arrange the outputs wrt buffer mutations.</span>

    <span class="c1"># Get updated graph signature</span>
    <span class="p">(</span>
        <span class="n">subgraph_signature</span><span class="p">,</span>
        <span class="n">subgraph_state_dict</span><span class="p">,</span>
        <span class="n">subgraph_constants</span><span class="p">,</span>
        <span class="n">toplevel_input_specs_to_delete</span><span class="p">,</span>
        <span class="n">toplevel_output_specs_to_delete</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">=</span> <span class="n">_get_new_signature</span><span class="p">(</span>
        <span class="n">owning_program</span><span class="p">,</span> <span class="n">submodule</span><span class="p">,</span> <span class="n">call_module_node</span><span class="p">,</span> <span class="n">tag</span><span class="p">,</span> <span class="n">is_submodule</span>
    <span class="p">)</span>

    <span class="n">in_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="nb">tuple</span><span class="p">(</span><span class="n">subgraph_signature</span><span class="o">.</span><span class="n">user_inputs</span><span class="p">),</span> <span class="p">{}))[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">out_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">subgraph_signature</span><span class="o">.</span><span class="n">user_outputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">ExportedProgram</span><span class="p">(</span>
            <span class="n">root</span><span class="o">=</span><span class="n">submodule</span><span class="p">,</span>
            <span class="n">graph</span><span class="o">=</span><span class="n">submodule</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span>
            <span class="n">graph_signature</span><span class="o">=</span><span class="n">subgraph_signature</span><span class="p">,</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">subgraph_state_dict</span><span class="p">,</span>
            <span class="n">range_constraints</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">owning_program</span><span class="o">.</span><span class="n">range_constraints</span><span class="p">),</span>
            <span class="n">module_call_graph</span><span class="o">=</span><span class="p">[</span>
                <span class="n">ModuleCallEntry</span><span class="p">(</span>
                    <span class="s2">&quot;&quot;</span><span class="p">,</span>
                    <span class="n">ModuleCallSignature</span><span class="p">(</span>
                        <span class="n">inputs</span><span class="o">=</span><span class="p">[],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[],</span> <span class="n">in_spec</span><span class="o">=</span><span class="n">in_spec</span><span class="p">,</span> <span class="n">out_spec</span><span class="o">=</span><span class="n">out_spec</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="p">],</span>
            <span class="n">constants</span><span class="o">=</span><span class="n">subgraph_constants</span><span class="p">,</span>
            <span class="n">verifiers</span><span class="o">=</span><span class="p">[</span><span class="n">owning_program</span><span class="o">.</span><span class="n">verifier</span><span class="p">],</span>
        <span class="p">),</span>
        <span class="n">toplevel_input_specs_to_delete</span><span class="p">,</span>
        <span class="n">toplevel_output_specs_to_delete</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">create_submodule_from_nodes</span><span class="p">(</span>
    <span class="n">gm</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">node_list</span><span class="p">:</span> <span class="n">NodeList</span><span class="p">,</span>
    <span class="n">tag</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">skip_legalize_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Modifies the given graph module in-place to separate out the given nodes</span>
<span class="sd">    into a submodule. The given node_list should form a fully connected</span>
<span class="sd">    subgraph.</span>

<span class="sd">    Args:</span>
<span class="sd">        gm: The graph module that we want to partition</span>
<span class="sd">        node_list: A list of nodes that belong in the partition</span>

<span class="sd">    Returns:</span>
<span class="sd">        The submodule that has been partitioned, the call_module node in the</span>
<span class="sd">        toplevel graph module calling the submodule</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sorted_nodes</span> <span class="o">=</span> <span class="n">topo_sort</span><span class="p">(</span><span class="n">node_list</span><span class="p">)</span>

    <span class="n">submodule_name</span> <span class="o">=</span> <span class="s2">&quot;fused_&quot;</span> <span class="o">+</span> <span class="n">tag</span>
    <span class="n">sub_gm</span><span class="p">,</span> <span class="n">orig_inputs</span><span class="p">,</span> <span class="n">orig_outputs</span> <span class="o">=</span> <span class="n">fuse_as_graphmodule</span><span class="p">(</span>
        <span class="n">gm</span><span class="p">,</span> <span class="n">sorted_nodes</span><span class="p">,</span> <span class="n">submodule_name</span>
    <span class="p">)</span>

    <span class="n">_fixup_output_node</span><span class="p">(</span><span class="n">sub_gm</span><span class="p">)</span>

    <span class="n">gm</span> <span class="o">=</span> <span class="n">insert_subgm</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">sub_gm</span><span class="p">,</span> <span class="n">orig_inputs</span><span class="p">,</span> <span class="n">orig_outputs</span><span class="p">)</span>
    <span class="n">submodule_node</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">submodule_name</span><span class="p">:</span>
            <span class="n">submodule_node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">if</span> <span class="n">submodule_node</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The submodule created with nodes </span><span class="si">{</span><span class="n">node_list</span><span class="si">}</span><span class="s2"> did not form </span><span class="se">\</span>
<span class="s2">            one fully contained subgraph. Check that these nodes form a </span><span class="se">\</span>
<span class="s2">            fully contained graph. Partitioned graph: </span><span class="si">{</span><span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">orig_outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">orig_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">],</span> <span class="n">FakeTensor</span><span class="p">):</span>
        <span class="c1"># If the original output is a single tensor, it has been</span>
        <span class="c1"># pytree.tree_flatten-ed to be a singleton list, so we want to replace</span>
        <span class="c1"># all uses with a getitem call to the 0th index of the result</span>
        <span class="k">with</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">inserting_after</span><span class="p">(</span><span class="n">submodule_node</span><span class="p">):</span>
            <span class="n">proxy_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Proxy</span><span class="p">(</span><span class="n">submodule_node</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">node</span>  <span class="c1"># type: ignore[index]</span>
            <span class="n">submodule_node</span><span class="o">.</span><span class="n">replace_all_uses_with</span><span class="p">(</span><span class="n">proxy_out</span><span class="p">)</span>
            <span class="n">proxy_out</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">submodule_node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span>
            <span class="c1"># Reset the args since it was overwritten in the previous line</span>
            <span class="n">proxy_out</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">submodule_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># fuse_as_graphmodule will automatically propagate the metadata of the</span>
        <span class="c1"># partition&#39;s last node to the getitem nodes that appear after the</span>
        <span class="c1"># call_module node. However, in the case of delegation we do not want</span>
        <span class="c1"># these getitem nodes to contain irrelevant previous metadata</span>
        <span class="c1"># (ex. source_fn, # nn_module_stack)</span>
        <span class="k">for</span> <span class="n">user_node</span> <span class="ow">in</span> <span class="n">submodule_node</span><span class="o">.</span><span class="n">users</span><span class="p">:</span>
            <span class="n">user_node</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;nn_module_stack&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">user_node</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;source_fn_stack&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="n">erase_nodes</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">sorted_nodes</span><span class="p">)</span>

    <span class="c1"># Topological sort original gm with newly created sub_gm</span>
    <span class="c1"># TODO : T153794167 Get rid of support for skipping legalize graph in create_submodule_from_nodes</span>
    <span class="c1"># once we transition to using fuse_by_partitions.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_legalize_graph</span><span class="p">:</span>
        <span class="n">legalize_graph</span><span class="p">(</span><span class="n">gm</span><span class="p">)</span>

    <span class="c1"># Get the call_module node</span>
    <span class="n">submodule_node</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">submodule_name</span><span class="p">:</span>
            <span class="n">submodule_node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">if</span> <span class="n">submodule_node</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The submodule created with nodes </span><span class="si">{</span><span class="n">node_list</span><span class="si">}</span><span class="s2"> did not form </span><span class="se">\</span>
<span class="s2">            one fully contained subgraph. Check that these nodes form a </span><span class="se">\</span>
<span class="s2">            fully contained graph. Partitioned graph: </span><span class="si">{</span><span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">submodule_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;No submodule was created with the nodes </span><span class="si">{</span><span class="n">node_list</span><span class="si">}</span><span class="s2"> in the graph </span><span class="si">{</span><span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">return</span> <span class="n">sub_gm</span><span class="p">,</span> <span class="n">submodule_node</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_lowered_submodules</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">LoweredBackendModule</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a list of lowered modules that are in the given graph (does not look</span>
<span class="sd">    into submodules). Specifically, the returned value is a list containing a</span>
<span class="sd">    tuple of (name of the lowered module that&#39;s stored in the graph module, the</span>
<span class="sd">    lowered module itself, and the fx node that called this lowered module).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lowered_submodules</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">executorch_call_delegate</span><span class="p">:</span>
            <span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">node</span> <span class="o">=</span> <span class="n">_get_submodule</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">LoweredBackendModule</span><span class="p">)</span>
            <span class="n">lowered_submodules</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">node</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">lowered_submodules</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_lowered_backend_modules</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">LoweredBackendModule</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a list of exported programs which were lowered by backen delegates</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lowered_programs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">graph_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">executorch_call_delegate</span><span class="p">:</span>
            <span class="n">lowered_backend_module</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">lowered_programs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lowered_backend_module</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">lowered_programs</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_unsafe_adjust_original_program</span><span class="p">(</span>  <span class="c1"># noqa: C901</span>
    <span class="n">original_program</span><span class="p">:</span> <span class="n">ExportedProgram</span><span class="p">,</span>
    <span class="n">call_delegate_node</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">,</span>
    <span class="n">input_specs_to_delete</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">InputSpec</span><span class="p">],</span>
    <span class="n">output_specs_to_delete</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OutputSpec</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Directly modify the original exported program&#39;s signature and state dict</span>
<span class="sd">    based on the consumed params/buffers in the delegate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">original_program</span><span class="o">.</span><span class="n">_graph_signature</span><span class="o">.</span><span class="n">input_specs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">input_spec</span>
        <span class="k">for</span> <span class="n">input_spec</span> <span class="ow">in</span> <span class="n">original_program</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">input_specs</span>
        <span class="k">if</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">arg</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">input_specs_to_delete</span>
    <span class="p">]</span>

    <span class="n">currently_used_targets</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">input_spec</span><span class="o">.</span><span class="n">target</span>
        <span class="k">for</span> <span class="n">input_spec</span> <span class="ow">in</span> <span class="n">original_program</span><span class="o">.</span><span class="n">_graph_signature</span><span class="o">.</span><span class="n">input_specs</span>
        <span class="k">if</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">}</span>

    <span class="n">original_program</span><span class="o">.</span><span class="n">_graph_signature</span><span class="o">.</span><span class="n">output_specs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">output_spec</span>
        <span class="k">for</span> <span class="n">output_spec</span> <span class="ow">in</span> <span class="n">original_program</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">output_specs</span>
        <span class="k">if</span> <span class="n">output_spec</span><span class="o">.</span><span class="n">arg</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_specs_to_delete</span>
    <span class="p">]</span>

    <span class="c1"># Delete all parameters/buffers consumed by the created exported program</span>
    <span class="c1"># from the graph signature, state dict, constants table</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">original_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">input_specs_to_delete</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">users</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="n">original_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">erase_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">for</span> <span class="n">input_spec</span> <span class="ow">in</span> <span class="n">input_specs_to_delete</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">input_target</span> <span class="o">=</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">target</span>
        <span class="k">assert</span> <span class="n">input_target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">input_target</span> <span class="ow">in</span> <span class="n">currently_used_targets</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="k">if</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">InputKind</span><span class="o">.</span><span class="n">PARAMETER</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">original_program</span><span class="o">.</span><span class="n">_state_dict</span><span class="p">[</span><span class="n">input_target</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">InputKind</span><span class="o">.</span><span class="n">BUFFER</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">persistent</span><span class="p">:</span>
                <span class="n">original_program</span><span class="o">.</span><span class="n">_state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">input_target</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">original_program</span><span class="o">.</span><span class="n">_constants</span><span class="p">[</span><span class="n">input_spec</span><span class="o">.</span><span class="n">target</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">InputKind</span><span class="o">.</span><span class="n">CONSTANT_TENSOR</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">original_program</span><span class="o">.</span><span class="n">_constants</span><span class="p">[</span><span class="n">input_spec</span><span class="o">.</span><span class="n">target</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid input spec </span><span class="si">{</span><span class="n">input_spec</span><span class="si">}</span><span class="s2"> received&quot;</span><span class="p">)</span>

    <span class="c1"># Delete buffer mutations from the output which were consumed by the delegate</span>
    <span class="n">toplevel_output_node</span> <span class="o">=</span> <span class="n">original_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output_node</span><span class="p">()</span>

    <span class="k">assert</span> <span class="n">toplevel_output_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">toplevel_output_node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Invalid output node: </span><span class="si">{</span><span class="n">toplevel_output_node</span><span class="si">}</span><span class="s2"> with args </span><span class="si">{</span><span class="n">toplevel_output_node</span><span class="o">.</span><span class="n">args</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="n">new_output_args</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">arg</span>
        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">toplevel_output_node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Node</span><span class="p">)</span> <span class="ow">or</span> <span class="n">arg</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_specs_to_delete</span>
    <span class="p">]</span>
    <span class="n">toplevel_output_node</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_output_args</span><span class="p">),)</span>

    <span class="c1"># Delete the buffer mutation getitem nodes</span>
    <span class="n">getitem_idxs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">user_nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">call_delegate_node</span><span class="o">.</span><span class="n">users</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">user</span> <span class="ow">in</span> <span class="n">user_nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">user</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">output_specs_to_delete</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">user</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="n">user</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">operator</span><span class="o">.</span><span class="n">getitem</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Invalid user </span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">, node.op is </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="si">}</span><span class="s2"> and node.target is </span><span class="si">{</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">user_idx</span> <span class="o">=</span> <span class="n">user</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">user_idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Invalid getitem type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">user_idx</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">getitem_idxs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">user_idx</span><span class="p">)</span>
            <span class="n">original_program</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">erase_node</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>

    <span class="n">getitem_idxs</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Adjust all the getitem indices after the deleted getitems</span>
    <span class="n">user_nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">call_delegate_node</span><span class="o">.</span><span class="n">users</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">user</span> <span class="ow">in</span> <span class="n">user_nodes</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">user</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="n">user</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">operator</span><span class="o">.</span><span class="n">getitem</span>
        <span class="n">user_idx</span> <span class="o">=</span> <span class="n">user</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">user_idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">getitem_idxs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">user_idx</span> <span class="o">&gt;</span> <span class="n">idx</span><span class="p">:</span>
                <span class="n">user</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">user</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">user_idx</span> <span class="o">-</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">getitem_idxs</span><span class="p">)</span> <span class="o">-</span> <span class="n">i</span><span class="p">))</span>
                <span class="k">break</span>
</pre></div>

                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, ExecuTorch.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "executorch.exir.lowered_backend_module",
       "headline": "executorch.exir.lowered_backend_module",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/_modules/executorch/exir/lowered_backend_module.html",
       "articleBody": "Source code for executorch.exir.lowered_backend_module # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # # This source code is licensed under the BSD-style license found in the # LICENSE file in the root directory of this source tree. # pyre-strict import copy import operator from collections import defaultdict from typing import Any, Dict, List, Optional, Set, Tuple, Union import torch import torch.utils._pytree as pytree from executorch.exir._serialize import _serialize_pte_binary from executorch.exir._serialize._named_data_store import NamedDataStoreOutput from executorch.exir.backend.compile_spec_schema import CompileSpec from executorch.exir.delegate import executorch_call_delegate, get_lowered_module_name from executorch.exir.emit import emit_program from executorch.exir.graph_module import _get_submodule from executorch.exir.passes.memory_planning_pass import MemoryPlanningPass from executorch.exir.passes.spec_prop_pass import make_spec, SpecPropPass from executorch.exir.schema import Program from executorch.exir.tracer import Value from torch._library.fake_class_registry import FakeScriptObject from torch._subclasses import FakeTensor from torch.export.exported_program import ( ConstantArgument, ExportedProgram, ExportGraphSignature, InputKind, InputSpec, ModuleCallEntry, ModuleCallSignature, OutputKind, OutputSpec, TensorArgument, ) from torch.fx.passes.utils.fuser_utils import ( erase_nodes, fuse_as_graphmodule, insert_subgm, legalize_graph, NodeList, topo_sort, ) [docs] class LoweredBackendModule(torch.nn.Module): \"\"\" A subclass of nn.Module that is generated for modules containing delegated functions. This is can be created by calling `to_backend`. \"\"\" _backend_id: str # The backend\u0027s name _processed_bytes: bytes # The delegate blobs created from backend.preprocess _compile_specs: List[ CompileSpec ] # A list of backend-specific objects with static metadata to configure the \"compilation\" process. _original_exported_program: ExportedProgram # The original EXIR module _named_data_store_output: Optional[ NamedDataStoreOutput ] # Named Data serialized by the backend meta: Optional[Dict[str, Any]] # Metadata for the lowered module def __init__( self, edge_program: ExportedProgram, backend_id: str, processed_bytes: bytes, compile_specs: List[CompileSpec], named_data_store_output: Optional[NamedDataStoreOutput] = None, ) -\u003e None: super().__init__() self._original_exported_program = edge_program self._backend_id = backend_id self._processed_bytes = processed_bytes self._compile_specs = compile_specs self._named_data_store_output = named_data_store_output self.meta = None # pyre-ignore def __deepcopy__(self, memo: Optional[Dict[int, Any]]) -\u003e \"LoweredBackendModule\": # Copy exported program copied_program = ExportedProgram( root=copy.deepcopy(self._original_exported_program.graph_module), graph=copy.deepcopy(self._original_exported_program.graph), graph_signature=copy.deepcopy( self._original_exported_program.graph_signature ), state_dict=self._original_exported_program.state_dict, range_constraints=copy.deepcopy( self._original_exported_program.range_constraints ), module_call_graph=copy.deepcopy( self._original_exported_program.module_call_graph ), constants=self._original_exported_program.constants, verifiers=[copy.deepcopy(self._original_exported_program.verifier)], ) res = LoweredBackendModule( edge_program=copied_program, backend_id=self._backend_id, processed_bytes=self._processed_bytes, compile_specs=copy.deepcopy(self._compile_specs, memo), named_data_store_output=self._named_data_store_output, ) res.meta = copy.copy(getattr(self, \"meta\", {})) return res @property def backend_id(self) -\u003e str: \"\"\" Returns the backends name. \"\"\" return self._backend_id @property def processed_bytes(self) -\u003e bytes: \"\"\" Returns the delegate blob created from backend.preprocess \"\"\" return self._processed_bytes @property def compile_specs(self) -\u003e List[CompileSpec]: \"\"\" Returns a list of backend-specific objects with static metadata to configure the \"compilation\" process. \"\"\" return self._compile_specs @property def original_module(self) -\u003e ExportedProgram: \"\"\" Returns the original EXIR module \"\"\" return self._original_exported_program @property def named_data_store_output(self) -\u003e Optional[NamedDataStoreOutput]: \"\"\" Returns the Named Data Store Output \"\"\" return self._named_data_store_output # TODO(chenlai): consolidate the seriailization config with serialize_to_flatbuffer api [docs] def buffer( self, extract_delegate_segments: bool = False, segment_alignment: int = 128, constant_tensor_alignment: Optional[int] = None, delegate_alignment: Optional[int] = None, memory_planning: MemoryPlanningPass = None, # pyre-fixme[9] ) -\u003e bytes: \"\"\" Returns a buffer containing the serialized ExecuTorch binary. \"\"\" # TODO(T181463742): avoid calling bytes(..) which incurs large copies. out = bytes( _serialize_pte_binary( program=self.program(memory_planning=memory_planning), extract_delegate_segments=extract_delegate_segments, segment_alignment=segment_alignment, constant_tensor_alignment=constant_tensor_alignment, delegate_alignment=delegate_alignment, named_data=self.named_data_store_output, ) ) return out # TODO(chenlai): re-consider recapture instead of manually constructing the program because # the meta data construction is done manually. def program( self, emit_stacktrace: bool = False, memory_planning: MemoryPlanningPass = None, # pyre-fixme[9] ) -\u003e Program: # Fix autodpes introuces cyclic dependencies: # program -\u003e verifier -\u003e lowered_backend_module -\u003e program # @manual from executorch.exir.program._program import ( _get_updated_graph_signature, _transform, ) \"\"\" Returns the object that represents the ExecuTorch binary before serialization. \"\"\" # Creates a new module based on the original module. The original module will # look something like following: # # opcode name target args kwargs # ------------- ------------------- ---------------- ------------------------------------------ -------- # placeholder arg0_1 arg0_1 () {} # placeholder arg1_1 arg1_1 () {} # call_function aten_repeat_default * (arg1_1, [4, 1]) {} # call_function aten_mul_tensor * (aten_repeat_default, aten_repeat_default) {} # call_function aten_add_tensor * (arg1_1, arg1_1) {} # output output output ([aten_mul_tensor, aten_add_tensor],) {} # # if the whole module is lowered, the resulting lowered module look like # # opcode name target args kwargs # ------------- ------------------------ --------------------------- ---------------------------------- -------- # placeholder arg0_1 arg0_1 () {} # placeholder arg1_1 arg1_1 () {} # get_attr lowered_module_0 lowered_module_0 () {} # call_function executorch_call_delegate executorch_call_delegate (lowered_module_0, arg0_1, arg1_1) {} # call_function getitem \u003cbuilt-in function getitem\u003e (executorch_call_delegate, 0) {} # call_function getitem_1 \u003cbuilt-in function getitem\u003e (executorch_call_delegate, 1) {} # output output_1 output ([getitem, getitem_1],) {} # # We\u0027ll remove all call_function nodes, insert an call_delegate node, inserting getitems nodes to get the result for call_delegate node # and return the list of getitems as the output lowered_exported_program = copy.deepcopy(self._original_exported_program) # The real input nodes are the ones not buffer or parameter all_input_nodes = [ node for node in lowered_exported_program.graph.nodes if ( node.op == \"placeholder\" and node.name not in lowered_exported_program.graph_signature.inputs_to_buffers and node.name not in lowered_exported_program.graph_signature.inputs_to_parameters ) ] output_node = lowered_exported_program.graph.output_node() # Step 1. Cleaning up the graph before inserting the call_delegate node # Remove the original output node lowered_exported_program.graph.erase_node(output_node) # Remove all the everything else except the input for node in reversed(lowered_exported_program.graph.nodes): if node.op != \"placeholder\": lowered_exported_program.graph.erase_node(node) # Find placeholders that are parameters or buffers, remove them from the main graph for node in lowered_exported_program.graph.nodes: if node.op == \"placeholder\" and ( node.name in lowered_exported_program.graph_signature.inputs_to_buffers or node.name in lowered_exported_program.graph_signature.inputs_to_parameters ): lowered_exported_program.graph.erase_node(node) # Step 2. Start constructing the graph lowered_name = get_lowered_module_name( lowered_exported_program.graph_module, self ) # Insert the lowered module to the graph module as an attibute lowered_node = lowered_exported_program.graph.get_attr(lowered_name) # Insert a call_delegate node to the graph module, with arguments from the arg list delegate_node = lowered_exported_program.graph.call_function( executorch_call_delegate, (lowered_node, *all_input_nodes) ) # Get the output list. Since the output node is a tuple of list, like ([aten_mul_tensor, aten_add_tensor],) # We add some handling logic to get the list `[aten_mul_tensor, aten_add_tensor]` properly original_output_nodes = ( self._original_exported_program.graph.output_node().args[0] ) delegate_node.meta[\"spec\"] = tuple( [make_spec(node.meta[\"val\"]) for node in original_output_nodes] ) delegate_node.meta[\"val\"] = tuple( [node.meta[\"val\"] for node in original_output_nodes] ) # The getitem nodes that are going to be inserted to the lowered graph module getitem_nodes = [] for i in range(len(original_output_nodes)): getitem_node = lowered_exported_program.graph.call_function( operator.getitem, args=(delegate_node, i), ) getitem_node.meta[\"val\"] = delegate_node.meta[\"val\"][i] getitem_nodes.append(getitem_node) lowered_exported_program.graph.output(getitem_nodes) lowered_exported_program.graph_module.recompile() lowered_exported_program.graph.lint() # Users output will be the get items nodes instead output_specs = [ OutputSpec( kind=OutputKind.USER_OUTPUT, arg=TensorArgument(name=getitem_node.name), target=None, ) for getitem_node in getitem_nodes ] # All data are consumed by the delegates so they should be removed from the state dict. inputs_to_parameters = ( lowered_exported_program.graph_signature.inputs_to_parameters ) inputs_to_buffers = lowered_exported_program.graph_signature.inputs_to_buffers input_specs = [ InputSpec( kind=InputKind.USER_INPUT, arg=TensorArgument(name=node.name), target=None, ) for user_input in lowered_exported_program.graph_signature.user_inputs if user_input not in inputs_to_parameters and user_input not in inputs_to_buffers ] # Double check the ExportedProgram data(especially everything except graph) is good exported_program = ExportedProgram( root=lowered_exported_program.graph_module, graph=lowered_exported_program.graph, graph_signature=_get_updated_graph_signature( ExportGraphSignature( input_specs=input_specs, output_specs=output_specs ), lowered_exported_program.graph_module, ), # TODO: May need to set lowered_exported_program.call_spec = CallSpec(None, None) # somewhere as we should pass it a list of tensors to the lowered module and output a # list of tensors. Putting call_spec=lowered_exported_program.call_spec is correct here as the # inputs/outputs to the toplevel program will be in the format of the eager module. state_dict={}, # None because all data are consumed by delegate range_constraints=lowered_exported_program.range_constraints, module_call_graph=lowered_exported_program.module_call_graph, example_inputs=None, verifiers=[lowered_exported_program.verifier], ) if memory_planning is None: memory_planning = MemoryPlanningPass() exported_program = _transform(exported_program, SpecPropPass(), memory_planning) emitted_program = emit_program( exported_program, emit_stacktrace=emit_stacktrace ).program return emitted_program # Used to patch each delegated function with a call_delegate call # @staticmethod def forward( self, *args: Value, **kwargs: Tuple[Value, ...], ) -\u003e Value: return executorch_call_delegate(self, *args) # TODO(zhxchen17) Try ExportPass def _fixup_output_node(gm: torch.fx.GraphModule) -\u003e None: for node in reversed(gm.graph.nodes): if node.op == \"output\": with gm.graph.inserting_before(node): assert len(node.args) == 1 outputs = node.args[0] if isinstance(outputs, torch.fx.Node): val = outputs.meta.get(\"val\") if isinstance(val, list): # If a list is returned, in some cases it is represented as a # singular node, like `split_copy_tensor` but EXIR will return a # opened-up list like `[getitem1, getitem2]` outputs = [ torch.fx.Proxy(outputs)[i].node for i in range(len(val)) ] returns, out_spec = pytree.tree_flatten(outputs) node.args = (returns,) return def arrange_graph_placeholders( gm: torch.fx.GraphModule, owning_program: ExportedProgram, tag ) -\u003e torch.fx.GraphModule: \"\"\" Modifies the graph of the given graphmodule with one that contains the same nodes as the original, but with placeholders in order of (Params + Buffers) (User Inputs) This is used by the delegate api which disturbs the placeholder ordering when creating a submodule from partitioned nodes Args: gm: The graph module that we want arranged owning_program: ExportedProgram that the submodule (gm) belongs to Returns: The graph module in-placed arranged \"\"\" new_graph = torch.fx.Graph() node_map = {} # mapping of nodes from old graph to new graph graph_sign = owning_program.graph_signature # Add all placeholders into the graph first: param_nodes = [] buffer_nodes = [] input_nodes = [] for node in gm.graph.nodes: if node.op != \"placeholder\": continue if ( node.name in graph_sign.inputs_to_parameters and node.meta.get(\"delegation_tag\", None) == tag ): param_nodes.append(node) elif ( node.name in graph_sign.inputs_to_buffers and node.meta.get(\"delegation_tag\", None) == tag ): buffer_nodes.append(node) else: input_nodes.append(node) for param_node in param_nodes: new_node = new_graph.node_copy(param_node, lambda x: node_map[x]) node_map[param_node] = new_node for buffer_node in buffer_nodes: new_node = new_graph.node_copy(buffer_node, lambda x: node_map[x]) node_map[buffer_node] = new_node for input_node in input_nodes: new_node = new_graph.node_copy(input_node, lambda x: node_map[x]) node_map[input_node] = new_node # Now add all the other nodes in order for node in gm.graph.nodes: if node.op == \"placeholder\": continue new_node = new_graph.node_copy(node, lambda x: node_map[x]) node_map[node] = new_node # lint to ensure correctness new_graph.lint() new_graph._codegen = gm.graph._codegen gm.graph = new_graph return gm # TODO Don\u0027t regenerate new signature manually. def _get_new_signature( # noqa: C901 original_program: ExportedProgram, gm: torch.fx.GraphModule, call_module_node: torch.fx.Node, tag: str, is_submodule: bool = False, ) -\u003e Tuple[ ExportGraphSignature, Dict[str, Union[torch.Tensor, torch.nn.Parameter]], Dict[str, Union[torch.Tensor, FakeScriptObject, torch.ScriptObject]], Dict[str, InputSpec], Dict[str, OutputSpec], ]: \"\"\" Args: original_program: The original program that we are paritioning gm: The partitioned graph module. call_module_node: The node in the original program that is calling the partitioned graph module. tag: The tag being used for this partitioned submodule. This is used to tell if a particular parameter/buffer/constant node is being tagged, aka consumed by the delegate. is_submodule: True if we are currently partitioning inside of a submodule (like cond\u0027s submodule). If we are inside of a submodule, we do not care about consuming params/buffers. Returns: new_signature (ExportGraphSignature): The new signature for the partitioned graph module. new_state_dict (Dict[str, Union[torch.Tensor, torch.nn.Parameter]]): The new state dict containing the consumed params/buffers. new_constants (Dict[str, Union[torch.Tensor, FakeScriptObject, torch.ScriptObject]]): The new constants table containing the consumed constants . input_specs_to_delete (Dict[str, InputSpec]): The input specs that have been consumed by the delegate (param/buffer input nodes) and should be removed from the toplevel ExportedProgram. output_specs_to_delete (Dict[str, InputSpec]): The output specs that have been consumed by the delegate (buffer mutation nodes) and should be removed from the toplevel ExportedProgram. \"\"\" old_signature = original_program.graph_signature input_specs = [] output_specs = [] input_specs_to_delete = {} output_specs_to_delete = {} new_state_dict = {} new_constants = {} # If we are within a submodule, we do not need to care about consuming # parameter/buffers input_node_to_sig: Dict[str, InputSpec] = ( {input_spec.arg.name: input_spec for input_spec in old_signature.input_specs} if not is_submodule else {} ) toplevel_output_node_to_sig: Dict[str, List[OutputSpec]] = defaultdict(list) if not is_submodule: for output_spec in old_signature.output_specs: toplevel_output_node_to_sig[output_spec.arg.name].append(output_spec) for node in gm.graph.nodes: if node.op == \"placeholder\": if node.name not in input_node_to_sig: input_specs.append( InputSpec( kind=InputKind.USER_INPUT, arg=TensorArgument(name=node.name), target=None, ) ) continue orig_input_spec = input_node_to_sig[node.name] if not isinstance(orig_input_spec.arg, TensorArgument): input_specs.append(orig_input_spec) elif node.meta.get(\"delegation_tag\", None) == tag: input_specs.append(orig_input_spec) if orig_input_spec.kind == InputKind.USER_INPUT: continue # The following input specs are all attributes that should be # consumed by the delegate, so we want to remove it from the # toplevel module input/output input_specs_to_delete[node.name] = orig_input_spec input_target = orig_input_spec.target if input_target in original_program.state_dict: assert orig_input_spec.kind in ( InputKind.PARAMETER, InputKind.BUFFER, ) new_state_dict[input_target] = original_program.state_dict[ input_target ] elif input_target in original_program.constants: assert orig_input_spec.kind in ( InputKind.CONSTANT_TENSOR, InputKind.CUSTOM_OBJ, InputKind.BUFFER, ) new_constants[input_target] = original_program.constants[ input_target ] else: raise RuntimeError(f\"Invalid input spec {orig_input_spec} received\") else: input_specs.append( InputSpec( kind=InputKind.USER_INPUT, arg=TensorArgument(name=node.name), target=None, ) ) if node.op == \"output\": buffer_mutation_idxs: Dict[int, List[OutputSpec]] = defaultdict(list) for user in call_module_node.users.keys(): if user.name in toplevel_output_node_to_sig: assert ( user.op == \"call_function\" and user.target == operator.getitem ), f\"Invalid user {user}, node.op is {user.op} and node.target is {user.target}\" getitem_idx = user.args[1] assert isinstance( getitem_idx, int ), f\"Invalid getitem type: {type(getitem_idx)}\" buffer_mutation_idxs[getitem_idx].extend( toplevel_output_node_to_sig[user.name] ) for i, output_node in enumerate(node.args[0]): if i in buffer_mutation_idxs: assert isinstance(output_node, torch.fx.Node) orig_output_specs = buffer_mutation_idxs[i] if any( orig_output_spec.kind == OutputKind.BUFFER_MUTATION and ( orig_output_spec.target in new_state_dict or orig_output_spec.target in new_constants ) for orig_output_spec in orig_output_specs ): # If the delegate wants to consume the buffer, then the # delegate should also consume the buffer mutation # (output spec would be a BUFFER_MUTATION). Otherwise # the delegate will just return the result of the # mutation as a USER_OUTPUT. orig_output_spec = [ orig_output_spec for orig_output_spec in orig_output_specs if orig_output_spec.kind == OutputKind.BUFFER_MUTATION and ( orig_output_spec.target in new_state_dict or orig_output_spec.target in new_constants ) ][0] assert len(orig_output_specs) == 1, ( f\"Constant {orig_output_spec.target} was tagged to be \" \"consumed by the buffer, and was found to also contain \" \"a buffer mutation. However this buffer mutation node \" \"was found to also be used as other types of outputs \" \"which is currently not supported. Please file an \" \"issue on Github. \\n\\n\" f\"The toplevel program: {original_program}\\n\" ) output_specs.append( OutputSpec( kind=OutputKind.BUFFER_MUTATION, arg=TensorArgument(name=output_node.name), target=orig_output_spec.target, ) ) output_specs_to_delete[orig_output_spec.arg.name] = ( orig_output_spec ) else: output_specs.append( OutputSpec( kind=OutputKind.USER_OUTPUT, arg=TensorArgument(name=output_node.name), target=None, ) ) elif not isinstance(output_node, torch.fx.Node): output_specs.append( OutputSpec( kind=OutputKind.USER_OUTPUT, arg=ConstantArgument(name=\"\", value=output_node), target=None, ) ) else: output_specs.append( OutputSpec( kind=OutputKind.USER_OUTPUT, arg=TensorArgument(name=output_node.name), target=None, ) ) new_signature = ExportGraphSignature( input_specs=input_specs, output_specs=output_specs ) return ( new_signature, new_state_dict, new_constants, input_specs_to_delete, output_specs_to_delete, ) def create_exported_program_from_submodule( submodule: torch.fx.GraphModule, owning_program: ExportedProgram, tag: str, call_module_node: torch.fx.Node, is_submodule: bool, ) -\u003e Tuple[ExportedProgram, Dict[str, InputSpec], Dict[str, OutputSpec]]: \"\"\" Creates an ExportedProgram from the given submodule using the parameters and buffers from the top-level owning program Args: submodule: submodule to create and exported program from owning_program: exported program containing the parameters and buffers used within the submodule Returns: The ExportedProgram created from submodule input_specs_to_delete (Dict[str, InputSpec]): The input specs that have been consumed by the delegate (param/buffer input nodes) and should be removed from the toplevel ExportedProgram. output_specs_to_delete (Dict[str, InputSpec]): The output specs that have been consumed by the delegate (buffer mutation nodes) and should be removed from the toplevel ExportedProgram. \"\"\" # Arrange the submodule\u0027s placeholders in order submodule = arrange_graph_placeholders(submodule, owning_program, tag) # TODO: we probably need to arrange the outputs wrt buffer mutations. # Get updated graph signature ( subgraph_signature, subgraph_state_dict, subgraph_constants, toplevel_input_specs_to_delete, toplevel_output_specs_to_delete, ) = _get_new_signature( owning_program, submodule, call_module_node, tag, is_submodule ) in_spec = pytree.tree_flatten((tuple(subgraph_signature.user_inputs), {}))[1] out_spec = pytree.tree_flatten(subgraph_signature.user_outputs)[1] return ( ExportedProgram( root=submodule, graph=submodule.graph, graph_signature=subgraph_signature, state_dict=subgraph_state_dict, range_constraints=copy.deepcopy(owning_program.range_constraints), module_call_graph=[ ModuleCallEntry( \"\", ModuleCallSignature( inputs=[], outputs=[], in_spec=in_spec, out_spec=out_spec ), ) ], constants=subgraph_constants, verifiers=[owning_program.verifier], ), toplevel_input_specs_to_delete, toplevel_output_specs_to_delete, ) def create_submodule_from_nodes( gm: torch.fx.GraphModule, node_list: NodeList, tag: str, skip_legalize_graph: bool = False, ) -\u003e Tuple[torch.fx.GraphModule, torch.fx.Node]: \"\"\" Modifies the given graph module in-place to separate out the given nodes into a submodule. The given node_list should form a fully connected subgraph. Args: gm: The graph module that we want to partition node_list: A list of nodes that belong in the partition Returns: The submodule that has been partitioned, the call_module node in the toplevel graph module calling the submodule \"\"\" sorted_nodes = topo_sort(node_list) submodule_name = \"fused_\" + tag sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule( gm, sorted_nodes, submodule_name ) _fixup_output_node(sub_gm) gm = insert_subgm(gm, sub_gm, orig_inputs, orig_outputs) submodule_node = None for node in gm.graph.nodes: if node.op == \"call_module\" and node.target == submodule_name: submodule_node = node if submodule_node is None: raise RuntimeError( f\"The submodule created with nodes {node_list} did not form \\ one fully contained subgraph. Check that these nodes form a \\ fully contained graph. Partitioned graph: {gm.graph}.\" ) if len(orig_outputs) == 1 and isinstance(orig_outputs[0].meta[\"val\"], FakeTensor): # If the original output is a single tensor, it has been # pytree.tree_flatten-ed to be a singleton list, so we want to replace # all uses with a getitem call to the 0th index of the result with gm.graph.inserting_after(submodule_node): proxy_out = torch.fx.Proxy(submodule_node)[0].node # type: ignore[index] submodule_node.replace_all_uses_with(proxy_out) proxy_out.meta[\"val\"] = submodule_node.meta[\"val\"] # Reset the args since it was overwritten in the previous line proxy_out.args = (submodule_node, 0) else: # fuse_as_graphmodule will automatically propagate the metadata of the # partition\u0027s last node to the getitem nodes that appear after the # call_module node. However, in the case of delegation we do not want # these getitem nodes to contain irrelevant previous metadata # (ex. source_fn, # nn_module_stack) for user_node in submodule_node.users: user_node.meta.pop(\"nn_module_stack\", None) user_node.meta.pop(\"source_fn_stack\", None) erase_nodes(gm, sorted_nodes) # Topological sort original gm with newly created sub_gm # TODO : T153794167 Get rid of support for skipping legalize graph in create_submodule_from_nodes # once we transition to using fuse_by_partitions. if not skip_legalize_graph: legalize_graph(gm) # Get the call_module node submodule_node = None for node in gm.graph.nodes: if node.op == \"call_module\" and node.target == submodule_name: submodule_node = node if submodule_node is None: raise RuntimeError( f\"The submodule created with nodes {node_list} did not form \\ one fully contained subgraph. Check that these nodes form a \\ fully contained graph. Partitioned graph: {gm.graph}.\" ) assert ( submodule_node is not None ), f\"No submodule was created with the nodes {node_list} in the graph {gm.graph}\" return sub_gm, submodule_node def get_lowered_submodules( graph_module: torch.fx.GraphModule, ) -\u003e List[Tuple[str, LoweredBackendModule, torch.fx.Node]]: \"\"\" Returns a list of lowered modules that are in the given graph (does not look into submodules). Specifically, the returned value is a list containing a tuple of (name of the lowered module that\u0027s stored in the graph module, the lowered module itself, and the fx node that called this lowered module). \"\"\" lowered_submodules = [] for node in graph_module.graph.nodes: if node.op == \"call_function\" and node.target == executorch_call_delegate: name, module, node = _get_submodule(graph_module, node, 0) assert isinstance(module, LoweredBackendModule) lowered_submodules.append((name, module, node)) return lowered_submodules def get_lowered_backend_modules( graph_module: torch.fx.GraphModule, ) -\u003e List[LoweredBackendModule]: \"\"\" Returns a list of exported programs which were lowered by backen delegates \"\"\" lowered_programs = [] for node in graph_module.graph.nodes: if node.op == \"call_function\" and node.target == executorch_call_delegate: lowered_backend_module = getattr(graph_module, node.args[0].name) lowered_programs.append(lowered_backend_module) return lowered_programs def _unsafe_adjust_original_program( # noqa: C901 original_program: ExportedProgram, call_delegate_node: torch.fx.Node, input_specs_to_delete: Dict[str, InputSpec], output_specs_to_delete: Dict[str, OutputSpec], ) -\u003e None: \"\"\" Directly modify the original exported program\u0027s signature and state dict based on the consumed params/buffers in the delegate. \"\"\" original_program._graph_signature.input_specs = [ input_spec for input_spec in original_program.graph_signature.input_specs if input_spec.arg.name not in input_specs_to_delete ] currently_used_targets: Set[str] = { input_spec.target for input_spec in original_program._graph_signature.input_specs if input_spec.target is not None } original_program._graph_signature.output_specs = [ output_spec for output_spec in original_program.graph_signature.output_specs if output_spec.arg.name not in output_specs_to_delete ] # Delete all parameters/buffers consumed by the created exported program # from the graph signature, state dict, constants table for node in original_program.graph.nodes: if node.op == \"placeholder\": if node.name in input_specs_to_delete: assert len(node.users) == 0 original_program.graph.erase_node(node) else: break for input_spec in input_specs_to_delete.values(): input_target = input_spec.target assert input_target is not None if input_target in currently_used_targets: continue if input_spec.kind == InputKind.PARAMETER: del original_program._state_dict[input_target] elif input_spec.kind == InputKind.BUFFER: if input_spec.persistent: original_program._state_dict.pop(input_target, None) else: del original_program._constants[input_spec.target] elif input_spec.kind == InputKind.CONSTANT_TENSOR: del original_program._constants[input_spec.target] else: raise RuntimeError(f\"Invalid input spec {input_spec} received\") # Delete buffer mutations from the output which were consumed by the delegate toplevel_output_node = original_program.graph.output_node() assert toplevel_output_node is not None assert ( len(toplevel_output_node.args) == 1 ), f\"Invalid output node: {toplevel_output_node} with args {toplevel_output_node.args}\" new_output_args = [ arg for arg in toplevel_output_node.args[0] if not isinstance(arg, torch.fx.Node) or arg.name not in output_specs_to_delete ] toplevel_output_node.args = (tuple(new_output_args),) # Delete the buffer mutation getitem nodes getitem_idxs: List[int] = [] user_nodes = list(call_delegate_node.users.keys()) for user in user_nodes: if user.name in output_specs_to_delete: assert ( user.op == \"call_function\" and user.target == operator.getitem ), f\"Invalid user {user}, node.op is {node.op} and node.target is {node.target}\" user_idx = user.args[1] assert isinstance(user_idx, int), f\"Invalid getitem type: {type(user_idx)}\" getitem_idxs.append(user_idx) original_program.graph.erase_node(user) getitem_idxs.sort(reverse=True) # Adjust all the getitem indices after the deleted getitems user_nodes = list(call_delegate_node.users.keys()) for user in user_nodes: assert user.op == \"call_function\" and user.target == operator.getitem user_idx = user.args[1] assert isinstance(user_idx, int) for i, idx in enumerate(getitem_idxs): if user_idx \u003e idx: user.args = (user.args[0], user_idx - (len(getitem_idxs) - i)) break",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/executorch/exir/lowered_backend_module.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>