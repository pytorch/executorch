#!/usr/bin/env python3
# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

"""
PTE Inspector - Extract and dump data from ExecuTorch .pte files.

This utility can:
1. Parse the PTE file structure (header, flatbuffer, segments)
2. Extract delegate payloads (e.g., MLX backend data)
3. Convert FlatBuffer data to JSON for inspection

Usage:
    python pte_inspector.py mlx_mlp.pte
    python pte_inspector.py mlx_mlp.pte --output output.json
    python pte_inspector.py mlx_mlp.pte --extract-delegate mlx --output mlx_payload.bin
"""

from __future__ import annotations

import argparse
import json
import struct
import sys
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


# =============================================================================
# PTE File Header Parsing
# =============================================================================

@dataclass
class PTEHeader:
    """Extended header from a PTE file."""
    magic: bytes
    length: int
    program_size: int
    segment_base_offset: int
    segment_data_size: int

    @classmethod
    def from_bytes(cls, data: bytes) -> "PTEHeader":
        """Parse extended header from raw bytes."""
        if len(data) < 32:
            raise ValueError(f"Not enough data for header: {len(data)} < 32")

        magic = data[0:4]
        length = int.from_bytes(data[4:8], byteorder="little")
        program_size = int.from_bytes(data[8:16], byteorder="little")
        segment_base_offset = int.from_bytes(data[16:24], byteorder="little")
        segment_data_size = int.from_bytes(data[24:32], byteorder="little") if length > 24 else 0

        return cls(
            magic=magic,
            length=length,
            program_size=program_size,
            segment_base_offset=segment_base_offset,
            segment_data_size=segment_data_size,
        )

    def is_valid(self) -> bool:
        return self.magic == b"eh00" and self.length >= 24

    def to_dict(self) -> Dict[str, Any]:
        return {
            "magic": self.magic.decode("utf-8", errors="replace"),
            "length": self.length,
            "program_size": self.program_size,
            "segment_base_offset": self.segment_base_offset,
            "segment_data_size": self.segment_data_size,
        }


# =============================================================================
# MLX Delegate Payload Parsing
# =============================================================================

MLX_MAGIC = b"MLX0"
MLX_HEADER_LENGTH = 24


@dataclass
class MLXHeader:
    """Header from MLX delegate payload."""
    magic: bytes
    data_segment_offset: int
    data_segment_size: int

    @classmethod
    def from_bytes(cls, data: bytes) -> "MLXHeader":
        """Parse MLX header from raw bytes."""
        if len(data) < MLX_HEADER_LENGTH:
            raise ValueError(f"Not enough data for MLX header: {len(data)} < {MLX_HEADER_LENGTH}")

        # Layout: [4 bytes padding][4 bytes magic][8 bytes offset][8 bytes size]
        magic = data[4:8]
        data_segment_offset = int.from_bytes(data[8:16], byteorder="little")
        data_segment_size = int.from_bytes(data[16:24], byteorder="little")

        return cls(
            magic=magic,
            data_segment_offset=data_segment_offset,
            data_segment_size=data_segment_size,
        )

    def is_valid(self) -> bool:
        return self.magic == MLX_MAGIC

    def to_dict(self) -> Dict[str, Any]:
        return {
            "magic": self.magic.decode("utf-8", errors="replace"),
            "data_segment_offset": self.data_segment_offset,
            "data_segment_size": self.data_segment_size,
        }


# Op type names - auto-generated by generate.py from schema.fbs
from executorch.backends.apple.mlx.serialization._generated_serializers import MLX_OP_TYPE_NAMES


def parse_mlx_flatbuffer(fb_data: bytes) -> Dict[str, Any]:
    """Parse MLX FlatBuffer data into a dict using the generated FlatBuffer bindings."""
    result = {}

    try:
        # Add the _generated directory to sys.path temporarily for imports
        import sys
        import os

        # Find the serialization/_generated directory and add it to path
        current_dir = os.path.dirname(os.path.abspath(__file__))
        generated_dir = os.path.join(current_dir, "serialization", "_generated")

        if not os.path.exists(generated_dir):
            # Try alternate location
            generated_dir = os.path.join(current_dir, "_generated")

        if os.path.exists(generated_dir) and generated_dir not in sys.path:
            sys.path.insert(0, generated_dir)

        from executorch.backends.apple.mlx.serialization._generated.mlx_delegate import MLXGraph as FBMLXGraph

        graph = FBMLXGraph.MLXGraph.GetRootAs(fb_data, 0)

        result = {
            "version": graph.Version().decode("utf-8") if graph.Version() else None,
            "num_constant_tensors": graph.NumConstantTensors(),
            "num_non_constant_tensors": graph.NumNonConstantTensors(),
            "num_non_constant_values": graph.NumNonConstantValues(),
            "num_instructions": graph.InstructionsLength(),
            "input_map_length": graph.InputMapLength(),
            "output_map_length": graph.OutputMapLength(),
            "mutable_buffer_map_length": graph.MutableBufferMapLength(),
            "named_slots_length": graph.NamedSlotsLength(),
            "tensor_meta_length": graph.TensorMetaLength(),
        }

        # Extract instructions with full op details
        instructions = []
        for i in range(graph.InstructionsLength()):
            try:
                instr = graph.Instructions(i)
                if instr:
                    op_type = instr.OpType()
                    op_name = MLX_OP_TYPE_NAMES.get(op_type, f"Unknown({op_type})")
                    instr_info = {
                        "index": i,
                        "op_type": op_type,
                        "op_name": op_name,
                    }

                    # Parse op-specific fields
                    op_data = parse_op_node(instr, op_type, op_name)
                    if op_data:
                        instr_info.update(op_data)

                    instructions.append(instr_info)
            except Exception as e:
                instructions.append({"index": i, "error": f"parse_failed: {e}"})
        result["instructions"] = instructions

        # Extract named slots
        named_slots = []
        for i in range(graph.NamedSlotsLength()):
            try:
                ns = graph.NamedSlots(i)
                if ns:
                    slot_info = {
                        "name": ns.Name().decode("utf-8") if ns.Name() else None,
                    }
                    slot = ns.Slot()
                    if slot:
                        slot_info["slot_idx"] = slot.Idx()
                        slot_info["slot_type"] = slot.SlotType()
                    named_slots.append(slot_info)
            except Exception:
                named_slots.append({"index": i, "error": "parse_failed"})
        result["named_slots"] = named_slots

        # Extract tensor metadata
        tensor_meta = []
        for i in range(graph.TensorMetaLength()):
            try:
                tm = graph.TensorMeta(i)
                if tm:
                    meta = {
                        "index": i,
                        "dtype": tm.Dtype(),
                        "shape": [tm.Shape(j) for j in range(tm.ShapeLength())],
                    }
                    if tm.StridesLength() > 0:
                        meta["strides"] = [tm.Strides(j) for j in range(tm.StridesLength())]
                    tensor_meta.append(meta)
            except Exception:
                tensor_meta.append({"index": i, "error": "parse_failed"})
        result["tensor_meta"] = tensor_meta

        # Extract I/O maps
        def extract_slot_variants(length_fn, getter_fn) -> List[Dict]:
            slots = []
            for i in range(length_fn()):
                try:
                    sv = getter_fn(i)
                    if sv:
                        slots.append({"idx": sv.Idx(), "slot_type": sv.SlotType()})
                except Exception:
                    slots.append({"index": i, "error": "parse_failed"})
            return slots

        result["input_map"] = extract_slot_variants(graph.InputMapLength, graph.InputMap)
        result["output_map"] = extract_slot_variants(graph.OutputMapLength, graph.OutputMap)
        result["mutable_buffer_map"] = extract_slot_variants(
            graph.MutableBufferMapLength, graph.MutableBufferMap
        )

        # Extract constant segment info
        try:
            cs = graph.ConstantSegment()
            if cs:
                result["constant_segment"] = {
                    "offset": cs.Offset(),
                    "size": cs.Size(),
                }
        except Exception:
            pass

    except ImportError as e:
        result["error"] = f"FlatBuffer bindings not available: {e}"
        result["_fallback"] = "Using basic header parsing only"

    except Exception as e:
        result["error"] = f"FlatBuffer parse error: {e}"
        import traceback
        result["traceback"] = traceback.format_exc()

    return result


def parse_op_node(instr, op_type: int, op_name: str) -> Optional[Dict[str, Any]]:
    """Parse the specific op node fields from an instruction.

    This function uses the generated FlatBuffer bindings to extract op-specific
    fields from each instruction type.
    """
    try:
        # Get the op union table
        op = instr.Op()
        if op is None:
            return None

        # Add the _generated directory to sys.path for nested imports
        import sys
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        generated_dir = os.path.join(current_dir, "serialization", "_generated")
        if os.path.exists(generated_dir) and generated_dir not in sys.path:
            sys.path.insert(0, generated_dir)

        result = {}

        # Helper to extract Tid (tensor id)
        def tid(t):
            if t is None:
                return None
            return {"tid": t.Idx()}

        # Helper to extract Vid (value id)
        def vid(v):
            if v is None:
                return None
            return {"vid": v.Idx()}

        # Helper to extract IntOrVid
        def int_or_vid(iov):
            if iov is None:
                return None
            if iov.IsVid():
                v = iov.Vid()
                return {"vid": v.Idx()} if v else None
            return {"literal": iov.Literal()}

        # Helper to init a node from a union member
        # For union members, op.Bytes and op.Pos give us the table location directly
        # We use Init() instead of GetRootAs() because the position is already resolved
        def init_node(node_class):
            node = node_class()
            node.Init(op.Bytes, op.Pos)
            return node

        # Import all node types dynamically
        from executorch.backends.apple.mlx.serialization._generated.mlx_delegate import (
            LinearNode, SiluNode, GeluNode, AddNode, MulNode, ReshapeNode,
            TransposeNode, ContiguousNode, GatherNode, SliceNode, RMSNormNode,
            LayerNormNode, QuantizedLinearNode, QuantizedGatherNode, SdpaNode,
            RopeNode, SliceUpdateNode, ExpandDimsNode, TileNode, ARangeNode,
            SymSizeNode, ItemIntNode, IdCopyNode, CastNode, ConcatNode,
            FullNode, ZerosNode, OnesNode, ArgmaxNode, Conv1DNode, TakeAlongAxisNode,
            AddScalarNode, NoopNode
        )

        # Map from op_name to (node_class, field_extractors)
        # Each extractor is (field_name, method_name, extractor_fn)
        OP_PARSERS = {
            "NoopNode": (NoopNode.NoopNode, []),
            "LinearNode": (LinearNode.LinearNode, [
                ("x", "X", tid), ("weight", "Weight", tid), ("out", "Out", tid), ("bias", "Bias", tid)
            ]),
            "SiluNode": (SiluNode.SiluNode, [
                ("x", "X", tid), ("out", "Out", tid)
            ]),
            "GeluNode": (GeluNode.GeluNode, [
                ("x", "X", tid), ("out", "Out", tid)
            ]),
            "AddNode": (AddNode.AddNode, [
                ("a", "A", tid), ("b", "B", tid), ("out", "Out", tid)
            ]),
            "MulNode": (MulNode.MulNode, [
                ("a", "A", tid), ("b", "B", tid), ("out", "Out", tid)
            ]),
            "AddScalarNode": (AddScalarNode.AddScalarNode, [
                ("a", "A", int_or_vid), ("b", "B", int_or_vid), ("out", "Out", vid)
            ]),
            "ReshapeNode": (ReshapeNode.ReshapeNode, [
                ("x", "X", tid), ("out", "Out", tid),
                ("shape", "Shape", lambda n: [int_or_vid(n.Shape(i)) for i in range(n.ShapeLength())])
            ]),
            "TransposeNode": (TransposeNode.TransposeNode, [
                ("x", "X", tid), ("out", "Out", tid),
                ("perm", "Perm", lambda n: [n.Perm(i) for i in range(n.PermLength())])
            ]),
            "ContiguousNode": (ContiguousNode.ContiguousNode, [
                ("x", "X", tid), ("out", "Out", tid)
            ]),
            "GatherNode": (GatherNode.GatherNode, [
                ("table", "Table_", tid), ("ids", "Ids", tid), ("out", "Out", tid)
            ]),
            "SliceNode": (SliceNode.SliceNode, [
                ("x", "X", tid), ("out", "Out", tid),
                ("axis", "Axis", int_or_vid), ("start", "Start", int_or_vid), ("end", "End", int_or_vid)
            ]),
            "RMSNormNode": (RMSNormNode.RMSNormNode, [
                ("x", "X", tid), ("weight", "Weight", tid), ("out", "Out", tid),
                ("eps", "Eps", lambda n: n.Eps())
            ]),
            "LayerNormNode": (LayerNormNode.LayerNormNode, [
                ("x", "X", tid), ("out", "Out", tid),
                ("weight", "Weight", tid), ("bias", "Bias", tid),
                ("eps", "Eps", lambda n: n.Eps())
            ]),
            "QuantizedLinearNode": (QuantizedLinearNode.QuantizedLinearNode, [
                ("x", "X", tid), ("w", "W", tid), ("scales", "Scales", tid),
                ("out", "Out", tid), ("biases", "Biases", tid), ("bias", "Bias", tid),
                ("group_size", "GroupSize", lambda n: n.GroupSize()),
                ("bits", "Bits", lambda n: n.Bits()),
                ("mode", "Mode", lambda n: n.Mode().decode("utf-8") if n.Mode() else None),
                ("out_dtype", "OutDtype", lambda n: n.OutDtype())
            ]),
            "QuantizedGatherNode": (QuantizedGatherNode.QuantizedGatherNode, [
                ("table_q", "TableQ", tid), ("scales", "Scales", tid), ("ids", "Ids", tid),
                ("out", "Out", tid), ("biases", "Biases", tid),
                ("group_size", "GroupSize", lambda n: n.GroupSize()),
                ("bits", "Bits", lambda n: n.Bits()),
                ("mode", "Mode", lambda n: n.Mode().decode("utf-8") if n.Mode() else None),
                ("out_dtype", "OutDtype", lambda n: n.OutDtype())
            ]),
            "SdpaNode": (SdpaNode.SdpaNode, [
                ("q", "Q", tid), ("k", "K", tid), ("v", "V", tid), ("out", "Out", tid),
                ("scale", "Scale", lambda n: n.Scale()),
                ("mask", "Mask", tid), ("causal", "Causal", lambda n: n.Causal())
            ]),
            "RopeNode": (RopeNode.RopeNode, [
                ("q_in", "QIn", tid), ("k_in", "KIn", tid),
                ("q_out", "QOut", tid), ("k_out", "KOut", tid),
                ("head_dim", "HeadDim", lambda n: n.HeadDim()),
                ("pos", "Pos", vid), ("freqs", "Freqs", tid),
                ("traditional", "Traditional", lambda n: n.Traditional()),
                ("base", "Base", lambda n: n.Base()),
                ("scale", "Scale", lambda n: n.Scale())
            ]),
            "SliceUpdateNode": (SliceUpdateNode.SliceUpdateNode, [
                ("dst", "Dst", tid), ("update", "Update", tid),
                ("axis", "Axis", int_or_vid), ("start", "Start", int_or_vid), ("stop", "Stop", int_or_vid)
            ]),
            "ExpandDimsNode": (ExpandDimsNode.ExpandDimsNode, [
                ("x", "X", tid), ("out", "Out", tid), ("axis", "Axis", lambda n: n.Axis())
            ]),
            "TileNode": (TileNode.TileNode, [
                ("x", "X", tid), ("out", "Out", tid),
                ("reps", "Reps", lambda n: [n.Reps(i) for i in range(n.RepsLength())])
            ]),
            "ARangeNode": (ARangeNode.ARangeNode, [
                ("out", "Out", tid),
                ("start", "Start", lambda n: n.Start()),
                ("stop", "Stop", lambda n: n.Stop()),
                ("step", "Step", lambda n: n.Step()),
                ("dtype", "Dtype", lambda n: n.Dtype())
            ]),
            "SymSizeNode": (SymSizeNode.SymSizeNode, [
                ("a", "A", tid), ("dim", "Dim", lambda n: n.Dim()), ("out", "Out", vid)
            ]),
            "ItemIntNode": (ItemIntNode.ItemIntNode, [
                ("x", "X", tid), ("out", "Out", vid)
            ]),
            "IdCopyNode": (IdCopyNode.IdCopyNode, [
                ("x", "X", tid), ("out", "Out", tid)
            ]),
            "CastNode": (CastNode.CastNode, [
                ("x", "X", tid), ("out", "Out", tid), ("dtype", "Dtype", lambda n: n.Dtype())
            ]),
            "ConcatNode": (ConcatNode.ConcatNode, [
                ("a", "A", tid), ("b", "B", tid), ("out", "Out", tid),
                ("axis", "Axis", lambda n: n.Axis())
            ]),
            "Conv1DNode": (Conv1DNode.Conv1DNode, [
                ("x", "X", tid), ("w", "W", tid), ("out", "Out", tid),
                ("stride", "Stride", lambda n: n.Stride()),
                ("padding", "Padding", lambda n: n.Padding()),
                ("dilation", "Dilation", lambda n: n.Dilation()),
                ("groups", "Groups", lambda n: n.Groups())
            ]),
            "TakeAlongAxisNode": (TakeAlongAxisNode.TakeAlongAxisNode, [
                ("x", "X", tid), ("indices", "Indices", tid), ("out", "Out", tid),
                ("axis", "Axis", lambda n: n.Axis())
            ]),
            "ArgmaxNode": (ArgmaxNode.ArgmaxNode, [
                ("x", "X", tid), ("out", "Out", tid), ("axis", "Axis", lambda n: n.Axis())
            ]),
            "FullNode": (FullNode.FullNode, [
                ("out", "Out", tid),
                ("shape", "Shape", lambda n: [n.Shape(i) for i in range(n.ShapeLength())]),
                ("v", "V", lambda n: n.V()),
                ("dtype", "Dtype", lambda n: n.Dtype())
            ]),
            "ZerosNode": (ZerosNode.ZerosNode, [
                ("out", "Out", tid),
                ("shape", "Shape", lambda n: [n.Shape(i) for i in range(n.ShapeLength())]),
                ("dtype", "Dtype", lambda n: n.Dtype())
            ]),
            "OnesNode": (OnesNode.OnesNode, [
                ("out", "Out", tid),
                ("shape", "Shape", lambda n: [n.Shape(i) for i in range(n.ShapeLength())]),
                ("dtype", "Dtype", lambda n: n.Dtype())
            ]),
        }

        if op_name not in OP_PARSERS:
            return {"error": f"Unknown op type: {op_name}"}

        node_class, field_extractors = OP_PARSERS[op_name]
        node = init_node(node_class)

        for field_name, method_name, extractor in field_extractors:
            try:
                method = getattr(node, method_name)
                value = method()
                # If extractor takes the node, pass it; otherwise pass the value
                if callable(extractor) and extractor.__code__.co_argcount == 1:
                    if extractor.__code__.co_varnames[0] == 'n':
                        result[field_name] = extractor(node)
                    else:
                        result[field_name] = extractor(value)
                else:
                    result[field_name] = extractor(value) if callable(extractor) else value
            except Exception as e:
                result[field_name] = {"error": str(e)}

        # Filter out None values
        result = {k: v for k, v in result.items() if v is not None}
        return result if result else None

    except Exception as e:
        import traceback
        return {"parse_error": str(e), "traceback": traceback.format_exc()}


def parse_mlx_payload(payload: bytes) -> Dict[str, Any]:
    """Parse a complete MLX delegate payload."""
    header = MLXHeader.from_bytes(payload)

    if not header.is_valid():
        return {
            "error": f"Invalid MLX magic: {header.magic!r}",
            "header": header.to_dict(),
        }

    result = {
        "header": header.to_dict(),
    }

    # Extract FlatBuffer portion
    fb_start = MLX_HEADER_LENGTH
    fb_end = header.data_segment_offset
    fb_data = payload[fb_start:fb_end]

    result["flatbuffer_size"] = len(fb_data)
    result["graph"] = parse_mlx_flatbuffer(fb_data)

    # Data segment info
    if header.data_segment_size > 0:
        data_start = header.data_segment_offset
        data_end = data_start + header.data_segment_size
        result["constant_data_size"] = header.data_segment_size

    return result


# =============================================================================
# ExecuTorch Program Parsing
# =============================================================================

def parse_executorch_program(pte_data: bytes) -> Dict[str, Any]:
    """Parse an ExecuTorch .pte file."""
    result: Dict[str, Any] = {}

    # Check for flatbuffer magic (first 4 bytes are root offset, next 4 are magic)
    if len(pte_data) < 8:
        raise ValueError("File too small to be a valid PTE file")

    fb_magic = pte_data[4:8]
    result["flatbuffer_magic"] = fb_magic.decode("utf-8", errors="replace")

    # Check for extended header (after flatbuffer header at offset 8)
    extended_header_offset = 8
    if len(pte_data) > extended_header_offset + 32:
        try:
            header = PTEHeader.from_bytes(pte_data[extended_header_offset:])
            if header.is_valid():
                result["extended_header"] = header.to_dict()

                # FlatBuffer data starts after the extended header
                fb_start = extended_header_offset + header.length
                fb_end = extended_header_offset + header.length + header.program_size - header.length

                result["flatbuffer_offset"] = fb_start
                result["flatbuffer_size"] = header.program_size
                result["segment_offset"] = header.segment_base_offset
                result["segment_size"] = header.segment_data_size
        except Exception as e:
            result["header_parse_error"] = str(e)

    # Try to parse the program FlatBuffer
    try:
        from executorch.exir._serialize._flatbuffer import _program_flatbuffer_to_json

        # The flatbuffer starts at offset 0 (the header is embedded in it)
        program_json = _program_flatbuffer_to_json(pte_data)
        program_data = json.loads(program_json)
        result["program"] = program_data

        # Extract delegate information
        if "execution_plan" in program_data:
            delegates = []
            for plan in program_data["execution_plan"]:
                if "delegates" in plan:
                    for delegate in plan["delegates"]:
                        delegate_info = {
                            "id": delegate.get("id"),
                            "processed_type": delegate.get("processed", {}).get("location"),
                        }
                        # Check for inline data
                        processed = delegate.get("processed", {})
                        if "data" in processed:
                            delegate_info["inline_data_size"] = len(processed["data"])
                        if "location" in processed:
                            delegate_info["location"] = processed["location"]
                        delegates.append(delegate_info)
            result["delegates"] = delegates

    except ImportError:
        result["program_parse_error"] = "ExecuTorch FlatBuffer parsing not available"
    except Exception as e:
        result["program_parse_error"] = str(e)

    return result


def extract_delegate_payload(pte_data: bytes, delegate_id: str) -> Optional[bytes]:
    """Extract a delegate payload from a PTE file."""
    try:
        from executorch.exir._serialize._flatbuffer import _program_flatbuffer_to_json

        program_json = _program_flatbuffer_to_json(pte_data)
        program_data = json.loads(program_json)

        # Parse extended header to get segment info
        extended_header = None
        if len(pte_data) > 40:
            try:
                header = PTEHeader.from_bytes(pte_data[8:])
                if header.is_valid():
                    extended_header = header
            except:
                pass

        # Look for the delegate in execution plans
        for plan in program_data.get("execution_plan", []):
            for delegate in plan.get("delegates", []):
                delegate_name = delegate.get("id", "")
                # Match by ID containing the search string (case-insensitive)
                if delegate_id.lower() in delegate_name.lower():
                    processed = delegate.get("processed", {})

                    # Check for inline data
                    if "data" in processed and processed["data"]:
                        # The data is stored as a list of ints (bytes)
                        data_list = processed["data"]
                        return bytes(data_list)

                    # Check for segment reference
                    location = processed.get("location", 0)
                    # Handle both string and integer location values
                    is_segment = location == 1 or location == "SEGMENT"
                    if is_segment:
                        if extended_header is None:
                            print(f"Warning: Delegate is in segment but no extended header found", file=sys.stderr)
                            return None

                        # Get segment index and offset info
                        index = processed.get("index", 0)

                        # Look up segment in program's segments list
                        segments = program_data.get("segments", [])
                        if index < len(segments):
                            segment = segments[index]
                            seg_offset = segment.get("offset", 0)
                            seg_size = segment.get("size", 0)

                            # Calculate actual offset in file
                            data_offset = extended_header.segment_base_offset + seg_offset
                            return pte_data[data_offset:data_offset + seg_size]
                        else:
                            # Try using the segment directly from the delegate reference
                            print(f"Warning: Segment index {index} not found in segments list (len={len(segments)})", file=sys.stderr)
                            # Fall back: assume single segment containing all delegate data
                            return pte_data[extended_header.segment_base_offset:
                                          extended_header.segment_base_offset + extended_header.segment_data_size]

        return None

    except Exception as e:
        print(f"Error extracting delegate: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return None


# =============================================================================
# Main CLI
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Inspect ExecuTorch .pte files and extract data"
    )
    parser.add_argument("pte_file", type=Path, help="Path to the .pte file")
    parser.add_argument(
        "--output", "-o", type=Path, help="Output file (default: stdout)"
    )
    parser.add_argument(
        "--extract-delegate",
        type=str,
        metavar="ID",
        help="Extract delegate payload by ID (e.g., 'mlx')",
    )
    parser.add_argument(
        "--parse-mlx",
        action="store_true",
        help="Parse extracted MLX payload (use with --extract-delegate mlx)",
    )
    parser.add_argument(
        "--format",
        choices=["json", "summary"],
        default="json",
        help="Output format (default: json)",
    )
    parser.add_argument(
        "--indent",
        type=int,
        default=2,
        help="JSON indentation (default: 2)",
    )

    args = parser.parse_args()

    # Read PTE file
    if not args.pte_file.exists():
        print(f"Error: File not found: {args.pte_file}", file=sys.stderr)
        sys.exit(1)

    pte_data = args.pte_file.read_bytes()
    print(f"Loaded {len(pte_data)} bytes from {args.pte_file}", file=sys.stderr)

    # Handle delegate extraction
    if args.extract_delegate:
        payload = extract_delegate_payload(pte_data, args.extract_delegate)
        if payload is None:
            print(f"Error: Delegate '{args.extract_delegate}' not found", file=sys.stderr)
            sys.exit(1)

        if args.parse_mlx and args.extract_delegate.lower() == "mlx":
            # Parse and output as JSON
            result = parse_mlx_payload(payload)
            output = json.dumps(result, indent=args.indent)

            if args.output:
                args.output.write_text(output)
                print(f"Wrote parsed MLX data to {args.output}", file=sys.stderr)
            else:
                print(output)
        else:
            # Output raw bytes
            if args.output:
                args.output.write_bytes(payload)
                print(f"Wrote {len(payload)} bytes to {args.output}", file=sys.stderr)
            else:
                print(f"Delegate payload: {len(payload)} bytes", file=sys.stderr)
                # Show header info for MLX
                if len(payload) >= MLX_HEADER_LENGTH:
                    header = MLXHeader.from_bytes(payload)
                    print(f"  Magic: {header.magic!r}", file=sys.stderr)
                    print(f"  Data offset: {header.data_segment_offset}", file=sys.stderr)
                    print(f"  Data size: {header.data_segment_size}", file=sys.stderr)
        return

    # Parse the full PTE file
    result = parse_executorch_program(pte_data)
    result["file_size"] = len(pte_data)
    result["file_path"] = str(args.pte_file)

    # Format output
    if args.format == "summary":
        print(f"PTE File: {args.pte_file}")
        print(f"  Size: {len(pte_data):,} bytes")
        if "extended_header" in result:
            h = result["extended_header"]
            print(f"  Program size: {h['program_size']:,} bytes")
            print(f"  Segment offset: {h['segment_base_offset']:,}")
            print(f"  Segment size: {h['segment_data_size']:,} bytes")
        if "delegates" in result:
            print(f"  Delegates: {len(result['delegates'])}")
            for d in result["delegates"]:
                print(f"    - {d.get('id', 'unknown')}")
    else:
        output = json.dumps(result, indent=args.indent, default=str)

        if args.output:
            args.output.write_text(output)
            print(f"Wrote JSON to {args.output}", file=sys.stderr)
        else:
            print(output)


if __name__ == "__main__":
    main()
