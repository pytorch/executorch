#!/usr/bin/env python3
# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

"""
PTE Inspector - Extract and dump data from ExecuTorch .pte files.

This utility can:
1. Parse the PTE file structure (header, flatbuffer, segments)
2. Extract delegate payloads (e.g., MLX backend data)
3. Convert FlatBuffer data to JSON for inspection

Usage:
    python pte_inspector.py mlx_mlp.pte
    python pte_inspector.py mlx_mlp.pte --output output.json
    python pte_inspector.py mlx_mlp.pte --extract-delegate mlx --output mlx_payload.bin
"""

from __future__ import annotations

import argparse
import json
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional


# =============================================================================
# PTE File Header Parsing
# =============================================================================


@dataclass
class PTEHeader:
    """Extended header from a PTE file."""

    magic: bytes
    length: int
    program_size: int
    segment_base_offset: int
    segment_data_size: int

    @classmethod
    def from_bytes(cls, data: bytes) -> "PTEHeader":
        """Parse extended header from raw bytes."""
        if len(data) < 32:
            raise ValueError(f"Not enough data for header: {len(data)} < 32")

        magic = data[0:4]
        length = int.from_bytes(data[4:8], byteorder="little")
        program_size = int.from_bytes(data[8:16], byteorder="little")
        segment_base_offset = int.from_bytes(data[16:24], byteorder="little")
        segment_data_size = (
            int.from_bytes(data[24:32], byteorder="little") if length > 24 else 0
        )

        return cls(
            magic=magic,
            length=length,
            program_size=program_size,
            segment_base_offset=segment_base_offset,
            segment_data_size=segment_data_size,
        )

    def is_valid(self) -> bool:
        return self.magic == b"eh00" and self.length >= 24

    def to_dict(self) -> Dict[str, Any]:
        return {
            "magic": self.magic.decode("utf-8", errors="replace"),
            "length": self.length,
            "program_size": self.program_size,
            "segment_base_offset": self.segment_base_offset,
            "segment_data_size": self.segment_data_size,
        }


# =============================================================================
# MLX Delegate Payload Parsing
# =============================================================================

MLX_MAGIC = b"MLX0"
MLX_HEADER_LENGTH = 24


@dataclass
class MLXHeader:
    """Header from MLX delegate payload."""

    magic: bytes
    data_segment_offset: int
    data_segment_size: int

    @classmethod
    def from_bytes(cls, data: bytes) -> "MLXHeader":
        """Parse MLX header from raw bytes."""
        if len(data) < MLX_HEADER_LENGTH:
            raise ValueError(
                f"Not enough data for MLX header: {len(data)} < {MLX_HEADER_LENGTH}"
            )

        # Layout: [4 bytes padding][4 bytes magic][8 bytes offset][8 bytes size]
        magic = data[4:8]
        data_segment_offset = int.from_bytes(data[8:16], byteorder="little")
        data_segment_size = int.from_bytes(data[16:24], byteorder="little")

        return cls(
            magic=magic,
            data_segment_offset=data_segment_offset,
            data_segment_size=data_segment_size,
        )

    def is_valid(self) -> bool:
        return self.magic == MLX_MAGIC

    def to_dict(self) -> Dict[str, Any]:
        return {
            "magic": self.magic.decode("utf-8", errors="replace"),
            "data_segment_offset": self.data_segment_offset,
            "data_segment_size": self.data_segment_size,
        }


# Op type names - auto-generated by generate.py from schema.fbs
from executorch.backends.apple.mlx.serialization._generated_serializers import (
    MLX_OP_TYPE_NAMES,
)


def parse_mlx_flatbuffer(fb_data: bytes) -> Dict[str, Any]:  # noqa: C901
    """Parse MLX FlatBuffer data into a dict using the generated FlatBuffer bindings."""
    result = {}

    try:
        import os

        # Add the _generated directory to sys.path temporarily for imports
        import sys

        # Find the serialization/_generated directory and add it to path
        current_dir = os.path.dirname(os.path.abspath(__file__))
        generated_dir = os.path.join(current_dir, "serialization", "_generated")

        if not os.path.exists(generated_dir):
            # Try alternate location
            generated_dir = os.path.join(current_dir, "_generated")

        if os.path.exists(generated_dir) and generated_dir not in sys.path:
            sys.path.insert(0, generated_dir)

        from executorch.backends.apple.mlx.serialization._generated.mlx_delegate import (
            MLXGraph as FBMLXGraph,
        )

        graph = FBMLXGraph.MLXGraph.GetRootAs(fb_data, 0)

        result = {
            "version": graph.Version().decode("utf-8") if graph.Version() else None,
            "num_constant_tensors": graph.NumConstantTensors(),
            "num_non_constant_tensors": graph.NumNonConstantTensors(),
            "num_non_constant_values": graph.NumNonConstantValues(),
            "num_instructions": graph.InstructionsLength(),
            "input_map_length": graph.InputMapLength(),
            "output_map_length": graph.OutputMapLength(),
            "mutable_buffer_map_length": graph.MutableBufferMapLength(),
            "named_slots_length": graph.NamedSlotsLength(),
            "tensor_meta_length": graph.TensorMetaLength(),
        }

        # Extract instructions with full op details
        instructions = []
        for i in range(graph.InstructionsLength()):
            try:
                instr = graph.Instructions(i)
                if instr:
                    op_type = instr.OpType()
                    op_name = MLX_OP_TYPE_NAMES.get(op_type, f"Unknown({op_type})")
                    instr_info = {
                        "index": i,
                        "op_type": op_type,
                        "op_name": op_name,
                    }

                    # Parse op-specific fields
                    op_data = parse_op_node(instr, op_type, op_name)
                    if op_data:
                        instr_info.update(op_data)

                    instructions.append(instr_info)
            except Exception as e:
                instructions.append({"index": i, "error": f"parse_failed: {e}"})
        result["instructions"] = instructions

        # Extract named slots
        named_slots = []
        for i in range(graph.NamedSlotsLength()):
            try:
                ns = graph.NamedSlots(i)
                if ns:
                    slot_info = {
                        "name": ns.Name().decode("utf-8") if ns.Name() else None,
                    }
                    slot = ns.Slot()
                    if slot:
                        slot_info["slot_idx"] = slot.Idx()
                        slot_info["slot_type"] = slot.SlotType()
                    named_slots.append(slot_info)
            except Exception:
                named_slots.append({"index": i, "error": "parse_failed"})
        result["named_slots"] = named_slots

        # Extract tensor metadata
        tensor_meta = []
        for i in range(graph.TensorMetaLength()):
            try:
                tm = graph.TensorMeta(i)
                if tm:
                    meta = {
                        "index": i,
                        "dtype": tm.Dtype(),
                        "shape": [tm.Shape(j) for j in range(tm.ShapeLength())],
                    }
                    if tm.StridesLength() > 0:
                        meta["strides"] = [
                            tm.Strides(j) for j in range(tm.StridesLength())
                        ]
                    tensor_meta.append(meta)
            except Exception:
                tensor_meta.append({"index": i, "error": "parse_failed"})
        result["tensor_meta"] = tensor_meta

        # Extract I/O maps
        def extract_slot_variants(length_fn, getter_fn) -> List[Dict]:
            slots = []
            for i in range(length_fn()):
                try:
                    sv = getter_fn(i)
                    if sv:
                        slots.append({"idx": sv.Idx(), "slot_type": sv.SlotType()})
                except Exception:
                    slots.append({"index": i, "error": "parse_failed"})
            return slots

        result["input_map"] = extract_slot_variants(
            graph.InputMapLength, graph.InputMap
        )
        result["output_map"] = extract_slot_variants(
            graph.OutputMapLength, graph.OutputMap
        )
        result["mutable_buffer_map"] = extract_slot_variants(
            graph.MutableBufferMapLength, graph.MutableBufferMap
        )

        # Extract constant segment info
        try:
            cs = graph.ConstantSegment()
            if cs:
                result["constant_segment"] = {
                    "offset": cs.Offset(),
                    "size": cs.Size(),
                }
        except Exception:
            pass

    except ImportError as e:
        result["error"] = f"FlatBuffer bindings not available: {e}"
        result["_fallback"] = "Using basic header parsing only"

    except Exception as e:
        result["error"] = f"FlatBuffer parse error: {e}"
        import traceback

        result["traceback"] = traceback.format_exc()

    return result


def parse_op_node(  # noqa: C901
    instr, op_type: int, op_name: str
) -> Optional[Dict[str, Any]]:
    """Parse the specific op node fields from an instruction.

    This function uses the generated FlatBuffer bindings and auto-generated
    field mappings to extract op-specific fields from each instruction type.
    """
    try:
        # Get the op union table
        op = instr.Op()
        if op is None:
            return None

        import os

        # Add the _generated directory to sys.path for nested imports
        import sys

        current_dir = os.path.dirname(os.path.abspath(__file__))
        generated_dir = os.path.join(current_dir, "serialization", "_generated")
        if os.path.exists(generated_dir) and generated_dir not in sys.path:
            sys.path.insert(0, generated_dir)

        result = {}

        # Import generated field mappings
        from executorch.backends.apple.mlx._generated_inspector import OP_NODE_FIELDS

        if op_name not in OP_NODE_FIELDS:
            return {"error": f"Unknown op type: {op_name}"}

        # Dynamically import the node class
        module = __import__(
            f"executorch.backends.apple.mlx.serialization._generated.mlx_delegate.{op_name}",
            fromlist=[op_name],
        )
        node_class = getattr(module, op_name)

        # Init the node from union member
        node = node_class()
        node.Init(op.Bytes, op.Pos)

        # Extract fields based on generated mappings
        for field_name, accessor_name, kind in OP_NODE_FIELDS[op_name]:
            try:
                if kind == "tid":
                    method = getattr(node, accessor_name)
                    t = method()
                    result[field_name] = {"tid": t.Idx()} if t else None
                elif kind == "vid":
                    method = getattr(node, accessor_name)
                    v = method()
                    result[field_name] = {"vid": v.Idx()} if v else None
                elif kind == "int_or_vid":
                    method = getattr(node, accessor_name)
                    iov = method()
                    if iov is None:
                        result[field_name] = None
                    elif iov.IsVid():
                        v = iov.Vid()
                        result[field_name] = {"vid": v.Idx()} if v else None
                    else:
                        result[field_name] = {"literal": iov.Literal()}
                elif kind == "float_or_vid":
                    method = getattr(node, accessor_name)
                    fov = method()
                    if fov is None:
                        result[field_name] = None
                    elif fov.IsVid():
                        v = fov.Vid()
                        result[field_name] = {"vid": v.Idx()} if v else None
                    else:
                        result[field_name] = {"literal": fov.Literal()}
                elif kind == "int_list":
                    length_method = getattr(node, f"{accessor_name}Length")
                    item_method = getattr(node, accessor_name)
                    result[field_name] = [
                        item_method(i) for i in range(length_method())
                    ]
                elif kind == "int_or_vid_list":
                    length_method = getattr(node, f"{accessor_name}Length")
                    item_method = getattr(node, accessor_name)
                    items = []
                    for i in range(length_method()):
                        iov = item_method(i)
                        if iov is None:
                            items.append(None)
                        elif iov.IsVid():
                            v = iov.Vid()
                            items.append({"vid": v.Idx()} if v else None)
                        else:
                            items.append({"literal": iov.Literal()})
                    result[field_name] = items
                elif kind == "string":
                    method = getattr(node, accessor_name)
                    val = method()
                    result[field_name] = val.decode("utf-8") if val else None
                else:  # scalar
                    method = getattr(node, accessor_name)
                    result[field_name] = method()
            except Exception as e:
                result[field_name] = {"error": str(e)}

        # Filter out None values
        result = {k: v for k, v in result.items() if v is not None}
        return result if result else None

    except Exception as e:
        import traceback

        return {"parse_error": str(e), "traceback": traceback.format_exc()}


def parse_mlx_payload(payload: bytes) -> Dict[str, Any]:
    """Parse a complete MLX delegate payload."""
    header = MLXHeader.from_bytes(payload)

    if not header.is_valid():
        return {
            "error": f"Invalid MLX magic: {header.magic!r}",
            "header": header.to_dict(),
        }

    result = {
        "header": header.to_dict(),
    }

    # Extract FlatBuffer portion
    fb_start = MLX_HEADER_LENGTH
    fb_end = header.data_segment_offset
    fb_data = payload[fb_start:fb_end]

    result["flatbuffer_size"] = len(fb_data)
    result["graph"] = parse_mlx_flatbuffer(fb_data)

    # Data segment info
    if header.data_segment_size > 0:
        result["constant_data_size"] = header.data_segment_size

    return result


# =============================================================================
# ExecuTorch Program Parsing
# =============================================================================


def parse_executorch_program(pte_data: bytes) -> Dict[str, Any]:  # noqa: C901
    """Parse an ExecuTorch .pte file."""
    result: Dict[str, Any] = {}

    # Check for flatbuffer magic (first 4 bytes are root offset, next 4 are magic)
    if len(pte_data) < 8:
        raise ValueError("File too small to be a valid PTE file")

    fb_magic = pte_data[4:8]
    result["flatbuffer_magic"] = fb_magic.decode("utf-8", errors="replace")

    # Check for extended header (after flatbuffer header at offset 8)
    extended_header_offset = 8
    if len(pte_data) > extended_header_offset + 32:
        try:
            header = PTEHeader.from_bytes(pte_data[extended_header_offset:])
            if header.is_valid():
                result["extended_header"] = header.to_dict()

                # FlatBuffer data starts after the extended header
                fb_start = extended_header_offset + header.length

                result["flatbuffer_offset"] = fb_start
                result["flatbuffer_size"] = header.program_size
                result["segment_offset"] = header.segment_base_offset
                result["segment_size"] = header.segment_data_size
        except Exception as e:
            result["header_parse_error"] = str(e)

    # Try to parse the program FlatBuffer
    try:
        from executorch.exir._serialize._flatbuffer import _program_flatbuffer_to_json

        # The flatbuffer starts at offset 0 (the header is embedded in it)
        program_json = _program_flatbuffer_to_json(pte_data)
        program_data = json.loads(program_json)
        result["program"] = program_data

        # Extract delegate information
        if "execution_plan" in program_data:
            delegates = []
            for plan in program_data["execution_plan"]:
                if "delegates" in plan:
                    for delegate in plan["delegates"]:
                        delegate_info = {
                            "id": delegate.get("id"),
                            "processed_type": delegate.get("processed", {}).get(
                                "location"
                            ),
                        }
                        # Check for inline data
                        processed = delegate.get("processed", {})
                        if "data" in processed:
                            delegate_info["inline_data_size"] = len(processed["data"])
                        if "location" in processed:
                            delegate_info["location"] = processed["location"]
                        delegates.append(delegate_info)
            result["delegates"] = delegates

    except ImportError:
        result["program_parse_error"] = "ExecuTorch FlatBuffer parsing not available"
    except Exception as e:
        result["program_parse_error"] = str(e)

    return result


def extract_delegate_payload(  # noqa: C901
    pte_data: bytes, delegate_id: str, delegate_index: Optional[int] = None
) -> Optional[bytes]:
    """Extract a delegate payload from a PTE file.

    Args:
        pte_data: Raw bytes of the PTE file.
        delegate_id: ID substring to match (e.g., 'mlx' matches 'MLXBackend').
        delegate_index: If specified, extract the Nth matching delegate (0-based).
                       If None, extracts the first match.

    Returns:
        Delegate payload bytes, or None if not found.
    """
    try:
        from executorch.exir._serialize._flatbuffer import _program_flatbuffer_to_json

        program_json = _program_flatbuffer_to_json(pte_data)
        program_data = json.loads(program_json)

        # Parse extended header to get segment info
        extended_header = None
        if len(pte_data) > 40:
            try:
                header = PTEHeader.from_bytes(pte_data[8:])
                if header.is_valid():
                    extended_header = header
            except:
                pass

        # Look for the delegate in execution plans
        match_count = 0
        for plan in program_data.get("execution_plan", []):
            for delegate in plan.get("delegates", []):
                delegate_name = delegate.get("id", "")
                # Match by ID containing the search string (case-insensitive)
                if delegate_id.lower() in delegate_name.lower():
                    # Check if this is the delegate we want
                    if delegate_index is not None and match_count != delegate_index:
                        match_count += 1
                        continue

                    processed = delegate.get("processed", {})

                    # Check for inline data
                    if "data" in processed and processed["data"]:
                        # The data is stored as a list of ints (bytes)
                        data_list = processed["data"]
                        return bytes(data_list)

                    # Check for segment reference
                    location = processed.get("location", 0)
                    # Handle both string and integer location values
                    is_segment = location == 1 or location == "SEGMENT"
                    if is_segment:
                        if extended_header is None:
                            print(
                                "Warning: Delegate is in segment but no extended header found",
                                file=sys.stderr,
                            )
                            return None

                        # Get segment index and offset info
                        index = processed.get("index", 0)

                        # Look up segment in program's segments list
                        segments = program_data.get("segments", [])
                        if index < len(segments):
                            segment = segments[index]
                            seg_offset = segment.get("offset", 0)
                            seg_size = segment.get("size", 0)

                            # Calculate actual offset in file
                            data_offset = (
                                extended_header.segment_base_offset + seg_offset
                            )
                            return pte_data[data_offset : data_offset + seg_size]
                        else:
                            # Try using the segment directly from the delegate reference
                            print(
                                f"Warning: Segment index {index} not found in segments list (len={len(segments)})",
                                file=sys.stderr,
                            )
                            # Fall back: assume single segment containing all delegate data
                            return pte_data[
                                extended_header.segment_base_offset : extended_header.segment_base_offset
                                + extended_header.segment_data_size
                            ]

                    match_count += 1

        return None

    except Exception as e:
        print(f"Error extracting delegate: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        return None


def show_mlx_summary(pte_data: bytes) -> None:  # noqa: C901
    """Show summary of all MLX delegates in a PTE file."""
    try:
        from executorch.exir._serialize._flatbuffer import _program_flatbuffer_to_json

        program_json = _program_flatbuffer_to_json(pte_data)
        program_data = json.loads(program_json)

        # Parse extended header
        extended_header = None
        if len(pte_data) > 40:
            try:
                header = PTEHeader.from_bytes(pte_data[8:])
                if header.is_valid():
                    extended_header = header
            except:
                pass

        # Find all MLX delegates
        mlx_delegates = []
        for plan in program_data.get("execution_plan", []):
            for i, delegate in enumerate(plan.get("delegates", [])):
                if "mlx" in delegate.get("id", "").lower():
                    mlx_delegates.append((i, delegate))

        if not mlx_delegates:
            print("No MLX delegates found in this PTE file.")
            return

        print(f"\n{'='*70}")
        print("MLX DELEGATE SUMMARY")
        print(f"{'='*70}")
        print(f"File contains {len(mlx_delegates)} MLX delegate(s)\n")

        segments = program_data.get("segments", [])

        for idx, (delegate_idx, delegate) in enumerate(mlx_delegates):
            print(f"\n--- Delegate {idx} (plan index {delegate_idx}) ---")
            print(f"ID: {delegate.get('id', 'unknown')}")

            processed = delegate.get("processed", {})
            location = processed.get("location", 0)
            is_segment = location == 1 or location == "SEGMENT"

            if is_segment and extended_header:
                seg_index = processed.get("index", 0)
                if seg_index < len(segments):
                    segment = segments[seg_index]
                    seg_offset = segment.get("offset", 0)
                    seg_size = segment.get("size", 0)
                    print(
                        f"Segment: index={seg_index}, offset={seg_offset}, size={seg_size:,} bytes"
                    )

                    # Extract and parse the MLX payload
                    data_offset = extended_header.segment_base_offset + seg_offset
                    payload = pte_data[data_offset : data_offset + seg_size]

                    if len(payload) >= MLX_HEADER_LENGTH:
                        mlx_header = MLXHeader.from_bytes(payload)
                        if mlx_header.is_valid():
                            # Parse the flatbuffer
                            fb_start = MLX_HEADER_LENGTH
                            fb_end = mlx_header.data_segment_offset
                            fb_data = payload[fb_start:fb_end]

                            graph_info = parse_mlx_flatbuffer(fb_data)

                            print("\nMLX Graph Info:")
                            print(
                                f"  num_constant_tensors:     {graph_info.get('num_constant_tensors', '?')}"
                            )
                            print(
                                f"  num_non_constant_tensors: {graph_info.get('num_non_constant_tensors', '?')}"
                            )
                            print(
                                f"  num_non_constant_values:  {graph_info.get('num_non_constant_values', '?')}"
                            )
                            print(
                                f"  num_instructions:         {graph_info.get('num_instructions', '?')}"
                            )

                            print("\nI/O Maps:")
                            print(
                                f"  input_map length:          {graph_info.get('input_map_length', '?')}"
                            )
                            print(
                                f"  output_map length:         {graph_info.get('output_map_length', '?')}"
                            )
                            print(
                                f"  mutable_buffer_map length: {graph_info.get('mutable_buffer_map_length', '?')}"
                            )

                            # Calculate expected regular inputs
                            input_len = graph_info.get("input_map_length", 0)
                            mutable_len = graph_info.get("mutable_buffer_map_length", 0)
                            if input_len and mutable_len is not None:
                                regular_inputs = input_len - mutable_len
                                print(
                                    f"  => regular inputs expected: {regular_inputs} (input_map - mutable_buffer_map)"
                                )

                            # Show input_map details
                            if "input_map" in graph_info:
                                print("\n  Input Map Details:")
                                for i, slot in enumerate(graph_info["input_map"]):
                                    slot_type_name = {
                                        0: "Tensor",
                                        1: "Int",
                                        2: "Float",
                                        3: "Bool",
                                    }.get(slot.get("slot_type", 0), "Unknown")
                                    print(
                                        f"    [{i}]: idx={slot.get('idx')}, type={slot_type_name}"
                                    )

                            # Show mutable_buffer_map details
                            if (
                                "mutable_buffer_map" in graph_info
                                and graph_info["mutable_buffer_map"]
                            ):
                                print("\n  Mutable Buffer Map Details:")
                                for i, slot in enumerate(
                                    graph_info["mutable_buffer_map"]
                                ):
                                    slot_type_name = {
                                        0: "Tensor",
                                        1: "Int",
                                        2: "Float",
                                        3: "Bool",
                                    }.get(slot.get("slot_type", 0), "Unknown")
                                    print(
                                        f"    [{i}]: idx={slot.get('idx')}, type={slot_type_name}"
                                    )

                            # Show output_map details
                            if "output_map" in graph_info:
                                print("\n  Output Map Details:")
                                for i, slot in enumerate(graph_info["output_map"]):
                                    slot_type_name = {
                                        0: "Tensor",
                                        1: "Int",
                                        2: "Float",
                                        3: "Bool",
                                    }.get(slot.get("slot_type", 0), "Unknown")
                                    print(
                                        f"    [{i}]: idx={slot.get('idx')}, type={slot_type_name}"
                                    )

                            # Show constant data size
                            if mlx_header.data_segment_size > 0:
                                print(
                                    f"\n  Constant data size: {mlx_header.data_segment_size:,} bytes"
                                )

                        else:
                            print(f"  Invalid MLX magic: {mlx_header.magic!r}")
            else:
                print("  Location: inline or unknown")

        print(f"\n{'='*70}\n")

    except Exception as e:
        print(f"Error showing MLX summary: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()


# =============================================================================
# Main CLI
# =============================================================================


def main():  # noqa: C901
    parser = argparse.ArgumentParser(
        description="Inspect ExecuTorch .pte files and extract data"
    )
    parser.add_argument("pte_file", type=Path, help="Path to the .pte file")
    parser.add_argument(
        "--output", "-o", type=Path, help="Output file (default: stdout)"
    )
    parser.add_argument(
        "--extract-delegate",
        type=str,
        metavar="ID",
        help="Extract delegate payload by ID (e.g., 'mlx')",
    )
    parser.add_argument(
        "--delegate-index",
        type=int,
        default=None,
        metavar="N",
        help="Index of delegate to extract (0-based). If not specified, extracts first matching delegate.",
    )
    parser.add_argument(
        "--parse-mlx",
        action="store_true",
        help="Parse extracted MLX payload (use with --extract-delegate mlx)",
    )
    parser.add_argument(
        "--mlx-summary",
        action="store_true",
        help="Show summary of all MLX delegates (input/output/mutable buffer counts)",
    )
    parser.add_argument(
        "--format",
        choices=["json", "summary"],
        default="json",
        help="Output format (default: json)",
    )
    parser.add_argument(
        "--indent",
        type=int,
        default=2,
        help="JSON indentation (default: 2)",
    )

    args = parser.parse_args()

    # Read PTE file
    if not args.pte_file.exists():
        print(f"Error: File not found: {args.pte_file}", file=sys.stderr)
        sys.exit(1)

    pte_data = args.pte_file.read_bytes()
    print(f"Loaded {len(pte_data)} bytes from {args.pte_file}", file=sys.stderr)

    # Handle MLX summary
    if args.mlx_summary:
        show_mlx_summary(pte_data)
        return

    # Handle delegate extraction
    if args.extract_delegate:
        payload = extract_delegate_payload(pte_data, args.extract_delegate)
        if payload is None:
            print(
                f"Error: Delegate '{args.extract_delegate}' not found", file=sys.stderr
            )
            sys.exit(1)

        if args.parse_mlx and args.extract_delegate.lower() == "mlx":
            # Parse and output as JSON
            result = parse_mlx_payload(payload)
            output = json.dumps(result, indent=args.indent)

            if args.output:
                args.output.write_text(output)
                print(f"Wrote parsed MLX data to {args.output}", file=sys.stderr)
            else:
                print(output)
        else:
            # Output raw bytes
            if args.output:
                args.output.write_bytes(payload)
                print(f"Wrote {len(payload)} bytes to {args.output}", file=sys.stderr)
            else:
                print(f"Delegate payload: {len(payload)} bytes", file=sys.stderr)
                # Show header info for MLX
                if len(payload) >= MLX_HEADER_LENGTH:
                    header = MLXHeader.from_bytes(payload)
                    print(f"  Magic: {header.magic!r}", file=sys.stderr)
                    print(
                        f"  Data offset: {header.data_segment_offset}", file=sys.stderr
                    )
                    print(f"  Data size: {header.data_segment_size}", file=sys.stderr)
        return

    # Parse the full PTE file
    result = parse_executorch_program(pte_data)
    result["file_size"] = len(pte_data)
    result["file_path"] = str(args.pte_file)

    # Format output
    if args.format == "summary":
        print(f"PTE File: {args.pte_file}")
        print(f"  Size: {len(pte_data):,} bytes")
        if "extended_header" in result:
            h = result["extended_header"]
            print(f"  Program size: {h['program_size']:,} bytes")
            print(f"  Segment offset: {h['segment_base_offset']:,}")
            print(f"  Segment size: {h['segment_data_size']:,} bytes")
        if "delegates" in result:
            print(f"  Delegates: {len(result['delegates'])}")
            for d in result["delegates"]:
                print(f"    - {d.get('id', 'unknown')}")
    else:
        output = json.dumps(result, indent=args.indent, default=str)

        if args.output:
            args.output.write_text(output)
            print(f"Wrote JSON to {args.output}", file=sys.stderr)
        else:
            print(output)


if __name__ == "__main__":
    main()
