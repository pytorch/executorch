// Copyright (c) Meta Platforms, Inc. and affiliates.
//
// FlatBuffer schema for MLX delegate - THIS IS THE SOURCE OF TRUTH
// Defines the IR that gets serialized into the .pte file and executed by MLX runtime
//
// After editing this file, regenerate dependent files with:
//     python backends/apple/mlx/serialization/generate.py

namespace mlx_delegate;

// =============================================================================
// Core types
// =============================================================================

// Note: DTypeId enum removed - we now use ET's ScalarType (int8) directly.
// See runtime/core/portable_type/scalar_type.h for ScalarType values.

// Tensor slot identifier

// Tensor slot identifier - indexes into tensors array
struct Tid {
    idx: uint32;
}

// Value slot identifier - indexes into values array
// Values are stored as variant<int64, double, bool> at runtime
struct Vid {
    idx: uint32;
}

// For fields that can be either a literal int or a runtime Vid
table IntOrVid {
    literal: int64;  // widened to int64 for future-proofing
    vid: Vid;
    is_vid: bool = false;
}

// For fields that can be either a literal float or a runtime Vid
table FloatOrVid {
    literal: double;  // widened to double for future-proofing
    vid: Vid;
    is_vid: bool = false;
}

// For fields that can be either a tensor (Tid) or a scalar value (Vid)
// Used for ops like RoPE where offset can be a scalar position or a tensor of positions
table TidOrVid {
    tid: Tid;
    vid: Vid;
    is_vid: bool = false;  // false = use tid, true = use vid
}

// =============================================================================
// Op nodes - mirrors ops_schema.py dataclasses
// =============================================================================

table NoopNode {}

table AddmmNode {
    mat1: Tid (required);      // First matrix
    mat2: Tid (required);      // Second matrix
    out: Tid (required);
    bias: Tid;                 // optional - added to result
    alpha: float = 1.0;        // Scalar multiplier for mat1 @ mat2
    beta: float = 1.0;         // Scalar multiplier for bias
}

table LinearNode {
    x: Tid (required);
    weight: Tid (required);
    out: Tid (required);
    bias: Tid;  // optional
}

table ItemIntNode {
    x: Tid (required);
    out: Vid (required);
}

table ExpandDimsNode {
    x: Tid (required);
    out: Tid (required);
    axis: int32;
}

table TileNode {
    x: Tid (required);
    out: Tid (required);
    reps: [IntOrVid] (required);
}

table TakeAlongAxisNode {
    x: Tid (required);
    indices: Tid (required);
    out: Tid (required);
    axis: int32;
}

table TakeNode {
    x: Tid (required);
    out: Tid (required);
    index: int32;  // Single index to select
    axis: int32;   // Axis along which to select
}

table RMSNormNode {
    x: Tid (required);
    weight: Tid (required);
    out: Tid (required);
    eps: float;
}

table LayerNormNode {
    x: Tid (required);
    out: Tid (required);
    weight: Tid;  // optional
    bias: Tid;    // optional
    eps: float;
}

table RopeNode {
    x: Tid (required);
    out: Tid (required);
    head_dim: int32;
    offset: TidOrVid (required);  // Position offset: scalar (Vid) or tensor of positions (Tid)
    freqs: Tid;  // optional
    traditional: bool = false;
    base: float = 500000.0;  // Llama 3 default
    scale: float = 1.0;
}

table SdpaNode {
    q: Tid (required);
    k: Tid (required);
    v: Tid (required);
    out: Tid (required);
    scale: float;
    mask: Tid;  // optional
    causal: bool = false;
}

table AddNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table AddIntNode {
    a: IntOrVid (required);
    b: IntOrVid (required);
    out: Vid (required);
}

table SubtractIntNode {
    a: IntOrVid (required);
    b: IntOrVid (required);
    out: Vid (required);
}

table MultiplyIntNode {
    a: IntOrVid (required);
    b: IntOrVid (required);
    out: Vid (required);
}

table FloorDivideIntNode {
    a: IntOrVid (required);
    b: IntOrVid (required);
    out: Vid (required);
}

table SymSizeNode {
    a: Tid (required);
    dim: int32;
    out: Vid (required);
}

table MultiplyNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table DivideNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table SubtractNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table Conv1DNode {
    x: Tid (required);
    w: Tid (required);
    out: Tid (required);
    stride: int32 = 1;
    padding: int32 = 0;
    dilation: int32 = 1;
    groups: int32 = 1;
}

table Conv2DNode {
    x: Tid (required);
    w: Tid (required);
    out: Tid (required);
    stride_h: int32 = 1;
    stride_w: int32 = 1;
    padding_h: int32 = 0;
    padding_w: int32 = 0;
    dilation_h: int32 = 1;
    dilation_w: int32 = 1;
    groups: int32 = 1;
}

table GeluNode {
    x: Tid (required);
    out: Tid (required);
    approximate: string (required);  // "none" or "tanh"
}

table ARangeNode {
    out: Tid (required);
    start: IntOrVid (required);  // Can be literal or dynamic (from item())
    stop: IntOrVid (required);   // Can be literal or dynamic (from item())
    step: IntOrVid (required);   // Can be literal or dynamic
    scalar_type: int8 = null;    // ET ScalarType (optional - None means infer from context)
}

table SiluNode {
    x: Tid (required);
    out: Tid (required);
}

table SigmoidNode {
    x: Tid (required);
    out: Tid (required);
}

table TanhNode {
    x: Tid (required);
    out: Tid (required);
}

table SqueezeNode {
    x: Tid (required);
    out: Tid (required);
    dims: [int32];  // Optional list of dimensions to squeeze. If empty, squeeze all dims of size 1
}

table SplitNode {
    x: Tid (required);
    outs: [Tid] (required);  // Multiple output tensor IDs (one for each split chunk)
    sizes: [IntOrVid] (required);  // Split sizes (can be dynamic)
    axis: int32;
}

table RsqrtNode {
    x: Tid (required);
    out: Tid (required);
}

table MaximumNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table MinimumNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table LogNode {
    x: Tid (required);
    out: Tid (required);
}

table SoftmaxNode {
    x: Tid (required);
    out: Tid (required);
    axis: int32;  // Dimension to compute softmax over
}

table BroadcastToNode {
    x: Tid (required);
    out: Tid (required);
    shape: [IntOrVid] (required);  // Target shape to broadcast to
}

table PadNode {
    x: Tid (required);
    out: Tid (required);
    pad_width: [int32] (required);  // Padding pairs: [(before_0, after_0), (before_1, after_1), ...]
    mode: string (required);        // "constant" or "edge"
    constant_value: float = 0.0;    // Value to pad with (for constant mode)
}

table WhereNode {
    condition: Tid (required);
    x: Tid (required);
    y: Tid (required);
    out: Tid (required);
}

table ReshapeNode {
    x: Tid (required);
    out: Tid (required);
    shape: [IntOrVid] (required);
}

table TransposeNode {
    x: Tid (required);
    out: Tid (required);
    perm: [int32] (required);
}

table ContiguousNode {
    x: Tid (required);
    out: Tid (required);
}

table IdCopyNode {
    x: Tid (required);
    out: Tid (required);
}

table GatherNode {
    table_: Tid (required);  // 'table' is reserved in flatbuffers
    ids: Tid (required);
    out: Tid (required);
}

table SliceNode {
    x: Tid (required);
    out: Tid (required);
    axis: IntOrVid (required);
    start: IntOrVid (required);
    stop: IntOrVid (required);
}

table AsTypeNode {
    x: Tid (required);
    out: Tid (required);
    scalar_type: int8;           // ET ScalarType
}

table QuantizedLinearNode {
    x: Tid (required);
    w: Tid (required);
    scales: Tid (required);
    out: Tid (required);
    biases: Tid;  // optional - quantization biases (required if scale_only=false)
    bias: Tid;    // optional - neural network bias
    group_size: int32;
    bits: int32;
    mode: string (required);
    out_scalar_type: int8;       // ET ScalarType for output
    scale_only: bool = false;  // if true, compute biases = -scales * 2^(bits-1); if false, biases tensor required
}

table ConcatenateNode {
    tensors: [Tid] (required);  // List of tensors to concatenate
    out: Tid (required);
    axis: int32;
}

table FullNode {
    out: Tid (required);
    shape: [IntOrVid] (required);
    v: float;
    scalar_type: int8;           // ET ScalarType
}

table FullLikeNode {
    x: Tid (required);          // Input tensor to copy shape from
    out: Tid (required);
    v: float;                   // Fill value
    scalar_type: int8 = null;   // ET ScalarType (optional - if null, use x's dtype)
}

table ArgmaxNode {
    x: Tid (required);
    out: Tid (required);
    axis: int32;
    keepdims: bool = false;
}

table SliceUpdateNode {
    dst: Tid (required);
    update: Tid (required);
    axis: IntOrVid (required);
    start: IntOrVid (required);
    stop: IntOrVid (required);
}

// Index-based update: copies update tensor into dst at positions specified by 1D indices
// Similar to torch.index_copy but performs in-place update
// Runtime optimizes these into slice_update calls for contiguous runs
table IndexUpdateNode {
    dst: Tid (required);       // destination tensor to update
    update: Tid (required);    // source tensor to copy from
    indices: Tid (required);   // 1D tensor of indices along axis
    axis: int32;               // dimension to index along
}

table QuantizedGatherNode {
    table_q: Tid (required);
    scales: Tid (required);
    ids: Tid (required);
    out: Tid (required);
    biases: Tid;  // optional
    group_size: int32;
    bits: int32;
    mode: string (required);
    out_scalar_type: int8;       // ET ScalarType for output
}

// Comparison ops (return bool arrays)
table LessNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table LessEqualNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table GreaterNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table GreaterEqualNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table EqualNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table NotEqualNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

// Logical ops
table LogicalNotNode {
    a: Tid (required);
    out: Tid (required);
}

table LogicalAndNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table LogicalOrNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

// Triangular matrix ops
table TriNode {
    out: Tid (required);
    n: IntOrVid (required);   // Number of rows
    m: IntOrVid (required);   // Number of columns
    k: int32 = 0;             // Diagonal offset: 0=main, +above, -below
    scalar_type: int8;        // ET ScalarType
}

table TrilNode {
    x: Tid (required);
    out: Tid (required);
    k: int32 = 0;  // Diagonal offset: 0=main, +above, -below
}

table TriuNode {
    x: Tid (required);
    out: Tid (required);
    k: int32 = 0;  // Diagonal offset: 0=main, +above, -below
}

// =============================================================================
// Math ops - Unary element-wise
// =============================================================================

table FloorNode {
    x: Tid (required);
    out: Tid (required);
}

table CeilNode {
    x: Tid (required);
    out: Tid (required);
}

table SquareNode {
    x: Tid (required);
    out: Tid (required);
}

table ExpNode {
    x: Tid (required);
    out: Tid (required);
}

table SinNode {
    x: Tid (required);
    out: Tid (required);
}

table CosNode {
    x: Tid (required);
    out: Tid (required);
}

table TanNode {
    x: Tid (required);
    out: Tid (required);
}

table ArcsinNode {
    x: Tid (required);
    out: Tid (required);
}

table ArccosNode {
    x: Tid (required);
    out: Tid (required);
}

table ArctanNode {
    x: Tid (required);
    out: Tid (required);
}

table SinhNode {
    x: Tid (required);
    out: Tid (required);
}

table CoshNode {
    x: Tid (required);
    out: Tid (required);
}

table ArcsinhNode {
    x: Tid (required);
    out: Tid (required);
}

table ArccoshNode {
    x: Tid (required);
    out: Tid (required);
}

table ArctanhNode {
    x: Tid (required);
    out: Tid (required);
}

table Log2Node {
    x: Tid (required);
    out: Tid (required);
}

table Log10Node {
    x: Tid (required);
    out: Tid (required);
}

table Log1pNode {
    x: Tid (required);
    out: Tid (required);
}

table ErfNode {
    x: Tid (required);
    out: Tid (required);
}

table Expm1Node {
    x: Tid (required);
    out: Tid (required);
}

table RoundNode {
    x: Tid (required);
    out: Tid (required);
    decimals: int32 = 0;
}

table ReciprocalNode {
    x: Tid (required);
    out: Tid (required);
}

table SqrtNode {
    x: Tid (required);
    out: Tid (required);
}

table AbsNode {
    x: Tid (required);
    out: Tid (required);
}

table NegNode {
    x: Tid (required);
    out: Tid (required);
}

// =============================================================================
// Math ops - Binary element-wise
// =============================================================================

table Atan2Node {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table LogAddExpNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table FloorDivideNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

table PowerNode {
    a: Tid (required);
    b: Tid (required);
    out: Tid (required);
}

// =============================================================================
// Math ops - Reduction
// =============================================================================

table LogSumExpNode {
    x: Tid (required);
    out: Tid (required);
    axes: [int32];
    keepdims: bool = false;
}

table SumNode {
    x: Tid (required);
    out: Tid (required);
    axes: [int32];           // Empty = reduce all
    keepdims: bool = false;
}

table MeanNode {
    x: Tid (required);
    out: Tid (required);
    axes: [int32];           // Empty = reduce all
    keepdims: bool = false;
}

table VarNode {
    x: Tid (required);
    out: Tid (required);
    axes: [int32];           // Empty = reduce all
    keepdims: bool = false;
    ddof: int32 = 0;         // Delta degrees of freedom (0=population var, 1=sample var)
}

table StdNode {
    x: Tid (required);
    out: Tid (required);
    axes: [int32];           // Empty = reduce all
    keepdims: bool = false;
    ddof: int32 = 0;         // Delta degrees of freedom
}

table ProdNode {
    x: Tid (required);
    out: Tid (required);
    axes: [int32];           // Empty = reduce all
    keepdims: bool = false;
}

table MaxNode {
    x: Tid (required);
    out: Tid (required);
    axes: [int32];           // Empty = reduce all
    keepdims: bool = false;
}

table MinNode {
    x: Tid (required);
    out: Tid (required);
    axes: [int32];           // Empty = reduce all
    keepdims: bool = false;
}

table ArgminNode {
    x: Tid (required);
    out: Tid (required);
    axis: int32;
    keepdims: bool = false;
}

table MedianNode {
    x: Tid (required);
    out: Tid (required);
    axes: [int32];           // Empty = reduce all
    keepdims: bool = false;
}

// =============================================================================
// Union of all op types
// =============================================================================

union OpNode {
    NoopNode,
    AddmmNode,
    LinearNode,
    ItemIntNode,
    ExpandDimsNode,
    TileNode,
    TakeAlongAxisNode,
    TakeNode,
    RMSNormNode,
    LayerNormNode,
    RopeNode,
    SdpaNode,
    AddNode,
    AddIntNode,
    SubtractIntNode,
    MultiplyIntNode,
    FloorDivideIntNode,
    SymSizeNode,
    MultiplyNode,
    DivideNode,
    SubtractNode,
    Conv1DNode,
    Conv2DNode,
    GeluNode,
    ARangeNode,
    SiluNode,
    SigmoidNode,
    TanhNode,
    SqueezeNode,
    SplitNode,
    RsqrtNode,
    MaximumNode,
    MinimumNode,
    LogNode,
    SoftmaxNode,
    BroadcastToNode,
    PadNode,
    WhereNode,
    ReshapeNode,
    TransposeNode,
    ContiguousNode,
    IdCopyNode,
    GatherNode,
    SliceNode,
    AsTypeNode,
    QuantizedLinearNode,
    ConcatenateNode,
    FullNode,
    FullLikeNode,
    ArgmaxNode,
    SliceUpdateNode,
    IndexUpdateNode,
    QuantizedGatherNode,
    LessNode,
    LessEqualNode,
    GreaterNode,
    GreaterEqualNode,
    EqualNode,
    NotEqualNode,
    LogicalNotNode,
    LogicalAndNode,
    LogicalOrNode,
    TriNode,
    TrilNode,
    TriuNode,
    // Math ops - Unary
    FloorNode,
    CeilNode,
    SquareNode,
    ExpNode,
    SinNode,
    CosNode,
    TanNode,
    ArcsinNode,
    ArccosNode,
    ArctanNode,
    SinhNode,
    CoshNode,
    ArcsinhNode,
    ArccoshNode,
    ArctanhNode,
    Log2Node,
    Log10Node,
    Log1pNode,
    ErfNode,
    Expm1Node,
    RoundNode,
    ReciprocalNode,
    SqrtNode,
    AbsNode,
    NegNode,
    // Math ops - Binary
    Atan2Node,
    LogAddExpNode,
    FloorDivideNode,
    PowerNode,
    // Math ops - Reduction
    LogSumExpNode,
    SumNode,
    MeanNode,
    VarNode,
    StdNode,
    ProdNode,
    MaxNode,
    MinNode,
    ArgminNode,
    MedianNode
}

// =============================================================================
// Instruction wrapper
// =============================================================================

table Instruction {
    op: OpNode (required);
}

// =============================================================================
// Tensor metadata
// =============================================================================

table TensorMeta {
    shape: [IntOrVid] (required);  // Can be literal ints or Vid refs for dynamic dims
    scalar_type: int8;             // ET ScalarType value (see runtime/core/portable_type/scalar_type.h)
    dim_order: [uint8];            // Memory layout order (matches TensorLayout.dim_order, DimOrderType = uint8_t)
}

// =============================================================================
// Slot variant for I/O mapping
// =============================================================================

enum SlotType : byte {
    TensorSlot = 0,
    IntValueSlot = 1,
    FloatValueSlot = 2,
    BoolValueSlot = 3
}

table SlotVariant {
    idx: uint32;
    slot_type: SlotType = TensorSlot;
}

// =============================================================================
// Name to slot mapping entry
// =============================================================================

table NamedSlot {
    name: string (required);
    slot: SlotVariant (required);
}

// =============================================================================
// Root type: MLX Graph
// =============================================================================

table MLXGraph {
    // Version for compatibility
    version: string;

    // Tensor slot counts

    num_constant_tensors: uint32;
    num_input_tensors: uint32;
    num_output_tensors: uint32;
    num_mutable_buffer_tensors: uint32;
    num_temp_tensors: uint32;
    num_values: uint32;

    // Instructions
    instructions: [Instruction] (required);      // Run every execute() call
    init_instructions: [Instruction];            // Run once at init()

    // I/O mappings
    input_map: [SlotVariant];
    output_map: [SlotVariant];
    mutable_buffer_map: [SlotVariant];

    // Name to slot lookup (used for constant/mutable buffer keys in named_data_map)
    named_slots: [NamedSlot];

    // Tensor metadata (for non-temp tensors), indexed by Tid
    tensor_meta: [TensorMeta];
}

root_type MLXGraph;
