/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

#ifndef INDEXING_GLSLH
#define INDEXING_GLSLH

#include "common.glslh"

#extension GL_EXT_control_flow_attributes : require

#define DIMLIMIT 8
#define DIMLIMIT_DIV4 2

//
// Hashed layout utils
//

/*
 * The hashed layout is a packed int32 where each group of 4 bits contain some
 * information about the memory layout of a tensor buffer or texture. It is
 * passed into shaders as a specialization constant and allows shader compilers
 * to select optimized code paths suited for a particular memory layout.
 *
 * Currently the following information is packed into the layout integer:
 * - bits  0-15: layout_id: dim order (buffer) or axis_map (texture)
 *   - bits  0- 3: dim_order[0] / axis_map[0]
 *   - bits  4- 7: dim_order[1] / axis_map[1]
 *   - bits  8-11: dim_order[2] / axis_map[2]
 *   - bits 12-15: dim_order[3] / axis_map[3]
 * - bits 16-31: packed_dim_info
 *   - bits 16-19: packed_dim
 *   - bits 20-23: outer_packed_dim
 *   - bits 24-27: packed_dim_block_size
 *   - bits 28-31: outer_packed_dim_block_size
 */

// Extracts the 4-bit packed value at the given position (0..7) from a 32-bit
// int. Position 0 corresponds to the least-significant 4 bits; position 7 to
// the most-significant.
int extract_4b(const int packed, const int pos) {
  return (packed >> (pos * 4)) & 0xF;
}

#define extract_buffer_packed_dim(layout) int(layout & 0xF)

// Corresponds to dim_order[:4] = [0, 1, 2, 3]
#define CONTIGUOUS_BUFFER_LAYOUT_ID 0x3210
// Corresponds to dim_order[:4] = [2, 0, 1, 3]
#define CHANNELS_LAST_BUFFER_LAYOUT_ID 0x3102

// Used as a default value for hashed layout ints, representing the most common
// layout used for buffer-backed tensors (i.e. contiguous buffers)
#define CONTIG_LAYOUT_INT 0x11103210

int layout_id(const int hashed_layout) {
  // Extract the first 16 bits
  return hashed_layout & 0xFFFF;
}

bool is_contiguous(const int hashed_layout) {
  return layout_id(hashed_layout) == CONTIGUOUS_BUFFER_LAYOUT_ID;
}

bool is_channels_last(const int hashed_layout) {
  return layout_id(hashed_layout) == CHANNELS_LAST_BUFFER_LAYOUT_ID;
}

#define get_fastest_dim(layout) ((layout) & 0xF)

// Extract packed dim info from hashed_layout (bits 16-31)
// These match the format created by create_hashed_layout() in Tensor.cpp
#define get_packed_dim(layout) (((layout) >> 16) & 0xF)
#define get_outer_packed_dim(layout) (((layout) >> 20) & 0xF)
#define get_packed_dim_block_size(layout) (((layout) >> 24) & 0xF)
#define get_outer_packed_dim_block_size(layout) (((layout) >> 28) & 0xF)

//
// BufferMetadata
//

struct BufferMetadata {
  uvec4 sizes[DIMLIMIT_DIV4];
  uvec4 dim_order[DIMLIMIT_DIV4];
  uvec4 strides[DIMLIMIT_DIV4];
  uvec2 ndim_numel;
};

uint ndim(const BufferMetadata meta) {
  return meta.ndim_numel[0];
}

int int_ndim(const BufferMetadata meta) {
  return int(meta.ndim_numel[0]);
}

uint numel(const BufferMetadata meta) {
  return meta.ndim_numel[1];
}

uint dim_order_at(const BufferMetadata meta, const int dim) {
  return meta.dim_order[div_4(dim)][mod_4(dim)];
}

uint dim_order_at(const BufferMetadata meta, const uint dim) {
  return meta.dim_order[div_4(dim)][mod_4(dim)];
}

uint stride_at(const BufferMetadata meta, const int dim) {
  return meta.strides[div_4(dim)][mod_4(dim)];
}

uint stride_at(const BufferMetadata meta, const uint dim) {
  return meta.strides[div_4(dim)][mod_4(dim)];
}

uint width(const BufferMetadata meta) {
  return meta.sizes[0][0];
}

uint height(const BufferMetadata meta) {
  return meta.sizes[0][1];
}

uint size_at(const BufferMetadata meta, const int dim) {
  return meta.sizes[div_4(dim)][mod_4(dim)];
}

uint size_at(const BufferMetadata meta, const uint dim) {
  return meta.sizes[div_4(dim)][mod_4(dim)];
}

bool are_equal(const BufferMetadata meta1, const BufferMetadata meta2) {
  // sizes and strides must be the same to be considered equal
  if (meta1.sizes[0] != meta2.sizes[0]) {
    return false;
  }
  if (meta1.sizes[1] != meta2.sizes[1]) {
    return false;
  }
  if (meta1.strides[0] != meta2.strides[0]) {
    return false;
  }
  if (meta1.strides[1] != meta2.strides[1]) {
    return false;
  }
  return true;
}

bool out_of_bounds(const uint bufi, const BufferMetadata meta) {
  return bufi >= meta.ndim_numel[1];
}

//
// TextureMetadata
//

struct TextureMetadata {
  ivec4 sizes;
  ivec3 limits;
  ivec4 axis_map;
  int packed_dim;
};

bool out_of_bounds(const ivec3 pos, const TextureMetadata meta) {
  return any(greaterThanEqual(pos, meta.limits));
}

//
// TensorIndex
//

struct TensorIndex {
  uvec4 data[DIMLIMIT_DIV4];
};

void initialize(out TensorIndex tidx) {
  tidx.data[0] = uvec4(0);
  tidx.data[1] = uvec4(0);
}

uint idx_at(const TensorIndex tidx, const int dim) {
  return tidx.data[div_4(dim)][mod_4(dim)];
}

uint idx_at(const TensorIndex tidx, const uint dim) {
  return tidx.data[div_4(dim)][mod_4(dim)];
}

uint x(const TensorIndex tidx) {
  return tidx.data[0][0];
}

bool out_of_bounds(const TensorIndex tidx, const BufferMetadata meta) {
  return any(greaterThanEqual(tidx.data[0], meta.sizes[0])) ||
         any(greaterThanEqual(tidx.data[1], meta.sizes[1]));
}

//
// TensorIndex4D (useful for texture backed tensors)
//

struct TensorIndex4D {
  ivec4 data;
};

TensorIndex4D zero_tensor4d_idx() {
  TensorIndex4D tidx;
  tidx.data = ivec4(0);
  return tidx;
}

bool out_of_bounds(const TensorIndex4D tidx, const BufferMetadata meta) {
  return any(greaterThanEqual(tidx.data, meta.sizes[0]));
}

bool out_of_bounds(const TensorIndex4D tidx, const TextureMetadata meta) {
  return any(greaterThanEqual(tidx.data, meta.sizes));
}

//
// TextureElementIndex
//

struct TextureElementIndex {
  ivec3 pos;
  int comp;
};

//
// Index Conversions
//

TensorIndex contiguous_idx_to_tensor_idx(
    const BufferMetadata meta,
    uint contiguous_idx) {
  TensorIndex tidx;

  uint contiguous_strides[DIMLIMIT];

  contiguous_strides[0] = 1;
  [[unroll]] for (int d = 1; d < DIMLIMIT; ++d) {
    contiguous_strides[d] = size_at(meta, d - 1) * contiguous_strides[d - 1];
  }

  [[unroll]] for (int d = DIMLIMIT - 1; d >= 0; --d) {
    tidx.data[div_4(d)][mod_4(d)] = contiguous_idx / contiguous_strides[d];
    contiguous_idx = contiguous_idx % contiguous_strides[d];
  }

  return tidx;
}

uint tensor_idx_to_contiguous_idx(
    const BufferMetadata meta,
    const TensorIndex tidx) {
  uint contiguous_strides[DIMLIMIT];

  contiguous_strides[0] = 1;
  [[unroll]] for (int d = 1; d < DIMLIMIT; ++d) {
    contiguous_strides[d] = size_at(meta, d - 1) * contiguous_strides[d - 1];
  }

  uint contig_idx = 0;
  [[unroll]] for (int d = 0; d < DIMLIMIT; ++d) {
    contig_idx += contiguous_strides[d] * idx_at(tidx, d);
  }

  return contig_idx;
}

TensorIndex linear_idx_to_tensor_idx(
    const BufferMetadata meta,
    uint linear_idx) {
  TensorIndex tidx;
  initialize(tidx);
  int dim = int_ndim(meta);
  int i = 0;
  for (int d = max(dim - 1, 0); d >= 0; d--) {
    uint dim_idx = meta.dim_order[div_4(d)][mod_4(d)];
    uint dim_stride = meta.strides[div_4(dim_idx)][mod_4(dim_idx)];

    tidx.data[div_4(dim_idx)][mod_4(dim_idx)] = linear_idx / dim_stride;
    linear_idx = linear_idx % dim_stride;
  }
  return tidx;
}

TensorIndex linear_idx_to_tensor_idx_contig_case(
    const BufferMetadata meta,
    uint linear_idx) {
  TensorIndex tidx;

  [[unroll]] for (int d = DIMLIMIT - 1; d >= 0; --d) {
    tidx.data[div_4(d)][mod_4(d)] = linear_idx / stride_at(meta, d);
    linear_idx = linear_idx % stride_at(meta, d);
  }

  return tidx;
}

TensorIndex linear_idx_to_tensor_idx_channelslast_case(
    const BufferMetadata meta,
    uint linear_idx) {
  TensorIndex tidx;

  const uint dim_order[DIMLIMIT] = uint[DIMLIMIT](2, 0, 1, 3, 6, 5, 4, 7);

  [[unroll]] for (int d = DIMLIMIT - 1; d >= 0; --d) {
    uint dim = dim_order[d];
    tidx.data[div_4(dim)][mod_4(dim)] = linear_idx / stride_at(meta, dim);
    linear_idx = linear_idx % stride_at(meta, dim);
  }

  return tidx;
}

TensorIndex linear_idx_to_tensor_idx(
    const BufferMetadata meta,
    uint linear_idx,
    int hashed_layout) {
  if (is_contiguous(hashed_layout)) {
    return linear_idx_to_tensor_idx_contig_case(meta, linear_idx);
  } else if (is_channels_last(hashed_layout)) {
    return linear_idx_to_tensor_idx_channelslast_case(meta, linear_idx);
  }
  return linear_idx_to_tensor_idx(meta, linear_idx);
}

/*
 * Convert a linear texel index to a TensorIndex4D.
 *
 * This function is used for texel-based dispatch where each thread handles
 * one packed texel (4 elements along the packed dimension). The texel index
 * is decomposed using the dim_order and strides from the tensor's layout.
 *
 * The strides in BufferMetadata should already be in texel space (with packed
 * dimension size divided by 4).
 *
 * Parameters:
 *   meta: BufferMetadata with tensor sizes and texel-space strides
 *   texel_idx: Linear index into packed texels (0 to num_texels-1)
 *   hashed_layout: Packed layout info containing dim_order and packed_dim
 *
 * Returns: TensorIndex4D with logical tensor coordinates (packed dim is base of 4-element block)
 */
TensorIndex4D texel_idx_to_tensor4d_idx(
    const BufferMetadata meta,
    uint texel_idx,
    const int hashed_layout) {
  TensorIndex4D tidx;

  const int packed_dim = get_packed_dim(hashed_layout);

  // Decompose texel_idx using dim_order from hashed_layout and strides from meta
  // Iterate from slowest-varying dimension (d=3) to fastest (d=0)
  // This follows the pattern of linear_idx_to_tensor_idx in indexing.glslh
  [[unroll]] for (int d = 3; d >= 0; d--) {
    // Get dim index from hashed_layout's dim_order (bits 0-15)
    int dim_idx = extract_4b(hashed_layout, d);

    // Get stride for this dimension from BufferMetadata
    uint dim_stride = meta.strides[0][dim_idx];

    // Compute coordinate for this dimension
    tidx.data[dim_idx] = int(texel_idx / dim_stride);
    texel_idx = texel_idx % dim_stride;
  }

  // Convert packed dimension from texel index to element index
  tidx.data[packed_dim] *= 4;

  return tidx;
}

uint tensor_idx_to_linear_idx(
    const BufferMetadata meta,
    const TensorIndex tidx) {
  uint lin_idx = 0;
  [[unroll]] for (int d = 0; d < DIMLIMIT; ++d) {
    lin_idx += stride_at(meta, d) * idx_at(tidx, d);
  }

  return lin_idx;
}

void clamp_tensor_idx(const BufferMetadata meta, inout TensorIndex tidx) {
  tidx.data[0] = min(tidx.data[0], meta.sizes[0] - 1);
  tidx.data[1] = min(tidx.data[1], meta.sizes[1] - 1);
}

// Does not account for axis mapping
TensorIndex4D texture_pos_to_tensor4d_idx_simple(
    const TextureMetadata meta, const ivec3 pos) {
  TensorIndex4D tidx;
  tidx.data.xyz = pos;
  tidx.data.w = 0;
  tidx.data[meta.packed_dim] *= 4;

  // Compute batch idx accounting for batch concatenation, assuming channels as
  // the concatenation dim.
  if (meta.sizes.w > 1) {
    int channels = meta.sizes.z;
    if (meta.packed_dim == 2) {
      channels = align_up_4(channels);
    }
    tidx.data.w = tidx.data.z / channels;
    tidx.data.z = tidx.data.z % channels;
  }
  return tidx;
}

// Does not account for axis mapping
ivec3 tensor4d_idx_to_texel_pos_simple(
    const TextureMetadata meta, const TensorIndex4D tidx) {
  ivec3 texel_pos;

  const int packed_dim_idx = tidx.data[meta.packed_dim];

  texel_pos = tidx.data.xyz;
  texel_pos[meta.packed_dim] = div_4(packed_dim_idx);

  // Account for batch concatenation, assuming channels as the concatenation dim
  if (meta.sizes.w > 1) {
    int channels_ntexels = meta.sizes.z;
    if (meta.packed_dim == 2) {
      channels_ntexels = div_up_4(channels_ntexels);
    }
    texel_pos.z += tidx.data.w * channels_ntexels;
  }

  return texel_pos;
}

// Does not account for axis mapping
TextureElementIndex tensor4d_idx_to_texture_element_idx_simple(
    const TextureMetadata meta, const TensorIndex4D tidx) {
  const int packed_dim_idx = tidx.data[meta.packed_dim];
  TextureElementIndex tex_idx;
  tex_idx.pos = tidx.data.xyz;
  tex_idx.pos[meta.packed_dim] = div_4(packed_dim_idx);
  tex_idx.comp = mod_4(packed_dim_idx);

  // Account for batch concatenation, assuming channels as the concatenation dim
  if (meta.sizes.w > 1) {
    int channels_ntexels = meta.sizes.z;
    if (meta.packed_dim == 2) {
      channels_ntexels = div_up_4(channels_ntexels);
    }
    tex_idx.pos.z += tidx.data.w * channels_ntexels;
  }

  return tex_idx;
}

uint tensor4d_idx_to_linear_idx(
    const BufferMetadata meta,
    const TensorIndex4D tidx) {
  uint lin_idx = 0;
  for (int d = 0; d < 4; ++d) {
    lin_idx += meta.strides[0][d] * tidx.data[d];
  }
  return lin_idx;
}

//
// Block-packed tensor indexing
//

/*
 * Get the number of elements per block from hashed_layout.
 */
int get_block_numel(const int hashed_layout) {
  const int inner_block_size = get_packed_dim_block_size(hashed_layout);
  const int outer_block_size = get_outer_packed_dim_block_size(hashed_layout);
  return inner_block_size * outer_block_size;
}

/*
 * Compute the intra-block index (position within a block).
 *
 * Within a block, elements are stored with inner dimension varying fastest:
 *   intra_block_idx = outer_offset * inner_block_size + inner_offset
 *
 * Parameters:
 *   tidx: TensorIndex4D with logical tensor coordinates
 *   hashed_layout: Packed layout info
 *
 * Returns: Intra-block index (0 to block_numel-1)
 */
int tensor4d_idx_to_intra_block_idx(
    const TensorIndex4D tidx,
    const int hashed_layout) {
  const int inner_dim = get_packed_dim(hashed_layout);
  const int outer_dim = get_outer_packed_dim(hashed_layout);
  const int inner_block_size = get_packed_dim_block_size(hashed_layout);
  const int outer_block_size = get_outer_packed_dim_block_size(hashed_layout);

  const int inner_offset = tidx.data[inner_dim] % inner_block_size;
  const int outer_offset = tidx.data[outer_dim] % outer_block_size;

  return outer_offset * inner_block_size + inner_offset;
}

/*
 * Convert a tensor index to a block-space linear index.
 *
 * The tensor index is converted to block-space coordinates by dividing
 * the packed dimensions by their block sizes, then the linear index is
 * computed using the block-space strides from BufferMetadata.
 *
 * Parameters:
 *   meta: BufferMetadata with block-space strides
 *   tidx: TensorIndex4D with logical tensor coordinates
 *   hashed_layout: Packed layout info
 *
 * Returns: Linear index in block space
 */
int tensor4d_idx_to_block_idx(
    const BufferMetadata meta,
    TensorIndex4D tidx,
    const int hashed_layout) {
  // Extract packed dim info
  const int inner_dim = get_packed_dim(hashed_layout);
  const int outer_dim = get_outer_packed_dim(hashed_layout);
  const int inner_block_size = get_packed_dim_block_size(hashed_layout);
  const int outer_block_size = get_outer_packed_dim_block_size(hashed_layout);

  // Convert to block-space coordinates
  tidx.data[inner_dim] = tidx.data[inner_dim] / inner_block_size;
  tidx.data[outer_dim] = tidx.data[outer_dim] / outer_block_size;

  // Compute block-space linear index
  int block_idx = 0;
  [[unroll]] for (int d = 0; d < 4; ++d) {
    block_idx += int(meta.strides[0][d]) * tidx.data[d];
  }
  return block_idx;
}

/*
 * Convert a tensor index to a linear buffer index for block-packed layouts.
 *
 * For block-packed tensors:
 * - Elements are grouped into blocks of (inner_block_size × outer_block_size)
 * - Strides in BufferMetadata are in "block space"
 * - Final index = block_index × block_numel + intra_block_index
 *
 * Parameters:
 *   meta: BufferMetadata containing sizes and block-space strides
 *   tidx: TensorIndex4D with logical tensor coordinates
 *   hashed_layout: Packed layout info from create_hashed_layout() (see
 * indexing.glslh)
 *
 * Returns: Linear buffer index for the element
 */
int tensor4d_idx_to_buf_idx(
    const BufferMetadata meta,
    const TensorIndex4D tidx,
    const int hashed_layout) {
  const int block_idx = tensor4d_idx_to_block_idx(meta, tidx, hashed_layout);
  const int intra_block_idx =
      tensor4d_idx_to_intra_block_idx(tidx, hashed_layout);
  const int block_numel = get_block_numel(hashed_layout);

  return block_idx * block_numel + intra_block_idx;
}

/*
 * Convert a tensor index to a texel index for block-packed layouts.
 *
 * For texel-packed tensors (outer_block_size == 1):
 * - Each block corresponds to one texel
 * - Returns block_idx directly
 *
 * For block-packed tensors (outer_block_size > 1, e.g., 4x4 blocks):
 * - Each block contains 4 texels (16 elements / 4 elements per texel)
 * - texel_idx = block_idx * 4 + (intra_block_idx / 4)
 *
 * Parameters:
 *   meta: BufferMetadata containing sizes and block-space strides
 *   tidx: TensorIndex4D with logical tensor coordinates
 *   hashed_layout: Packed layout info
 *
 * Returns: Linear texel index
 */
int tensor4d_idx_to_texel_idx(
    const BufferMetadata meta,
    const TensorIndex4D tidx,
    const int hashed_layout) {
  const int block_idx = tensor4d_idx_to_block_idx(meta, tidx, hashed_layout);

  if (get_outer_packed_dim_block_size(hashed_layout) == 4) {
    const int intra_block_idx =
        tensor4d_idx_to_intra_block_idx(tidx, hashed_layout);
    return block_idx * 4 + div_4(intra_block_idx);
  }

  return block_idx;
}

//
// Debug utilities
//

#ifdef DEBUG_MODE

void printTensorIndex(const TensorIndex tidx) {
    debugPrintfEXT(
        "TensorIndex: tidx=[%u %u %u %u %u %u %u %u]\\n",
        tidx.data[0][0], tidx.data[0][1], tidx.data[0][2], tidx.data[0][3],
        tidx.data[1][0], tidx.data[1][1], tidx.data[1][2], tidx.data[1][3]
    );
}

void printTensorIndex4D(const TensorIndex4D tidx) {
    debugPrintfEXT(
        "TensorIndex4: [%d, %d, %d, %d]\\n",
        tidx.data[0], tidx.data[1], tidx.data[2], tidx.data[3]
    );
}

void printTextureElementIndex(const TextureElementIndex tex_idx) {
    debugPrintfEXT(
        "TextureElementIndex: pos=[%d %d %d] comp=%d\\n",
        tex_idx.pos.x, tex_idx.pos.y, tex_idx.pos.z, tex_idx.comp
    );
}


void printBufferMetadata(const BufferMetadata meta) {
    debugPrintfEXT(
        "BufferMetadata: ndim=%u numel=%u\\n  sizes=[%u %u %u %u %u %u %u %u]\\n  dim_order=[%u %u %u %u %u %u %u %u]\\n  strides=[%u %u %u %u %u %u %u %u]\\n",
        meta.ndim_numel[0], meta.ndim_numel[1],
        meta.sizes[0][0], meta.sizes[0][1], meta.sizes[0][2], meta.sizes[0][3],
        meta.sizes[1][1], meta.sizes[1][1], meta.sizes[1][2], meta.sizes[1][3],
        meta.dim_order[0][0], meta.dim_order[0][1],
        meta.dim_order[0][2], meta.dim_order[0][3],
        meta.dim_order[1][0], meta.dim_order[1][1],
        meta.dim_order[1][2], meta.dim_order[1][3],
        meta.strides[0][0], meta.strides[0][1],
        meta.strides[0][2], meta.strides[0][3],
        meta.strides[1][1], meta.strides[1][1],
        meta.strides[1][2], meta.strides[1][3]
    );
}

void printTextureMetadata(const TextureMetadata meta) {
    debugPrintfEXT(
        "TextureMetadata:\\n  sizes=[%u %u %u %u]\\n  limits=[%u %u %u]\\n  axis_map=[%u %u %u %u]\\n  packed_dim=%u\\n",
        meta.sizes[0], meta.sizes[1], meta.sizes[2], meta.sizes[3],
        meta.limits[0], meta.limits[1], meta.limits[2],
        meta.axis_map[0], meta.axis_map[1], meta.axis_map[2], meta.axis_map[3],
        meta.packed_dim
    );
}

#endif

#endif // INDEXING_GLSLH
