{
  "dim": 2048,
  "ffn_dim_multiplier": 1,
  "hidden_dim": 6144,
  "n_heads": 16,
  "head_dim": 128,
  "n_kv_heads": 4,
  "n_layers": 28,
  "norm_eps": 1e-05,
  "rope_theta": 10000.0,
  "use_scaled_rope": false,
  "vocab_size": 59264,
  "use_hf_rope": true,
  "attention_qkv_bias": false,
  "use_qk_norm": false,
  "model_architecture" : "GlmForCausalLM"
}
