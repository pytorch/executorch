- func: llama_quantized::embedding_byte.out(Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, int weight_quant_min, int weight_quant_max, Tensor indices, *, Tensor(a!) out) -> Tensor(a!)
  variants: function
  kernels:
    - arg_meta: null
      kernel_name: torch::executor::quantized_embedding_byte_out

- func: llama_quantized::embedding_byte.dtype_out(Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, int weight_quant_min, int weight_quant_max, Tensor indices, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
  variants: function
  kernels:
    - arg_meta: null
      kernel_name: torch::executor::quantized_embedding_byte_dtype_out

- func: quantized_decomposed::mixed_linear.out(Tensor input, Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
  variants: function
  kernels:
    - arg_meta: null
      kernel_name: torch::executor::quantized_mixed_linear_out
