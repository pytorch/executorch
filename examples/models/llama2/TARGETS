# Any targets that should be shared between fbcode and xplat must be defined in
# targets.bzl. This file can contain fbcode-only targets.

load("@fbsource//xplat/executorch/build:runtime_wrapper.bzl", "runtime")
load(":targets.bzl", "define_common_targets")

oncall("executorch")

define_common_targets()

runtime.python_library(
    name = "llama_transformer",
    srcs = ["llama_transformer.py"],
    _is_external_target = True,
    base_module = "executorch.examples.models.llama2",
    visibility = [
        "//executorch/...",
    ],
    deps = [
        "//caffe2:torch",
    ],
)

runtime.python_library(
    name = "llama2_model",
    srcs = [
        "__init__.py",
        "fairseq2.py",
        "model.py",
    ],
    _is_external_target = True,
    base_module = "executorch.examples.models.llama2",
    resources = {
        "//executorch/examples/models/llama2/params:params": "params",
    },
    visibility = [
        "//bento/...",
        "//bento_kernels/...",
        "//executorch/...",
    ],
    deps = [
        "//caffe2:torch",
        "//executorch/examples/models:model_base",
        "//executorch/examples/models/llama2:llama_transformer",
    ],
)

runtime.python_binary(
    name = "export_llama",
    main_module = "executorch.examples.models.llama2.export_llama",
    # visibility = ["//executorch/examples/..."],
    preload_deps = [
        "//executorch/examples/models/llama2/custom_ops:custom_ops_aot_lib",
        "//executorch/kernels/quantized:aot_lib",
    ],
    deps = [
        ":export_library",
        "//caffe2:torch",
        "//executorch/extension/pybindings:aten_lib",
    ],
)

runtime.python_library(
    name = "export_library",
    srcs = [
        "builder.py",
        "export_llama.py",
        "export_llama_lib.py",
        "lib/partitioner_lib.py",
        "lib/quant_lib.py",
        "model.py",
        "source_transformation/quantize.py",
        "source_transformation/rope.py",
        "source_transformation/sdpa.py",
    ],
    _is_external_target = True,
    base_module = "executorch.examples.models.llama2",
    visibility = [
        "//bento/...",
        "//bento_kernels/...",
        "//executorch/examples/...",
        "@EXECUTORCH_CLIENTS",
    ],
    deps = [
        "//caffe2:torch",
        "//executorch/backends/apple/coreml:backend",
        "//executorch/backends/apple/coreml:partitioner",
        "//executorch/backends/transforms:duplicate_dynamic_quant_chain",
        "//executorch/backends/vulkan/partitioner:vulkan_partitioner",
        "//executorch/backends/xnnpack:xnnpack_backend",
        "//executorch/backends/xnnpack/partition:xnnpack_partitioner",
        "//executorch/examples/models:model_base",
        "//executorch/examples/models:models",
        "//executorch/examples/models/llama2/custom_ops:custom_ops_aot_py",
        "//executorch/examples/portable:utils",
        "//executorch/exir:lib",
        # one definition has to be included in the user of the libarary
        # depending on what library the client wants to use
        # "//executorch/extension/pybindings:aten_lib",
        # "//executorch/extension/pybindings:portable_lib",
        # "//executorch/extension/pybindings:portable_lib_plus_custom",
        "//executorch/sdk/etrecord:etrecord",
        "//executorch/util:memory_profiler",
        "//executorch/util:python_profiler",
        "fbsource//third-party/pypi/coremltools:coremltools",
        "fbsource//third-party/pypi/sentencepiece:sentencepiece",
    ],
)
