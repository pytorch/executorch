##
## Runner-side Attention Sink configuration
##
## This uses KVCacheWithAttentionSink (model-side) together with
## the runner's AttentionSinkIOManager for position bookkeeping.
##
## Key behavior:
## - Model has KVCacheWithAttentionSink which preserves sink tokens and
##   uses a ring buffer for the sliding window (is_ring_buffer=True)
## - Runner's AttentionSinkIOManager tracks logical position and allows
##   generation to continue past max_context_len
## - KV cache size = sink_size + window_size * 2 = 4 + 124*2 = 252
##

base:
  metadata: '{"get_bos_id":128000, "get_eos_ids":[128009, 128001]}'

model:
  use_sdpa_with_kv_cache: True
  use_kv_cache: True
  dtype_override: fp32
  enable_dynamic_shape: True
  # Attention Sink: "sink_size,window_size,eviction_batch_size"
  # sink_size=4, window_size=124, eviction_batch_size=1
  # Max Context (Buffer) = 4 + 1 * 124 = 128
  use_attention_sink: "4,124,1"

export:
  # max_seq_length for single prefill chunk
  max_context_length: 128
  max_seq_length: 128

quantization:
  qmode: 8da4w
  group_size: 128
  embedding_quantize: 4,32

backend:
  xnnpack:
    enabled: True
    extended_ops: True
