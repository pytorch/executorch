tokenizer:
  _component_: torchtune.models.qwen2.qwen2_tokenizer
  path: /tmp/Qwen2-0.5B-Instruct/vocab.json
  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt
  max_seq_len: 512

dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
seed: null
shuffle: True
batch_size: 1

loss:
  _component_: torch.nn.CrossEntropyLoss

model:
  _component_: torchtune.models.qwen2.lora_qwen2_0_5b
  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
  apply_lora_to_mlp: False
  lora_rank: 32
  lora_alpha: 64

checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct
  checkpoint_files: [
    model.safetensors
  ]
  recipe_checkpoint: null
  output_dir: /tmp/Qwen2-0.5B-Instruct
  model_type: QWEN2
resume_from_checkpoint: False
save_adapter_weights_only: False

device: cpu
dtype: fp32

enable_activation_checkpointing: True
compile: False
