{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 Arm Limited and/or its affiliates.\n",
    "#\n",
    "# This source code is licensed under the BSD-style license found in the\n",
    "# LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethos-U delegate flow example\n",
    "\n",
    "This guide demonstrates the full flow for running a module on Arm Ethos-U55 using ExecuTorch.\n",
    "Tested on Linux x86_64 and macOS aarch64. If something is not working for you, please raise a GitHub issue and tag Arm.\n",
    "\n",
    "Before you begin:\n",
    "1. (In a clean virtual environment with a compatible Python version) Install executorch using `./install_executorch.sh`\n",
    "2. Install Arm cross-compilation toolchain and simulators using `./examples/arm/setup.sh --i-agree-to-the-contained-eula`\n",
    "\n",
    "With all commands executed from the base `executorch` folder.\n",
    "\n",
    "\n",
    "\n",
    "*Some scripts in this notebook produces long output logs: Configuring the 'Customizing Notebook Layout' settings to enable 'Output:scrolling' and setting 'Output:Text Line Limit' makes this more manageable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AOT Flow\n",
    "\n",
    "The first step is creating the PyTorch module and exporting it. Exporting converts the python code in the module into a graph structure. The result is still runnable python code, which can be displayed by printing the `graph_module` of the exported program.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Add(torch.nn.Module):\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        return x + y\n",
    "\n",
    "example_inputs = (torch.ones(1,1,1,1),torch.ones(1,1,1,1))\n",
    "\n",
    "model = Add()\n",
    "model = model.eval()\n",
    "exported_program = torch.export.export(model, example_inputs)\n",
    "graph_module = exported_program.module()\n",
    "\n",
    "_ = graph_module.print_readable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run on Ethos-U the `graph_module` must be quantized using the `arm_quantizer`. Quantization can be done in multiple ways and it can be customized for different parts of the graph; shown here is the recommended path for the EthosUBackend. Quantization also requires calibrating the module with example inputs.\n",
    "\n",
    "Again printing the module, it can be seen that the quantization wraps the node in quantization/dequantization nodes which contain the computed quanitzation parameters.\n",
    "\n",
    "With the default passes for the Arm Ethos-U backend, assuming the model lowers fully to the Ethos-U, the exported program is composed of a Quantize node, Ethos-U custom delegate and a Dequantize node. In some circumstances, you may want to feed quantized input to the Neural Network straight away, e.g. if you have a camera sensor outputting (u)int8 data and keep all the arithmetic of the application in the int8 domain. For these cases, you can apply the `exir/passes/quantize_io_pass.py`. See the unit test in `backends/arm/test/passes/test_ioquantization_pass.py`for an example how to feed quantized inputs and obtain quantized outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.backends.arm.arm_backend import ArmCompileSpecBuilder\n",
    "from executorch.backends.arm.quantizer import (\n",
    "    EthosUQuantizer,\n",
    "    get_symmetric_quantization_config,\n",
    ")\n",
    "from torchao.quantization.pt2e.quantize_pt2e import convert_pt2e, prepare_pt2e\n",
    "\n",
    "# Create a compilation spec describing the target for configuring the quantizer\n",
    "# Some args are used by the Arm Vela graph compiler later in the example. Refer to Arm Vela documentation for an\n",
    "# explanation of its flags: https://gitlab.arm.com/artificial-intelligence/ethos-u/ethos-u-vela/-/blob/main/OPTIONS.md\n",
    "spec_builder = ArmCompileSpecBuilder().ethosu_compile_spec(\n",
    "            target=\"ethos-u55-128\",\n",
    "            system_config=\"Ethos_U55_High_End_Embedded\",\n",
    "            memory_mode=\"Shared_Sram\",\n",
    "            extra_flags=\"--output-format=raw --debug-force-regor\"\n",
    "        )\n",
    "compile_spec = spec_builder.build()\n",
    "\n",
    "# Create and configure quantizer to use a symmetric quantization config globally on all nodes\n",
    "quantizer = EthosUQuantizer(compile_spec)\n",
    "operator_config = get_symmetric_quantization_config()\n",
    "quantizer.set_global(operator_config)\n",
    "\n",
    "# Post training quantization\n",
    "quantized_graph_module = prepare_pt2e(graph_module, quantizer)\n",
    "quantized_graph_module(*example_inputs) # Calibrate the graph module with the example input\n",
    "quantized_graph_module = convert_pt2e(quantized_graph_module)\n",
    "\n",
    "_ = quantized_graph_module.print_readable()\n",
    "\n",
    "# Create a new exported program using the quantized_graph_module\n",
    "quantized_exported_program = torch.export.export(quantized_graph_module, example_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowering in the EthosUBackend happens in five steps:\n",
    "\n",
    "1. **Lowering to core Aten operator set**: Transform module to use a subset of operators applicable to edge devices. \n",
    "2. **Partitioning**: Find subgraphs which are supported for running on Ethos-U\n",
    "3. **Lowering to TOSA compatible operator set**: Perform transforms to make the Ethos-U subgraph(s) compatible with TOSA \n",
    "4. **Serialization to TOSA**: Compiles the graph module into a TOSA graph \n",
    "5. **Compilation to NPU**: Compiles the TOSA graph into an EthosU command stream using the Arm Vela graph compiler. This makes use of the `compile_spec` created earlier.\n",
    "Step 5 also prints a Network summary for each processed subgraph.\n",
    "\n",
    "All of this happens behind the scenes in `to_edge_transform_and_lower`. Printing the graph module shows that what is left in the graph is two quantization nodes for `x` and `y` going into an `executorch_call_delegate` node, followed by a dequantization node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.backends.arm.ethosu import EthosUPartitioner\n",
    "from executorch.exir import (\n",
    "    EdgeCompileConfig,\n",
    "    ExecutorchBackendConfig,\n",
    "    to_edge_transform_and_lower,\n",
    ")\n",
    "from executorch.extension.export_util.utils import save_pte_program\n",
    "\n",
    "# Create partitioner from compile spec\n",
    "partitioner = EthosUPartitioner(compile_spec)\n",
    "\n",
    "# Lower the exported program to the Ethos-U backend\n",
    "edge_program_manager = to_edge_transform_and_lower(\n",
    "            quantized_exported_program,\n",
    "            partitioner=[partitioner],\n",
    "            compile_config=EdgeCompileConfig(\n",
    "                _check_ir_validity=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "# Convert edge program to executorch\n",
    "executorch_program_manager = edge_program_manager.to_executorch(\n",
    "            config=ExecutorchBackendConfig(extract_delegate_segments=False)\n",
    "        )\n",
    "\n",
    "_ = executorch_program_manager.exported_program().module().print_readable()\n",
    "\n",
    "# Save pte file\n",
    "save_pte_program(executorch_program_manager, \"ethos_u_minimal_example.pte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build executor runtime\n",
    "\n",
    "After the AOT compilation flow is done, the runtime can be cross compiled and linked to the produced .pte-file using the Arm cross-compilation toolchain. This is done in two steps:\n",
    "1. Build and install the executorch libraries and EthosUDelegate.\n",
    "2. Build and link the `arm_executor_runner` and generate kernel bindings for any non delegated ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Ensure the arm-none-eabi-gcc toolchain and FVP:s are available on $PATH\n",
    "source ethos-u-scratch/setup_path.sh\n",
    "\n",
    "# Build executorch libraries cross-compiled for arm baremetal to executorch/cmake-out-arm\n",
    "cmake --preset arm-baremetal \\\n",
    "-DCMAKE_BUILD_TYPE=Release \\\n",
    "-B../../cmake-out-arm ../..\n",
    "cmake --build ../../cmake-out-arm --target install -j$(nproc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source ethos-u-scratch/setup_path.sh\n",
    "\n",
    "# Build example executor runner application to examples/arm/ethos_u_minimal_example\n",
    "cmake -DCMAKE_TOOLCHAIN_FILE=$(pwd)/ethos-u-setup/arm-none-eabi-gcc.cmake \\\n",
    "      -DCMAKE_BUILD_TYPE=Release \\\n",
    "      -DET_PTE_FILE_PATH=ethos_u_minimal_example.pte \\\n",
    "      -DTARGET_CPU=cortex-m55 \\\n",
    "      -DETHOSU_TARGET_NPU_CONFIG=ethos-u55-128 \\\n",
    "      -DMEMORY_MODE=Shared_Sram \\\n",
    "      -DSYSTEM_CONFIG=Ethos_U55_High_End_Embedded \\\n",
    "      -Bethos_u_minimal_example \\\n",
    "      executor_runner\n",
    "cmake --build ethos_u_minimal_example -j$(nproc) -- arm_executor_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on simulated model\n",
    "\n",
    "We can finally use the `backends/arm/scripts/run_fvp.sh` utility script to run the .elf-file on simulated Arm hardware. The example application is by default built with an input of ones, so the expected result of the quantized addition should be close to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source ethos-u-scratch/setup_path.sh\n",
    "\n",
    "# Run the example\n",
    "../../backends/arm/scripts/run_fvp.sh --elf=ethos_u_minimal_example/arm_executor_runner --target=ethos-u55-128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.15)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
