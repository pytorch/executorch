/* Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 * Copyright 2025 Arm Limited and/or its affiliates.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

#include <errno.h>
#include <executorch/examples/arm/executor_runner/arm_memory_allocator.h>
#include <executorch/extension/data_loader/buffer_data_loader.h>
#include <executorch/extension/runner_util/inputs.h>
#include <executorch/runtime/core/memory_allocator.h>
#include <executorch/runtime/executor/program.h>
#include <executorch/runtime/platform/log.h>
#include <executorch/runtime/platform/platform.h>
#include <executorch/runtime/platform/runtime.h>
#include <math.h>
#include <stdio.h>
#include <unistd.h>
#include <zephyr/kernel.h>
#include <zephyr/sys/printk.h>
#include <cstring>
#include <memory>
#include <vector>

/**
 * This header file is generated by the build process based on the .pte file
 * specified in the ET_PTE_FILE_PATH variable to the cmake build.
 * Control of the action of the .pte, it's use of operators and delegates, and
 * which are included in the bare metal build are also orchestrated by the
 * CMakeLists file. For example use see examples/arm/run.sh
 *
 * e.g. This includes the pte as a big chunk of data struct into this file
 */
#include "model_pte.h"

// Runtime copy of the model blob placed in ISRAM so that the Ethos-U DMA can
// access command stream and weights. The original model_pte[] lives in flash.
alignas(16) static unsigned char model_pte_runtime[sizeof(model_pte)];
static bool model_pte_runtime_initialized = false;

using executorch::aten::ScalarType;
using executorch::aten::Tensor;
using executorch::aten::TensorImpl;
using executorch::extension::BufferCleanup;
using executorch::extension::BufferDataLoader;
using executorch::runtime::Error;
using executorch::runtime::EValue;
using executorch::runtime::HierarchicalAllocator;
using executorch::runtime::MemoryAllocator;
using executorch::runtime::MemoryManager;
using executorch::runtime::Method;
using executorch::runtime::MethodMeta;
using executorch::runtime::Program;
using executorch::runtime::Result;
using executorch::runtime::Span;
using executorch::runtime::Tag;
using executorch::runtime::TensorInfo;

#if defined(CONFIG_ETHOS_U)
extern "C" executorch::runtime::Error
executorch_delegate_EthosUBackend_registered(void);
#endif

/**
 * The method_allocation_pool should be large enough to fit the setup, input
 * used and other data used like the planned memory pool (e.g. memory-planned
 * buffers to use for mutable tensor data) In this example we run on a
 * Corstone-3xx FVP so we can use a lot of memory to be able to run and test
 * large models if you run on HW this should be lowered to fit into your
 * availible memory.
 */

#if !defined(ET_ARM_METHOD_ALLOCATOR_POOL_SIZE)
#define ET_ARM_METHOD_ALLOCATOR_POOL_SIZE (160 * 1024 * 1024)
#endif
const size_t method_allocation_pool_size = ET_ARM_METHOD_ALLOCATOR_POOL_SIZE;
unsigned char __attribute__((
    aligned(16))) method_allocation_pool[method_allocation_pool_size];

/**
 * The temp_allocation_pool is used for allocating temporary data during kernel
 * or delegate execution. This will be reset after each kernel or delegate call.
 * Currently a MemoryAllocator is used but a PlatformMemoryAllocator is probably
 * a better fit.
 *
 * The Corstone-300/Corstone-320 platforms have 2MB/4MB of SRAM respectively.
 * For Shared_Sram, ET_ARM_BAREMETAL_SCRATCH_TEMP_ALLOCATOR_POOL_SIZE is
 * 2MB and the linker script places the .bss.tensor_arena symbol in the SRAM.
 * For Dedicated_Sram, the .bss.tensor_arena symbol is placed in the DDR in the
 * linker script. The examples/arm/zephyr/prj.conf contains the logic for the
 * sizes of:
 * CONFIG_EXECUTORCH_METHOD_ALLOCATOR_POOL_SIZE and
 * CONFIG_EXECUTORCH_TEMP_ALLOCATOR_POOL_SIZE
 */

#if !defined(ET_ARM_BAREMETAL_SCRATCH_TEMP_ALLOCATOR_POOL_SIZE)
#define ET_ARM_BAREMETAL_SCRATCH_TEMP_ALLOCATOR_POOL_SIZE 2048
#endif
#if !defined(ET_ARM_BAREMETAL_FAST_SCRATCH_TEMP_ALLOCATOR_POOL_SIZE)
#define ET_ARM_BAREMETAL_FAST_SCRATCH_TEMP_ALLOCATOR_POOL_SIZE 0x600
#endif

const size_t temp_allocation_pool_size =
    ET_ARM_BAREMETAL_SCRATCH_TEMP_ALLOCATOR_POOL_SIZE;
unsigned char __attribute__((
    section(".bss.tensor_arena"),
    aligned(16))) temp_allocation_pool[temp_allocation_pool_size];
#if defined(ET_ARM_BAREMETAL_FAST_SCRATCH_TEMP_ALLOCATOR_POOL_SIZE)
extern "C" {
size_t ethosu_fast_scratch_size =
    ET_ARM_BAREMETAL_FAST_SCRATCH_TEMP_ALLOCATOR_POOL_SIZE;
unsigned char __attribute__((section(".bss.ethosu_scratch"), aligned(16)))
dedicated_sram[ET_ARM_BAREMETAL_FAST_SCRATCH_TEMP_ALLOCATOR_POOL_SIZE];
unsigned char* ethosu_fast_scratch = dedicated_sram;
}
#endif

namespace {

Result<BufferCleanup> prepare_input_tensors(
    Method& method,
    MemoryAllocator& allocator,
    const std::vector<std::pair<char*, size_t>>& input_buffers) {
  MethodMeta method_meta = method.method_meta();
  size_t num_inputs = method_meta.num_inputs();
  size_t num_allocated = 0;

  void** inputs =
      static_cast<void**>(allocator.allocate(num_inputs * sizeof(void*)));
  ET_CHECK_OR_RETURN_ERROR(
      inputs != nullptr,
      MemoryAllocationFailed,
      "Could not allocate memory for pointers to input buffers.");

  for (size_t i = 0; i < num_inputs; i++) {
    auto tag = method_meta.input_tag(i);
    ET_CHECK_OK_OR_RETURN_ERROR(tag.error());

    if (tag.get() != Tag::Tensor) {
      ET_LOG(Debug, "Skipping non-tensor input %zu", i);
      continue;
    }
    Result<TensorInfo> tensor_meta = method_meta.input_tensor_meta(i);
    ET_CHECK_OK_OR_RETURN_ERROR(tensor_meta.error());

    // Input is a tensor. Allocate a buffer for it.
    void* data_ptr = allocator.allocate(tensor_meta->nbytes());
    ET_CHECK_OR_RETURN_ERROR(
        data_ptr != nullptr,
        MemoryAllocationFailed,
        "Could not allocate memory for input buffers.");
    inputs[num_allocated++] = data_ptr;

    Error err = Error::Ok;
    if (input_buffers.size() > 0) {
      auto [buffer, buffer_size] = input_buffers.at(i);
      if (buffer_size != tensor_meta->nbytes()) {
        ET_LOG(
            Error,
            "input size (%d) and tensor size (%d) mismatch!",
            buffer_size,
            tensor_meta->nbytes());
        err = Error::InvalidArgument;
      } else {
        ET_LOG(Info, "Copying read input to tensor.");
        std::memcpy(data_ptr, buffer, buffer_size);
      }
    }

    TensorImpl impl = TensorImpl(
        tensor_meta.get().scalar_type(),
        tensor_meta.get().sizes().size(),
        const_cast<TensorImpl::SizesType*>(tensor_meta.get().sizes().data()),
        data_ptr,
        const_cast<TensorImpl::DimOrderType*>(
            tensor_meta.get().dim_order().data()));
    Tensor t(&impl);

    // If input_buffers.size <= 0, we don't have any input, fill it with 1's.
    if (input_buffers.size() <= 0) {
      for (size_t j = 0; j < t.numel(); j++) {
        switch (t.scalar_type()) {
          case ScalarType::Int:
            t.mutable_data_ptr<int>()[j] = 1;
            break;
          case ScalarType::Float:
            t.mutable_data_ptr<float>()[j] = 1.;
            break;
          case ScalarType::Char:
            t.mutable_data_ptr<int8_t>()[j] = 1;
            break;
        }
      }
    }

    err = method.set_input(t, i);

    if (err != Error::Ok) {
      ET_LOG(
          Error, "Failed to prepare input %zu: 0x%" PRIx32, i, (uint32_t)err);
      // The BufferCleanup will free the inputs when it goes out of scope.
      BufferCleanup cleanup({inputs, num_allocated});
      return err;
    }
  }
  return BufferCleanup({inputs, num_allocated});
}

} // namespace

int main(int argc, const char* argv[]) {
  (void)argc;
  (void)argv;

#if defined(CONFIG_ETHOS_U)
  if (executorch_delegate_EthosUBackend_registered() != Error::Ok) {
    ET_LOG(
        Error,
        "Ethos-U backend registration failed; model execution cannot continue");
    return 1;
  }
#endif
  executorch::runtime::runtime_init();
  std::vector<std::pair<char*, size_t>> input_buffers;
  size_t pte_size = sizeof(model_pte);

  ET_LOG(Info, "PTE at %p Size: %lu bytes", model_pte, pte_size);

  // Find the offset to the embedded Program.
  if (!model_pte_runtime_initialized) {
    std::memcpy(model_pte_runtime, model_pte, sizeof(model_pte));
    model_pte_runtime_initialized = true;
  }
  const void* program_data = model_pte_runtime;
  size_t program_data_len = pte_size;

  auto loader = BufferDataLoader(program_data, program_data_len);
  ET_LOG(Info, "PTE Model data loaded. Size: %lu bytes.", program_data_len);

  // Parse the program file. This is immutable, and can also be reused
  // between multiple execution invocations across multiple threads.
  Result<Program> program = Program::load(&loader);
  if (!program.ok()) {
    ET_LOG(
        Info,
        "Program loading failed @ 0x%p: 0x%" PRIx32,
        program_data,
        program.error());
  }

  ET_LOG(Info, "Model buffer loaded, has %zu methods", program->num_methods());

  const char* method_name = nullptr;
  {
    const auto method_name_result = program->get_method_name(0);
    ET_CHECK_MSG(method_name_result.ok(), "Program has no methods");
    method_name = *method_name_result;
  }
  ET_LOG(Info, "Running method %s", method_name);

  Result<MethodMeta> method_meta = program->method_meta(method_name);
  if (!method_meta.ok()) {
    ET_LOG(
        Info,
        "Failed to get method_meta for %s: 0x%x",
        method_name,
        (unsigned int)method_meta.error());
  }

  ET_LOG(
      Info,
      "Setup Method allocator pool. Size: %zu bytes.",
      method_allocation_pool_size);

  ArmMemoryAllocator method_allocator(
      method_allocation_pool_size, method_allocation_pool);

  std::vector<uint8_t*> planned_buffers; // Owns the memory
  std::vector<Span<uint8_t>> planned_spans; // Passed to the allocator
  size_t num_memory_planned_buffers = method_meta->num_memory_planned_buffers();

  size_t planned_buffer_membase = method_allocator.used_size();

  for (size_t id = 0; id < num_memory_planned_buffers; ++id) {
    size_t buffer_size =
        static_cast<size_t>(method_meta->memory_planned_buffer_size(id).get());
    ET_LOG(Info, "Setting up planned buffer %zu, size %zu.", id, buffer_size);

    /* Move to it's own allocator when MemoryPlanner is in place. */
    uint8_t* buffer =
        reinterpret_cast<uint8_t*>(method_allocator.allocate(buffer_size));
    ET_CHECK_MSG(
        buffer != nullptr,
        "Could not allocate memory for memory planned buffer size %zu",
        buffer_size);
    planned_buffers.push_back(buffer);
    planned_spans.push_back({planned_buffers.back(), buffer_size});
  }

  size_t planned_buffer_memsize =
      method_allocator.used_size() - planned_buffer_membase;
  ET_LOG(Info, "Computed planned buffer size=%zu", planned_buffer_memsize);

  HierarchicalAllocator planned_memory(
      {planned_spans.data(), planned_spans.size()});

  ArmMemoryAllocator temp_allocator(
      temp_allocation_pool_size, temp_allocation_pool);

  MemoryManager memory_manager(
      &method_allocator, &planned_memory, &temp_allocator);

  size_t method_loaded_membase = method_allocator.used_size();

  executorch::runtime::EventTracer* event_tracer_ptr = nullptr;

  ET_LOG(Info, "Loading method.");
  Result<Method> method =
      program->load_method(method_name, &memory_manager, event_tracer_ptr);

  if (!method.ok()) {
    ET_LOG(
        Info,
        "Loading of method %s failed with status 0x%" PRIx32,
        method_name,
        method.error());
  }
  size_t method_loaded_memsize =
      method_allocator.used_size() - method_loaded_membase;
  ET_LOG(Info, "Method '%s' loaded.", method_name);

  size_t input_membase = method_allocator.used_size();
  ET_LOG(
      Info,
      "Preparing input: In use: %zuB, Free: %zuB",
      input_membase,
      method_allocator.free_size());

  {
    // Here you would add code to get input from your Hardware
    // Get inputs from SEMIHOSTING or fake it with a lot of "1"
    // Use "static" to force to compiler to remove this when it goes out of
    // scope
    static auto prepared_inputs =
        ::prepare_input_tensors(*method, method_allocator, input_buffers);

    if (!prepared_inputs.ok()) {
      ET_LOG(
          Info,
          "Preparing inputs tensors for method %s failed with status 0x%" PRIx32,
          method_name,
          prepared_inputs.error());
    }
  }
#if defined(ET_DUMP_INPUT)
  {
    std::vector<EValue> inputs(method->inputs_size());
    ET_LOG(Info, "%zu inputs: ", inputs.size());
    Error status = method->get_inputs(inputs.data(), inputs.size());
    ET_CHECK(status == Error::Ok);

    for (int i = 0; i < inputs.size(); ++i) {
      if (inputs[i].isTensor()) {
        Tensor tensor = inputs[i].toTensor();
        // The output might be collected and parsed so printf() is used instead
        // of ET_LOG() here
        for (int j = 0; j < tensor.numel(); ++j) {
          if (tensor.scalar_type() == ScalarType::Int) {
            printf(
                "Input[%d][%d]: (int) %d\n",
                i,
                j,
                tensor.const_data_ptr<int>()[j]);
          } else if (tensor.scalar_type() == ScalarType::Float) {
            printf(
                "Input[%d][%d]: (float) %f\n",
                i,
                j,
                tensor.const_data_ptr<float>()[j]);
          } else if (tensor.scalar_type() == ScalarType::Char) {
            printf(
                "Input[%d][%d]: (char) %d\n",
                i,
                j,
                tensor.const_data_ptr<int8_t>()[j]);
          } else if (tensor.scalar_type() == ScalarType::Bool) {
            printf(
                "Input[%d][%d]: (bool) %s (0x%x)\n",
                i,
                j,
                tensor.const_data_ptr<int8_t>()[j] ? "true" : "false",
                tensor.const_data_ptr<int8_t>()[j]);
          }
        }
      } else {
        printf("Input[%d]: Not Tensor\n", i);
      }
    }
  }
#endif
  size_t input_memsize = method_allocator.used_size() - input_membase;
  ET_LOG(Info, "Input prepared.");

  ET_LOG(Info, "Starting the model execution...");
  size_t executor_membase = method_allocator.used_size();
  // Run the model.
  Error status = method->execute();
  size_t executor_memsize = method_allocator.used_size() - executor_membase;

  ET_LOG(Info, "model_pte_program_size:     %lu bytes.", program_data_len);
  ET_LOG(Info, "model_pte_loaded_size:      %lu bytes.", pte_size);
  if (method_allocator.size() != 0) {
    size_t method_allocator_used = method_allocator.used_size();
    ET_LOG(
        Info,
        "method_allocator_used:     %zu / %zu  free: %zu ( used: %zu %% ) ",
        method_allocator_used,
        method_allocator.size(),
        method_allocator.free_size(),
        100 * method_allocator_used / method_allocator.size());
    ET_LOG(
        Info, "method_allocator_planned:  %zu bytes", planned_buffer_memsize);
    ET_LOG(Info, "method_allocator_loaded:   %zu bytes", method_loaded_memsize);
    ET_LOG(Info, "method_allocator_input:    %zu bytes", input_memsize);
    ET_LOG(Info, "method_allocator_executor: %zu bytes", executor_memsize);
  }

  if (status != Error::Ok) {
    ET_LOG(
        Info,
        "Execution of method %s failed with status 0x%" PRIx32,
        method_name,
        status);
  } else {
    ET_LOG(Info, "Model executed successfully.");
  }

  std::vector<EValue> outputs(method->outputs_size());
  status = method->get_outputs(outputs.data(), outputs.size());
  ET_CHECK(status == Error::Ok);

  ET_LOG(Info, "Model outputs:");
  for (size_t i = 0; i < outputs.size(); ++i) {
    if (!outputs[i].isTensor()) {
      ET_LOG(Info, "  output[%zu]: non-tensor value", i);
      continue;
    }
    Tensor tensor = outputs[i].toTensor();
    ET_LOG(
        Info,
        "  output[%zu]: tensor scalar_type=%s numel=%zd",
        i,
        executorch::runtime::toString(tensor.scalar_type()),
        tensor.numel());
    switch (tensor.scalar_type()) {
      case ScalarType::Int: {
        const int* data = tensor.const_data_ptr<int>();
        for (ssize_t j = 0; j < tensor.numel(); ++j) {
          ET_LOG(Info, "    [%zd] = %d", j, data[j]);
        }
        break;
      }
      case ScalarType::Float: {
        const float* data = tensor.const_data_ptr<float>();
        for (ssize_t j = 0; j < tensor.numel(); ++j) {
          ET_LOG(Info, "    [%zd] = %f", j, static_cast<double>(data[j]));
        }
        break;
      }
      case ScalarType::Char: {
        const int8_t* data = tensor.const_data_ptr<int8_t>();
        for (ssize_t j = 0; j < tensor.numel(); ++j) {
          ET_LOG(Info, "    [%zd] = %d", j, data[j]);
        }
        break;
      }
      default:
        ET_LOG(
            Info,
            "    (%s tensor dump skipped)",
            executorch::runtime::toString(tensor.scalar_type()));
    }
  }
  ET_LOG(Info, "SUCCESS: Program complete, exiting.");
  ET_LOG(Info, "\04");
  return 0;
}
