From 0fe8caba3068da05021232912c069124a81e0d94 Mon Sep 17 00:00:00 2001
From: Rob Elliott <robert.elliott@arm.com>
Date: Wed, 4 Oct 2023 13:31:33 +0000
Subject: [PATCH] Add delegate runner test

Signed-off-by: Rob Elliott <robert.elliott@arm.com>
---
 applications/executorch_tests/CMakeLists.txt  |  27 ++-
 .../executorch_tests/pte_to_header.py         |  11 +-
 .../executorch_tests/runner_delegate.cpp      | 160 ++++++++++++++++++
 cmake/toolchain/arm-none-eabi-gcc.cmake       |   6 +-
 4 files changed, 195 insertions(+), 9 deletions(-)
 create mode 100644 applications/executorch_tests/runner_delegate.cpp

diff --git a/applications/executorch_tests/CMakeLists.txt b/applications/executorch_tests/CMakeLists.txt
index c95d53e..835f824 100644
--- a/applications/executorch_tests/CMakeLists.txt
+++ b/applications/executorch_tests/CMakeLists.txt
@@ -28,20 +28,24 @@ set(ET_DIR_PATH "<..>/executorch" CACHE PATH "Path to ExecuTorch dir")
 set(ET_BUILD_DIR_PATH "${ET_DIR_PATH}/cmake-out" CACHE PATH "Path to ExecuTorch build dir")
 set(ET_INCLUDE_PATH "${ET_DIR_PATH}/.." CACHE PATH "Path to ExecuTorch headers")
 set(ET_PTE_FILE_PATH "${ET_PTE_FILE_PATH}" CACHE PATH "Path to ExecuTorch model pte")
+set(ET_PTE_DELEGATE_FILE_PATH "${ET_PTE_DELGATE__FILE_PATH}" CACHE PATH "Path to ExecuTorch delegate model pte")
 
 get_filename_component(ET_BUILD_DIR_PATH ${ET_BUILD_DIR_PATH} REALPATH)
 get_filename_component(ET_DIR_PATH ${ET_DIR_PATH} REALPATH)
 get_filename_component(ET_INCLUDE_PATH ${ET_INCLUDE_PATH} REALPATH)
 get_filename_component(ET_PTE_FILE_PATH ${ET_PTE_FILE_PATH} REALPATH)
+get_filename_component(ET_PTE_DELEGATE_FILE_PATH ${ET_PTE_DELEGATE_FILE_PATH} REALPATH)
 
 message("**********************")
 message("ExecuTorch dir      (ET_DIR_PATH)       : ${ET_DIR_PATH}")
 message("ExecuTorch build dir(ET_BUILD_DIR_PATH) : ${ET_BUILD_DIR_PATH}")
 message("ExecuTorch headers  (ET_INCUDE_PATH)    : ${ET_INCLUDE_PATH}")
 message("ExecuTorch pte file (ET_PTE_FILE_PATH)  : ${ET_PTE_FILE_PATH}")
+message("ExecuTorch pte delegate file (ET_PTE_DELEGATE_FILE_PATH)  : ${ET_PTE_DELEGATE_FILE_PATH}")
 message("**********************")
 
 set(LIB_ET_RUNTIME "${ET_BUILD_DIR_PATH}/libexecutorch.a")
+set(LIB_ET_ETHOS "${ET_BUILD_DIR_PATH}/backends/arm/libexecutorch_delegate_ethos_u.a")
 set(LIB_ET_OP_REGISTRATION "${ET_BUILD_DIR_PATH}/kernels/portable/libportable_ops_lib.a")
 set(LIB_ET_OP_KERNELS "${ET_BUILD_DIR_PATH}/kernels/portable/libportable_kernels.a")
 
@@ -54,8 +58,11 @@ add_custom_command(
     OUTPUT
         ${CMAKE_CURRENT_BINARY_DIR}/fake_dep
         ${CMAKE_CURRENT_BINARY_DIR}/model_pte.h
+		${CMAKE_CURRENT_BINARY_DIR}/model_delegate_pte.h
     COMMAND ${PYTHON_EXECUTABLE} ./pte_to_header.py --pte ${ET_PTE_FILE_PATH}
-    --out ${CMAKE_CURRENT_BINARY_DIR}
+    --outdir ${CMAKE_CURRENT_BINARY_DIR}
+    COMMAND ${PYTHON_EXECUTABLE} ./pte_to_header.py --pte ${ET_PTE_DELEGATE_FILE_PATH}
+    --outdir ${CMAKE_CURRENT_BINARY_DIR} --outfile model_delegate_pte.h
     WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}
     )
 
@@ -67,10 +74,24 @@ ethosu_add_executable_test(executor_runner PRIVATE
     ${LIB_ET_OP_REGISTRATION}
     ${LIB_ET_OP_KERNELS})
 
-add_dependencies(executor_runner gen_model_header)
-
 target_include_directories(executor_runner PRIVATE
 ${ET_INCLUDE_PATH}
 ${CMAKE_CURRENT_BINARY_DIR})
 
+ethosu_add_executable_test(executor_runner_delegate PRIVATE
+    WHOLE_ARCHIVE TRUE
+    SOURCES runner_delegate.cpp
+    LIBRARIES
+    ${LIB_ET_RUNTIME}
+	${LIB_ET_ETHOS}
+  )
+
+target_include_directories(executor_runner_delegate PRIVATE
+${ET_INCLUDE_PATH}
+${CMAKE_CURRENT_BINARY_DIR})
+  
+add_dependencies(executor_runner gen_model_header)
+
+
+
 # TODO Memory setup
diff --git a/applications/executorch_tests/pte_to_header.py b/applications/executorch_tests/pte_to_header.py
index 37d88aa..be3282d 100644
--- a/applications/executorch_tests/pte_to_header.py
+++ b/applications/executorch_tests/pte_to_header.py
@@ -30,11 +30,18 @@ parser.add_argument(
 )
 parser.add_argument(
     "--outdir",
-    help="Output dir for model_pte.h",
+    help="Output dir for model header",
     type=str,
     required=False,
     default=".",
 )
+parser.add_argument(
+    "--outfile",
+    help="Output filename for model header",
+    type=str,
+    required=False,
+    default="model_pte.h",
+)
 parser.add_argument(
     "--section",
     help="Section attribute for the data array",
@@ -43,7 +50,7 @@ parser.add_argument(
     default=".sram.data",
 )
 args = parser.parse_args()
-outfile = os.path.join(args.outdir, "model_pte.h")
+outfile = os.path.join(args.outdir, args.outfile)
 attr = f'__attribute__((section("{args.section}"), aligned(16))) char '
 
 with open(args.pte, "rb") as fr, open(
diff --git a/applications/executorch_tests/runner_delegate.cpp b/applications/executorch_tests/runner_delegate.cpp
new file mode 100644
index 0000000..ff40084
--- /dev/null
+++ b/applications/executorch_tests/runner_delegate.cpp
@@ -0,0 +1,160 @@
+/*
+ * SPDX-FileCopyrightText: Copyright 2021-2023 Arm Limited and/or its affiliates <open-source-office@arm.com>
+ *
+ * SPDX-License-Identifier: Apache-2.0
+ *
+ * Licensed under the Apache License, Version 2.0 (the License); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an AS IS BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/****************************************************************************
+ * Includes
+ ****************************************************************************/
+
+#include <stdio.h>
+#include <vector>
+#include <memory>
+
+using namespace std;
+
+#include <executorch/runtime/platform/runtime.h>
+#include <executorch/runtime/executor/program.h>
+#include <executorch/extension/data_loader/buffer_data_loader.h>
+#include <executorch/runtime/platform/log.h>
+#include <executorch/util/util.h>
+
+/****************************************************************************
+ * Data
+ ****************************************************************************/
+
+// Our .pte file generated from the AoT flow
+#include "model_delegate_pte.h" // contains model_pte
+
+// Storage for intermediate data in SRAM
+__attribute__((section(".sram.data"), aligned(16))) uint8_t method_allocator_pool[4 * 1024U];
+
+void et_pal_init(void) {}
+
+__ET_NORETURN void et_pal_abort(void) {
+    __builtin_trap();
+}
+
+et_timestamp_t et_pal_current_ticks(void) {
+    // libc.a - warning: _gettimeofday is not implemented and will always fail
+    return 11223344;
+}
+
+/**
+ * Emit a log message via platform output (serial port, console, etc).
+ */
+void et_pal_emit_log_message(
+    __ET_UNUSED et_timestamp_t timestamp,
+    et_pal_log_level_t level,
+    const char* filename,
+    __ET_UNUSED const char* function,
+    size_t line,
+    const char* message,
+    __ET_UNUSED size_t length) {
+    fprintf(
+        stderr,
+        "%c executorch:%s:%zu] %s\n",
+        level,
+        filename,
+        line,
+        message);
+}
+
+int main()
+{
+    ET_LOG(Info, "Initialising runtime");
+    torch::executor::runtime_init();
+
+    using torch::executor::Result;
+    using torch::executor::Error;
+
+    // Load pte from the global model_pte .pte file loaded into SRAM.
+    auto loader = torch::executor::util::BufferDataLoader(model_pte, sizeof(model_pte));
+    Result<torch::executor::Program> program = torch::executor::Program::load(&loader);
+    if(!program.ok()) {
+        ET_LOG(Info, "Program loading failed @ 0x%p: 0x%x", model_pte, (int)program.error());
+    }
+    ET_LOG(Info, "Model buffer loaded, has %u methods", program->num_methods());
+
+    // Find our entrypoint in the .pte program
+    const char* method_name = nullptr;
+    const auto method_name_result = program->get_method_name(0);
+    ET_CHECK_MSG(method_name_result.ok(), "Program has no methods");
+    method_name = *method_name_result;
+    ET_LOG(Info, "Found (and will run) method '%s'", method_name);
+
+    // Allocate necessary memories for this method
+    Result<torch::executor::MethodMeta> method_meta = program->method_meta(method_name);
+    if (!method_meta.ok()) {
+        ET_LOG(Info, "Failed to get method_meta for %s: 0x%x",
+                method_name, (unsigned int)method_meta.error());
+    }
+    
+    torch::executor::MemoryAllocator method_allocator{
+        torch::executor::MemoryAllocator(sizeof(method_allocator_pool), method_allocator_pool)};
+
+    std::vector<std::unique_ptr<uint8_t[]>> planned_buffers; // Owns the memory
+    std::vector<torch::executor::Span<uint8_t>> planned_spans; // Passed to the allocator
+    size_t num_memory_planned_buffers = method_meta->num_memory_planned_buffers();
+
+    for (size_t id = 0; id < num_memory_planned_buffers; ++id) {
+        size_t buffer_size = static_cast<size_t>(method_meta->memory_planned_buffer_size(id).get());
+        ET_LOG(Info, "Setting up planned buffer %zu, size %zu.", id, buffer_size);
+
+        planned_buffers.push_back(std::make_unique<uint8_t[]>(buffer_size));
+        planned_spans.push_back({planned_buffers.back().get(), buffer_size});
+    }
+
+    torch::executor::HierarchicalAllocator planned_memory(
+      {planned_spans.data(), planned_spans.size()});
+
+    torch::executor::MemoryManager memory_manager(&method_allocator, &planned_memory);
+
+    Result<torch::executor::Method> method = program->load_method(method_name, &memory_manager);
+
+    if(!method.ok()) {
+        ET_LOG(Info, "Loading of method %s failed with status 0x%x", method_name, (int)method.error());
+    }
+    ET_LOG(Info, "Loading of method '%s' succesful", method_name);
+
+    auto inputs = torch::executor::util::PrepareInputTensors(*method);
+
+    ET_LOG(Info, "Starting the model execution...");
+    Error status = method->execute();
+    if(status != Error::Ok){
+        ET_LOG(Info, "Execution of method %s failed with status 0x%x", method_name, (int)status);
+    } else {
+        ET_LOG(Info, "Model executed successfully.");
+    }
+
+    // Print the outputs.
+    std::vector<torch::executor::EValue> outputs(method->outputs_size());
+    ET_LOG(Info, "%d outputs - ", outputs.size());
+    status = method->get_outputs(outputs.data(), outputs.size());
+    ET_CHECK(status == Error::Ok);
+    for (size_t i = 0; i < outputs.size(); ++i)
+    {
+        ET_LOG(Info, "Output %d numel %d", i, outputs[i].toTensor().numel());
+        for (size_t j = 0; j < outputs[i].toTensor().numel(); ++j)
+        {
+            ET_LOG(Info, "   Output[%d]: %d", j, outputs[i].toTensor().const_data_ptr<int>()[j]);
+        }
+    }
+
+    return 0;
+}
+
+
diff --git a/cmake/toolchain/arm-none-eabi-gcc.cmake b/cmake/toolchain/arm-none-eabi-gcc.cmake
index 0e6a2ed..fdb0d7c 100644
--- a/cmake/toolchain/arm-none-eabi-gcc.cmake
+++ b/cmake/toolchain/arm-none-eabi-gcc.cmake
@@ -98,8 +98,6 @@ add_compile_options(
     # -Wswitch
     # -Wswitch-default
     # -Wunused
-
-    # -Wno-redundant-decls
-
-    # -Wno-psabi
+    -Wno-redundant-decls
+    -Wno-psabi
 )
-- 
2.41.0

