From 0c91f25a52d32d7f4b6ec787a40633a92af7f885 Mon Sep 17 00:00:00 2001
From: Digant Desai <digantdesai@meta.com>
Date: Thu, 28 Sep 2023 19:07:51 -0700
Subject: [PATCH 6/6] [Executorch] Add executorch_runner test

---
 applications/CMakeLists.txt                  |   2 +
 applications/executorch_tests/CMakeLists.txt |  76 +++++++++++
 applications/executorch_tests/runner.cpp     | 133 +++++++++++++++++++
 cmake/helpers.cmake                          |  13 +-
 4 files changed, 222 insertions(+), 2 deletions(-)
 create mode 100644 applications/executorch_tests/CMakeLists.txt
 create mode 100644 applications/executorch_tests/runner.cpp

diff --git a/applications/CMakeLists.txt b/applications/CMakeLists.txt
index 1fa2b2e..68e5427 100644
--- a/applications/CMakeLists.txt
+++ b/applications/CMakeLists.txt
@@ -28,6 +28,8 @@ add_subdirectory(threadx_demo)
 
 add_subdirectory(message_handler_openamp)
 
+add_subdirectory(executorch_tests)
+
 if (CMAKE_CXX_COMPILER_ID STREQUAL "ARMClang")
     # Only armclang supported for now
     add_subdirectory(trustzone_inference)
diff --git a/applications/executorch_tests/CMakeLists.txt b/applications/executorch_tests/CMakeLists.txt
new file mode 100644
index 0000000..c95d53e
--- /dev/null
+++ b/applications/executorch_tests/CMakeLists.txt
@@ -0,0 +1,76 @@
+#
+# Copyright (c) 2021 Arm Limited. All rights reserved.
+#
+# SPDX-License-Identifier: Apache-2.0
+#
+# Licensed under the Apache License, Version 2.0 (the License); you may
+# not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an AS IS BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+if (NOT TARGET ethosu_core_driver)
+  return()
+endif()
+
+####
+#### ExecuTorch demo app/test
+####
+
+set(ET_DIR_PATH "<..>/executorch" CACHE PATH "Path to ExecuTorch dir")
+set(ET_BUILD_DIR_PATH "${ET_DIR_PATH}/cmake-out" CACHE PATH "Path to ExecuTorch build dir")
+set(ET_INCLUDE_PATH "${ET_DIR_PATH}/.." CACHE PATH "Path to ExecuTorch headers")
+set(ET_PTE_FILE_PATH "${ET_PTE_FILE_PATH}" CACHE PATH "Path to ExecuTorch model pte")
+
+get_filename_component(ET_BUILD_DIR_PATH ${ET_BUILD_DIR_PATH} REALPATH)
+get_filename_component(ET_DIR_PATH ${ET_DIR_PATH} REALPATH)
+get_filename_component(ET_INCLUDE_PATH ${ET_INCLUDE_PATH} REALPATH)
+get_filename_component(ET_PTE_FILE_PATH ${ET_PTE_FILE_PATH} REALPATH)
+
+message("**********************")
+message("ExecuTorch dir      (ET_DIR_PATH)       : ${ET_DIR_PATH}")
+message("ExecuTorch build dir(ET_BUILD_DIR_PATH) : ${ET_BUILD_DIR_PATH}")
+message("ExecuTorch headers  (ET_INCUDE_PATH)    : ${ET_INCLUDE_PATH}")
+message("ExecuTorch pte file (ET_PTE_FILE_PATH)  : ${ET_PTE_FILE_PATH}")
+message("**********************")
+
+set(LIB_ET_RUNTIME "${ET_BUILD_DIR_PATH}/libexecutorch.a")
+set(LIB_ET_OP_REGISTRATION "${ET_BUILD_DIR_PATH}/kernels/portable/libportable_ops_lib.a")
+set(LIB_ET_OP_KERNELS "${ET_BUILD_DIR_PATH}/kernels/portable/libportable_kernels.a")
+
+add_custom_target(
+    gen_model_header ALL
+    DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/fake_dep
+)
+
+add_custom_command(
+    OUTPUT
+        ${CMAKE_CURRENT_BINARY_DIR}/fake_dep
+        ${CMAKE_CURRENT_BINARY_DIR}/model_pte.h
+    COMMAND ${PYTHON_EXECUTABLE} ./pte_to_header.py --pte ${ET_PTE_FILE_PATH}
+    --out ${CMAKE_CURRENT_BINARY_DIR}
+    WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}
+    )
+
+ethosu_add_executable_test(executor_runner PRIVATE
+    WHOLE_ARCHIVE TRUE
+    SOURCES runner.cpp
+    LIBRARIES
+    ${LIB_ET_RUNTIME}
+    ${LIB_ET_OP_REGISTRATION}
+    ${LIB_ET_OP_KERNELS})
+
+add_dependencies(executor_runner gen_model_header)
+
+target_include_directories(executor_runner PRIVATE
+${ET_INCLUDE_PATH}
+${CMAKE_CURRENT_BINARY_DIR})
+
+# TODO Memory setup
diff --git a/applications/executorch_tests/runner.cpp b/applications/executorch_tests/runner.cpp
new file mode 100644
index 0000000..7ef920d
--- /dev/null
+++ b/applications/executorch_tests/runner.cpp
@@ -0,0 +1,133 @@
+/* Copyright (c) Meta Platforms, Inc. and affiliates.
+ * All rights reserved.
+ *
+ * This source code is licensed under the BSD-style license found in the
+ * LICENSE file in the root directory of this source tree.
+ */
+
+#include <stdio.h>
+#include <vector>
+#include <memory>
+
+#include <executorch/extension/data_loader/buffer_data_loader.h>
+#include <executorch/runtime/executor/program.h>
+#include <executorch/runtime/platform/log.h>
+#include <executorch/runtime/platform/platform.h>
+#include <executorch/runtime/platform/runtime.h>
+#include <executorch/util/util.h>
+
+// Model file - TODO make this configurable through CMake
+#include "model_pte.h"
+
+using namespace std;
+using torch::executor::Result;
+using torch::executor::Error;
+
+__attribute__((section(".sram.data"), aligned(16))) uint8_t method_allocator_pool[4 * 1024U];
+
+void et_pal_init(void) {}
+
+__ET_NORETURN void et_pal_abort(void) {
+  __builtin_trap();
+}
+
+et_timestamp_t et_pal_current_ticks(void) {
+ // libc.a - warning: _gettimeofday is not implemented and will always fail
+ return 11223344;
+}
+
+/**
+ * Emit a log message via platform output (serial port, console, etc).
+ */
+void et_pal_emit_log_message(
+    __ET_UNUSED et_timestamp_t timestamp,
+    et_pal_log_level_t level,
+    const char* filename,
+    __ET_UNUSED const char* function,
+    size_t line,
+    const char* message,
+    __ET_UNUSED size_t length) {
+  fprintf(
+      stderr,
+      "%c executorch:%s:%zu] %s\n",
+      level,
+      filename,
+      line,
+      message);
+}
+
+int main() {
+    torch::executor::runtime_init();
+
+    auto loader = torch::executor::util::BufferDataLoader(model_pte, sizeof(model_pte));
+    ET_LOG(Info, "Model PTE file loaded. Size: %lu bytes.", sizeof(model_pte));
+    Result<torch::executor::Program> program = torch::executor::Program::load(&loader);
+    if(!program.ok()) {
+       ET_LOG(Info,"Program loading failed @ 0x%p: 0x%" PRIx32, model_pte, program.error());
+    }
+
+    ET_LOG(Info,"Model buffer loaded, has %lu methods", program->num_methods());
+
+    const char* method_name = nullptr;
+    {
+      const auto method_name_result = program->get_method_name(0);
+      ET_CHECK_MSG(method_name_result.ok(), "Program has no methods");
+      method_name = *method_name_result;
+    }
+    ET_LOG(Info,"Running method %s", method_name);
+
+    Result<torch::executor::MethodMeta> method_meta = program->method_meta(method_name);
+    if (!method_meta.ok()) {
+        ET_LOG(Info,"Failed to get method_meta for %s: 0x%x",
+                method_name, (unsigned int)method_meta.error());
+    }
+
+    torch::executor::MemoryAllocator method_allocator{
+        torch::executor::MemoryAllocator(sizeof(method_allocator_pool), method_allocator_pool)};
+
+    std::vector<std::unique_ptr<uint8_t[]>> planned_buffers; // Owns the memory
+    std::vector<torch::executor::Span<uint8_t>> planned_spans; // Passed to the allocator
+    size_t num_memory_planned_buffers = method_meta->num_memory_planned_buffers();
+
+    for (size_t id = 0; id < num_memory_planned_buffers; ++id) {
+        size_t buffer_size = static_cast<size_t>(method_meta->memory_planned_buffer_size(id).get());
+        ET_LOG(Info,"Setting up planned buffer %zu, size %zu.", id, buffer_size);
+
+        planned_buffers.push_back(std::make_unique<uint8_t[]>(buffer_size));
+        planned_spans.push_back({planned_buffers.back().get(), buffer_size});
+    }
+
+    torch::executor::HierarchicalAllocator planned_memory(
+      {planned_spans.data(), planned_spans.size()});
+
+    torch::executor::MemoryManager memory_manager(&method_allocator, &planned_memory);
+
+    Result<torch::executor::Method> method = program->load_method(method_name, &memory_manager);
+    if(!method.ok()) {
+        ET_LOG(Info,"Loading of method %s failed with status 0x%" PRIx32, method_name, method.error());
+    }
+    ET_LOG(Info,"Method loaded.");
+
+    ET_LOG(Info,"Preparing inputs...");
+    auto inputs = torch::executor::util::PrepareInputTensors(*method);
+    ET_LOG(Info,"Input prepared.");
+
+    ET_LOG(Info,"Starting the model execution...");
+    Error status = method->execute();
+    if(status != Error::Ok){
+        ET_LOG(Info,"Execution of method %s failed with status 0x%" PRIx32, method_name, status);
+    } else {
+        ET_LOG(Info,"Model executed successfully.");
+    }
+
+    std::vector<torch::executor::EValue> outputs(method->outputs_size());
+    ET_LOG(Info, "%zu outputs: ", outputs.size());
+    status = method->get_outputs(outputs.data(), outputs.size());
+    ET_CHECK(status == Error::Ok);
+    for (int i = 0; i < outputs.size(); ++i) {
+       for (int j = 0; j < outputs[i].toTensor().numel(); ++j) {
+          printf("Output[%d][%d]: %f\n", i, j, outputs[i].toTensor().const_data_ptr<float>()[j]);
+       }
+    }
+    return 0;
+}
diff --git a/cmake/helpers.cmake b/cmake/helpers.cmake
index a21d9f0..036f189 100644
--- a/cmake/helpers.cmake
+++ b/cmake/helpers.cmake
@@ -85,7 +85,7 @@ endfunction()
 #############################################################################
 
 function(ethosu_add_executable target)
-    cmake_parse_arguments(ARGS "" "TARGET_LIBRARY" "SOURCES;LIBRARIES" ${ARGN})
+    cmake_parse_arguments(ARGS "WHOLE_ARCHIVE" "TARGET_LIBRARY" "SOURCES;LIBRARIES" ${ARGN})
     add_executable(${target})
 
     target_sources(${target} PRIVATE
@@ -95,8 +95,17 @@ function(ethosu_add_executable target)
         set(ARGS_TARGET_LIBRARY ethosu_target_init)
     endif()
 
+    if (ARGS_WHOLE_ARCHIVE)
+        set(PRE_LINKER_FLAGS "-Wl,--whole-archive")
+        set(POST_LINKER_FLAGS "-Wl,--no-whole-archive")
+    endif()
+
     target_link_libraries(${target} PRIVATE
-        ${ARGS_TARGET_LIBRARY} ${ARGS_LIBRARIES})
+        ${PRE_LINKER_FLAGS}
+        ${ARGS_TARGET_LIBRARY} 
+        ${ARGS_LIBRARIES}
+        ${POST_LINKER_FLAGS}
+        )
 
     ethosu_eval_link_options(${target})
 
-- 
2.39.3

