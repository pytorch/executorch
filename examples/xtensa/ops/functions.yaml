# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This yaml file contains operators that are also defined by the ATen library.
# For lean mode:
#   - Codegen'd target `executorch_generated_lib` will be reading all the information
#     from this file, including operator schema and kernel metadata.
#   - Selective build target `codegen:executorch_defined_ops` now is selecting all the
#     operators in this file, by dumping all the op names into `selected_operators.yaml`.
#
# See the README.md file in executorch/kernels/portable for a description of the syntax used
# by this file.


- op: add.out
  kernels:
    - arg_meta: null
      kernel_name: torch::executor::add_out

- op: full.out
  kernels:
    - arg_meta: null
      kernel_name: torch::executor::full_out

- func: xtensa::quantized_linear.out(Tensor src, Tensor weight, Tensor bias, float src_scale, int src_zero_point, float weight_scale, int weight_zero_point, Tensor out_multiplier, Tensor out_shift, int out_zero_point, *, Tensor(a!) out) -> Tensor(a!)
  kernels:
    - arg_meta: null
      kernel_name: impl::HiFi::quantized_linear_out

- func: xtensa::quantized_conv.out(Tensor input, Tensor weight, Tensor bias, int[] stride, SymInt[] padding, int[] dilation, int groups, int input_zero_point, Tensor weight_zero_point, Tensor bias_scale, float out_scale, int out_zero_point, Tensor out_multiplier, Tensor out_shift, bool channel_last=False, *, Tensor(a!) out) -> Tensor(a!)
  kernels:
    - arg_meta: null
      kernel_name: impl::HiFi::quantized_conv_out

- func: xtensa::dequantize_per_tensor.out(Tensor input, float scale, int zero_point, int quant_min, int quant_max, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
  variants: function
  kernels:
    - arg_meta: null
      kernel_name: impl::HiFi::dequantize_per_tensor_out

- func: xtensa::quantize_per_tensor.out(Tensor input, float scale, int zero_point, int quant_min, int quant_max, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
  variants: function
  kernels:
    - arg_meta: null
      kernel_name: impl::HiFi::quantize_per_tensor_out
