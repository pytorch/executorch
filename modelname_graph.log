/home/sraut/ext_main/cad_rlc/executorch/.venv/lib64/python3.11/site-packages/torch/_inductor/cpp_builder.py:1183: UserWarning: Can't find Python.h in /usr/include/python3.11
  warnings.warn(f"Can't find Python.h in {str(include_dir)}")
/home/sraut/ext_main/cad_rlc/executorch/.venv/lib64/python3.11/site-packages/torch/_inductor/cpp_builder.py:1183: UserWarning: Can't find Python.h in /usr/include/python3.11
  warnings.warn(f"Can't find Python.h in {str(include_dir)}")
[INFO 2025-12-22 07:39:07,644 utils.py:246] 
+-------------------------------------------------------+---------------+-----------------+
| Final Operators                                       |   Final Graph |   To_edge Graph |
+=======================================================+===============+=================+
| cadence::quantized_conv2d_nchw.per_tensor             |            20 |              20 |
| cadence::dequantize_per_tensor                        |            18 |               0 |
| cadence::quantized_relu.per_tensor                    |            17 |              17 |
| cadence::quantize_per_tensor                          |            11 |               0 |
| aten::add.Tensor                                      |             8 |               8 |
| aten::max_pool2d_with_indices                         |             1 |               1 |
| aten::mean.dim                                        |             1 |               1 |
| aten::view_copy                                       |             1 |               1 |
| cadence::quantized_linear.per_tensor                  |             1 |               1 |
+-------------------------------------------------------+---------------+-----------------+
[INFO 2025-12-22 07:39:07,644 utils.py:262] +-------------------------------------------------------+---------------+-----------------+
| Deleted Operators                                     |   Final Graph |   To_edge Graph |
+=======================================================+===============+=================+
| quantized_decomposed::quantize_per_tensor             |             0 |              11 |
| quantized_decomposed::dequantize_per_tensor           |             0 |              18 |
+-------------------------------------------------------+---------------+-----------------+
[INFO 2025-12-22 07:39:08,349 memory_planning.py:322] 
+----------------+----------------+-----------------------+-----------------------------+
|   Memory Space | Base Address   |   Memory Size (Bytes) |   Peak Memory Usage (Bytes) |
+================+================+=======================+=============================+
|              1 |                |           68719476736 |                      458752 |
+----------------+----------------+-----------------------+-----------------------------+
[INFO 2025-12-22 07:39:08,352 memory_planning.py:355] 
+-------------------------------------+--------------+---------+
| Peak memory usage across all spaces | 458752 bytes | Node 52 |
+-------------------------------------+--------------+---------+
[INFO 2025-12-22 07:39:08,630 compiler.py:478] Generated ETRecord at /tmp/tmps8ia8j3x/etrecord.bin
[INFO 2025-12-22 07:39:08,631 export_example.py:76] Final exported graph:

opcode         name                                         target                                        args                                                                                                                                                                                 kwargs
-------------  -------------------------------------------  --------------------------------------------  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------
placeholder    b__frozen_param0                             b__frozen_param0                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param1                             b__frozen_param1                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param2                             b__frozen_param2                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param3                             b__frozen_param3                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param4                             b__frozen_param4                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param5                             b__frozen_param5                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param6                             b__frozen_param6                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param7                             b__frozen_param7                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param8                             b__frozen_param8                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param9                             b__frozen_param9                              ()                                                                                                                                                                                   {}
placeholder    b__frozen_param10                            b__frozen_param10                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param11                            b__frozen_param11                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param12                            b__frozen_param12                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param13                            b__frozen_param13                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param14                            b__frozen_param14                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param15                            b__frozen_param15                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param16                            b__frozen_param16                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param17                            b__frozen_param17                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param18                            b__frozen_param18                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param19                            b__frozen_param19                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param20                            b__frozen_param20                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param21                            b__frozen_param21                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param22                            b__frozen_param22                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param23                            b__frozen_param23                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param24                            b__frozen_param24                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param25                            b__frozen_param25                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param26                            b__frozen_param26                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param27                            b__frozen_param27                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param28                            b__frozen_param28                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param29                            b__frozen_param29                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param30                            b__frozen_param30                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param31                            b__frozen_param31                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param32                            b__frozen_param32                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param33                            b__frozen_param33                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param34                            b__frozen_param34                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param35                            b__frozen_param35                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param36                            b__frozen_param36                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param37                            b__frozen_param37                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param38                            b__frozen_param38                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param39                            b__frozen_param39                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param40                            b__frozen_param40                             ()                                                                                                                                                                                   {}
placeholder    b__frozen_param41                            b__frozen_param41                             ()                                                                                                                                                                                   {}
placeholder    x                                            x                                             ()                                                                                                                                                                                   {}
call_function  alloc                                        <function alloc at 0x7fbd8f8837e0>            (((1, 3, 64, 64), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantize_per_tensor_default          cadence.quantize_per_tensor.out               (x, 0.02952140010893345, -1, -128, 127, torch.int8)                                                                                                                                  {'out': alloc}
call_function  alloc_1                                      <function alloc at 0x7fbd8f8837e0>            (((1, 64, 32, 32), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_conv2d_nchw_per_tensor     cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantize_per_tensor_default, b__frozen_param0, b__frozen_param22, [2, 2], [3, 3], [1, 1], 1, -1, 0, 9.119807304310132e-05, 0.02939300797879696, 6, 1705734656, -8)          {'out': alloc_1}
call_function  alloc_2                                      <function alloc at 0x7fbd8f8837e0>            (((1, 64, 32, 32), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_relu_per_tensor            cadence.quantized_relu.per_tensor_out         (cadence_quantized_conv2d_nchw_per_tensor, 6, -128, 1209599616, 2)                                                                                                                   {'out': alloc_2}
call_function  alloc_3                                      <function alloc at 0x7fbd8f8837e0>            (((1, 64, 32, 32), torch.float32),)                                                                                                                                                  {}
call_function  cadence_dequantize_per_tensor_default        cadence.dequantize_per_tensor.out             (cadence_quantized_relu_per_tensor, 0.013045846484601498, -128, -128, 127, torch.int8)                                                                                               {'out': alloc_3}
call_function  alloc_4                                      <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.float32),)                                                                                                                                                  {}
call_function  alloc_5                                      <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int64),)                                                                                                                                                    {}
call_function  aten_max_pool2d_with_indices_default         aten.max_pool2d_with_indices.out              (cadence_dequantize_per_tensor_default, [3, 3], [2, 2], [1, 1], [1, 1], False)                                                                                                       {'out': alloc_4, 'indices': alloc_5}
call_function  getitem                                      <built-in function getitem>                   (aten_max_pool2d_with_indices_default, 0)                                                                                                                                            {}
call_function  alloc_6                                      <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantize_per_tensor_default_1        cadence.quantize_per_tensor.out               (getitem, 0.013953262008726597, -128, -128, 127, torch.int8)                                                                                                                         {'out': alloc_6}
call_function  alloc_7                                      <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_1   cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantize_per_tensor_default_1, b__frozen_param1, b__frozen_param23, [1, 1], [1, 1], [1, 1], 1, -128, 0, 4.0984742889934884e-05, 0.018558798357844353, 53, 1214065664, -8)   {'out': alloc_7}
call_function  alloc_8                                      <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_relu_per_tensor_1          cadence.quantized_relu.per_tensor_out         (cadence_quantized_conv2d_nchw_per_tensor_1, 53, -128, 1837768064, 2)                                                                                                                {'out': alloc_8}
call_function  alloc_9                                      <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_2   cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_1, b__frozen_param2, b__frozen_param24, [1, 1], [1, 1], [1, 1], 1, -128, 0, 3.277554574008688e-05, 0.01659408211708069, -4, 1085841792, -8)       {'out': alloc_9}
call_function  alloc_10                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.float32),)                                                                                                                                                  {}
call_function  cadence_dequantize_per_tensor_default_1      cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_2, 0.01659408211708069, -4, -128, 127, torch.int8)                                                                                         {'out': alloc_10}
call_function  alloc_11                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.float32),)                                                                                                                                                  {}
call_function  aten_add_tensor                              aten.add.out                                  (cadence_dequantize_per_tensor_default_1, getitem)                                                                                                                                   {'alpha': 1, 'out': alloc_11}
call_function  alloc_12                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantize_per_tensor_default_2        cadence.quantize_per_tensor.out               (aten_add_tensor, 0.022634277120232582, -37, -128, 127, torch.int8)                                                                                                                  {'out': alloc_12}
call_function  alloc_13                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_relu_per_tensor_2          cadence.quantized_relu.per_tensor_out         (cadence_quantize_per_tensor_default_2, -37, -128, 1669185280, 1)                                                                                                                    {'out': alloc_13}
call_function  alloc_14                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_3   cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_2, b__frozen_param3, b__frozen_param25, [1, 1], [1, 1], [1, 1], 1, -128, 0, 3.1970076788288056e-05, 0.017917048186063766, 1, 1961900800, -9)      {'out': alloc_14}
call_function  alloc_15                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_relu_per_tensor_3          cadence.quantized_relu.per_tensor_out         (cadence_quantized_conv2d_nchw_per_tensor_3, 1, -128, 1083468544, 2)                                                                                                                 {'out': alloc_15}
call_function  alloc_16                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_4   cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_3, b__frozen_param4, b__frozen_param26, [1, 1], [1, 1], [1, 1], 1, -128, 0, 7.293207549077287e-05, 0.026471495628356934, 40, 1514641792, -8)      {'out': alloc_16}
call_function  alloc_17                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.float32),)                                                                                                                                                  {}
call_function  cadence_dequantize_per_tensor_default_2      cadence.dequantize_per_tensor.out             (cadence_quantized_relu_per_tensor_2, 0.014560019597411156, -128, -128, 127, torch.int8)                                                                                             {'out': alloc_17}
call_function  alloc_18                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.float32),)                                                                                                                                                  {}
call_function  cadence_dequantize_per_tensor_default_3      cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_4, 0.026471495628356934, 40, -128, 127, torch.int8)                                                                                        {'out': alloc_18}
call_function  alloc_19                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.float32),)                                                                                                                                                  {}
call_function  aten_add_tensor_1                            aten.add.out                                  (cadence_dequantize_per_tensor_default_3, cadence_dequantize_per_tensor_default_2)                                                                                                   {'alpha': 1, 'out': alloc_19}
call_function  alloc_20                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantize_per_tensor_default_3        cadence.quantize_per_tensor.out               (aten_add_tensor_1, 0.03831474110484123, -19, -128, 127, torch.int8)                                                                                                                 {'out': alloc_20}
call_function  alloc_21                                     <function alloc at 0x7fbd8f8837e0>            (((1, 64, 16, 16), torch.int8),)                                                                                                                                                     {}
call_function  cadence_quantized_relu_per_tensor_4          cadence.quantized_relu.per_tensor_out         (cadence_quantize_per_tensor_default_3, -19, -128, 1872044544, 1)                                                                                                                    {'out': alloc_21}
call_function  alloc_22                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_5   cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_4, b__frozen_param7, b__frozen_param29, [2, 2], [0, 0], [1, 1], 1, -128, 0, 0.00011931715768333847, 0.02016621083021164, 14, 1626366592, -7)      {'out': alloc_22}
call_function  alloc_23                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_6   cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_4, b__frozen_param5, b__frozen_param27, [2, 2], [1, 1], [1, 1], 1, -128, 0, 3.66604923844752e-05, 0.016489792615175247, 0, 1222230016, -8)        {'out': alloc_23}
call_function  alloc_24                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_5          cadence.quantized_relu.per_tensor_out         (cadence_quantized_conv2d_nchw_per_tensor_6, 0, -128, 1075072512, 2)                                                                                                                 {'out': alloc_24}
call_function  alloc_25                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_7   cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_5, b__frozen_param6, b__frozen_param28, [1, 1], [1, 1], [1, 1], 1, -128, 0, 4.676983033102648e-05, 0.01661989651620388, -14, 1547060480, -8)      {'out': alloc_25}
call_function  alloc_26                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_4      cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_5, 0.02016621083021164, 14, -128, 127, torch.int8)                                                                                         {'out': alloc_26}
call_function  alloc_27                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_5      cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_7, 0.01661989651620388, -14, -128, 127, torch.int8)                                                                                        {'out': alloc_27}
call_function  alloc_28                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.float32),)                                                                                                                                                   {}
call_function  aten_add_tensor_2                            aten.add.out                                  (cadence_dequantize_per_tensor_default_5, cadence_dequantize_per_tensor_default_4)                                                                                                   {'alpha': 1, 'out': alloc_28}
call_function  alloc_29                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantize_per_tensor_default_4        cadence.quantize_per_tensor.out               (aten_add_tensor_2, 0.02489815279841423, -2, -128, 127, torch.int8)                                                                                                                  {'out': alloc_29}
call_function  alloc_30                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_6          cadence.quantized_relu.per_tensor_out         (cadence_quantize_per_tensor_default_4, -2, -128, 2114147328, 1)                                                                                                                     {'out': alloc_30}
call_function  alloc_31                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_8   cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_6, b__frozen_param8, b__frozen_param30, [1, 1], [1, 1], [1, 1], 1, -128, 0, 3.084982038067019e-05, 0.015364459715783596, 5, 1103837568, -8)       {'out': alloc_31}
call_function  alloc_32                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_7          cadence.quantized_relu.per_tensor_out         (cadence_quantized_conv2d_nchw_per_tensor_8, 5, -128, 1125436544, 2)                                                                                                                 {'out': alloc_32}
call_function  alloc_33                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_9   cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_7, b__frozen_param9, b__frozen_param31, [1, 1], [1, 1], [1, 1], 1, -128, 0, 5.041901909988433e-05, 0.013887997716665268, 20, 1995834752, -8)      {'out': alloc_33}
call_function  alloc_34                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_6      cadence.dequantize_per_tensor.out             (cadence_quantized_relu_per_tensor_6, 0.012645375914871693, -128, -128, 127, torch.int8)                                                                                             {'out': alloc_34}
call_function  alloc_35                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_7      cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_9, 0.013887997716665268, 20, -128, 127, torch.int8)                                                                                        {'out': alloc_35}
call_function  alloc_36                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.float32),)                                                                                                                                                   {}
call_function  aten_add_tensor_3                            aten.add.out                                  (cadence_dequantize_per_tensor_default_7, cadence_dequantize_per_tensor_default_6)                                                                                                   {'alpha': 1, 'out': alloc_36}
call_function  alloc_37                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantize_per_tensor_default_5        cadence.quantize_per_tensor.out               (aten_add_tensor_3, 0.023267870768904686, -40, -128, 127, torch.int8)                                                                                                                {'out': alloc_37}
call_function  alloc_38                                     <function alloc at 0x7fbd8f8837e0>            (((1, 128, 8, 8), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_8          cadence.quantized_relu.per_tensor_out         (cadence_quantize_per_tensor_default_5, -40, -128, 1640967168, 1)                                                                                                                    {'out': alloc_38}
call_function  alloc_39                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_10  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_8, b__frozen_param12, b__frozen_param34, [2, 2], [0, 0], [1, 1], 1, -128, 0, 4.8739175489421506e-05, 0.005743513349443674, 55, 1166300288, -6)    {'out': alloc_39}
call_function  alloc_40                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_11  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_8, b__frozen_param10, b__frozen_param32, [2, 2], [1, 1], [1, 1], 1, -128, 0, 2.8147383362872114e-05, 0.010383650660514832, 17, 1490245376, -8)    {'out': alloc_40}
call_function  alloc_41                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_9          cadence.quantized_relu.per_tensor_out         (cadence_quantized_conv2d_nchw_per_tensor_11, 17, -128, 1249011328, 2)                                                                                                               {'out': alloc_41}
call_function  alloc_42                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_12  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_9, b__frozen_param11, b__frozen_param33, [1, 1], [1, 1], [1, 1], 1, -128, 0, 1.9740595123302063e-05, 0.01279838103801012, -12, 1695918720, -9)    {'out': alloc_42}
call_function  alloc_43                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_8      cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_10, 0.005743513349443674, 55, -128, 127, torch.int8)                                                                                       {'out': alloc_43}
call_function  alloc_44                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_9      cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_12, 0.01279838103801012, -12, -128, 127, torch.int8)                                                                                       {'out': alloc_44}
call_function  alloc_45                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.float32),)                                                                                                                                                   {}
call_function  aten_add_tensor_4                            aten.add.out                                  (cadence_dequantize_per_tensor_default_9, cadence_dequantize_per_tensor_default_8)                                                                                                   {'alpha': 1, 'out': alloc_45}
call_function  alloc_46                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantize_per_tensor_default_6        cadence.quantize_per_tensor.out               (aten_add_tensor_4, 0.014023727737367153, 10, -128, 127, torch.int8)                                                                                                                 {'out': alloc_46}
call_function  alloc_47                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_10         cadence.quantized_relu.per_tensor_out         (cadence_quantize_per_tensor_default_6, 10, -128, 1166551552, 2)                                                                                                                     {'out': alloc_47}
call_function  alloc_48                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_13  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_10, b__frozen_param13, b__frozen_param35, [1, 1], [1, 1], [1, 1], 1, -128, 0, 1.378793714260345e-05, 0.00814746506512165, 15, 1860701056, -9)     {'out': alloc_48}
call_function  alloc_49                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_11         cadence.quantized_relu.per_tensor_out         (cadence_quantized_conv2d_nchw_per_tensor_13, 15, -128, 1225150464, 2)                                                                                                               {'out': alloc_49}
call_function  alloc_50                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_14  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_11, b__frozen_param14, b__frozen_param36, [1, 1], [1, 1], [1, 1], 1, -128, 0, 2.7165558268670443e-05, 0.012041022069752216, 27, 1240295296, -8)   {'out': alloc_50}
call_function  alloc_51                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_10     cadence.dequantize_per_tensor.out             (cadence_quantized_relu_per_tensor_10, 0.006454006768763065, -128, -128, 127, torch.int8)                                                                                            {'out': alloc_51}
call_function  alloc_52                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_11     cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_14, 0.012041022069752216, 27, -128, 127, torch.int8)                                                                                       {'out': alloc_52}
call_function  alloc_53                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.float32),)                                                                                                                                                   {}
call_function  aten_add_tensor_5                            aten.add.out                                  (cadence_dequantize_per_tensor_default_11, cadence_dequantize_per_tensor_default_10)                                                                                                 {'alpha': 1, 'out': alloc_53}
call_function  alloc_54                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantize_per_tensor_default_7        cadence.quantize_per_tensor.out               (aten_add_tensor_5, 0.013384100049734116, 11, -128, 127, torch.int8)                                                                                                                 {'out': alloc_54}
call_function  alloc_55                                     <function alloc at 0x7fbd8f8837e0>            (((1, 256, 4, 4), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_12         cadence.quantized_relu.per_tensor_out         (cadence_quantize_per_tensor_default_7, 11, -128, 1181769856, 2)                                                                                                                     {'out': alloc_55}
call_function  alloc_56                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_15  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_12, b__frozen_param17, b__frozen_param39, [2, 2], [0, 0], [1, 1], 1, -128, 0, 4.760329874597003e-05, 0.009780516847968102, 19, 1337873536, -7)    {'out': alloc_56}
call_function  alloc_57                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_16  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_12, b__frozen_param15, b__frozen_param37, [2, 2], [1, 1], [1, 1], 1, -128, 0, 1.4384016300412507e-05, 0.006108534522354603, 5, 1294532480, -8)    {'out': alloc_57}
call_function  alloc_58                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_13         cadence.quantized_relu.per_tensor_out         (cadence_quantized_conv2d_nchw_per_tensor_16, 5, -128, 1124414720, 2)                                                                                                                {'out': alloc_58}
call_function  alloc_59                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_17  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_13, b__frozen_param16, b__frozen_param38, [1, 1], [1, 1], [1, 1], 1, -128, 0, 2.6164395009107416e-05, 0.013929501175880432, -26, 2065261056, -9)  {'out': alloc_59}
call_function  alloc_60                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_12     cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_15, 0.009780516847968102, 19, -128, 127, torch.int8)                                                                                       {'out': alloc_60}
call_function  alloc_61                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_13     cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_17, 0.013929501175880432, -26, -128, 127, torch.int8)                                                                                      {'out': alloc_61}
call_function  alloc_62                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.float32),)                                                                                                                                                   {}
call_function  aten_add_tensor_6                            aten.add.out                                  (cadence_dequantize_per_tensor_default_13, cadence_dequantize_per_tensor_default_12)                                                                                                 {'alpha': 1, 'out': alloc_62}
call_function  alloc_63                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantize_per_tensor_default_8        cadence.quantize_per_tensor.out               (aten_add_tensor_6, 0.017937982454895973, -8, -128, 127, torch.int8)                                                                                                                 {'out': alloc_63}
call_function  alloc_64                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_14         cadence.quantized_relu.per_tensor_out         (cadence_quantize_per_tensor_default_8, -8, -128, 2025429888, 1)                                                                                                                     {'out': alloc_64}
call_function  alloc_65                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_18  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_14, b__frozen_param18, b__frozen_param40, [1, 1], [1, 1], [1, 1], 1, -128, 0, 2.1872725555899687e-05, 0.007668694481253624, 28, 1568018944, -8)   {'out': alloc_65}
call_function  alloc_66                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_15         cadence.quantized_relu.per_tensor_out         (cadence_quantized_conv2d_nchw_per_tensor_18, 28, -128, 1381518336, 2)                                                                                                               {'out': alloc_66}
call_function  alloc_67                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_conv2d_nchw_per_tensor_19  cadence.quantized_conv2d_nchw.per_tensor_out  (cadence_quantized_relu_per_tensor_15, b__frozen_param19, b__frozen_param41, [1, 1], [1, 1], [1, 1], 1, -128, 0, 8.527387087900505e-05, 0.03805326297879219, -37, 1231952384, -8)    {'out': alloc_67}
call_function  alloc_68                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_14     cadence.dequantize_per_tensor.out             (cadence_quantized_relu_per_tensor_14, 0.009509468451142311, -128, -128, 127, torch.int8)                                                                                            {'out': alloc_68}
call_function  alloc_69                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_15     cadence.dequantize_per_tensor.out             (cadence_quantized_conv2d_nchw_per_tensor_19, 0.03805326297879219, -37, -128, 127, torch.int8)                                                                                       {'out': alloc_69}
call_function  alloc_70                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.float32),)                                                                                                                                                   {}
call_function  aten_add_tensor_7                            aten.add.out                                  (cadence_dequantize_per_tensor_default_15, cadence_dequantize_per_tensor_default_14)                                                                                                 {'alpha': 1, 'out': alloc_70}
call_function  alloc_71                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantize_per_tensor_default_9        cadence.quantize_per_tensor.out               (aten_add_tensor_7, 0.04343002662062645, -48, -128, 127, torch.int8)                                                                                                                 {'out': alloc_71}
call_function  alloc_72                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantized_relu_per_tensor_16         cadence.quantized_relu.per_tensor_out         (cadence_quantize_per_tensor_default_9, -48, -128, 1561470208, 1)                                                                                                                    {'out': alloc_72}
call_function  alloc_73                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 2, 2), torch.float32),)                                                                                                                                                   {}
call_function  cadence_dequantize_per_tensor_default_16     cadence.dequantize_per_tensor.out             (cadence_quantized_relu_per_tensor_16, 0.029864570125937462, -128, -128, 127, torch.int8)                                                                                            {'out': alloc_73}
call_function  alloc_74                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 1, 1), torch.float32),)                                                                                                                                                   {}
call_function  aten_mean_dim                                aten.mean.out                                 (cadence_dequantize_per_tensor_default_16, [-1, -2], True)                                                                                                                           {'dtype': None, 'out': alloc_74}
call_function  alloc_75                                     <function alloc at 0x7fbd8f8837e0>            (((1, 512, 1, 1), torch.int8),)                                                                                                                                                      {}
call_function  cadence_quantize_per_tensor_default_10       cadence.quantize_per_tensor.out               (aten_mean_dim, 0.028050869703292847, -128, -128, 127, torch.int8)                                                                                                                   {'out': alloc_75}
call_function  aten_view_copy_default                       <function view at 0x7fbd8f883920>             (cadence_quantize_per_tensor_default_10, [1, 512])                                                                                                                                   {}
call_function  alloc_76                                     <function alloc at 0x7fbd8f8837e0>            (((1, 1000), torch.int8),)                                                                                                                                                           {}
call_function  cadence_quantized_linear_per_tensor          cadence.quantized_linear.per_tensor_out       (aten_view_copy_default, b__frozen_param20, b__frozen_param21, -128, -57, 1541186688, -9, -33, None)                                                                                 {'out': alloc_76}
call_function  alloc_77                                     <function alloc at 0x7fbd8f8837e0>            (((1, 1000), torch.float32),)                                                                                                                                                        {}
call_function  cadence_dequantize_per_tensor_default_17     cadence.dequantize_per_tensor.out             (cadence_quantized_linear_per_tensor, 0.07793639600276947, -33, -128, 127, torch.int8)                                                                                               {'out': alloc_77}
output         output_1                                     output                                        ((cadence_dequantize_per_tensor_default_17,),)                                                                                                                                       {}[INFO 2025-12-22 07:39:20,906 utils.py:287] Saved exported program to /tmp/tmps8ia8j3x/CadenceDemoModel.pte
[INFO 2025-12-22 07:39:20,913 utils.py:304] Saved exported program to /tmp/tmps8ia8j3x/CadenceDemoModel.bpte
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/sraut/ext_main/cad_rlc/executorch/examples/cadence/models/resnet18.py", line 30, in <module>
    export_and_run_model(model, example_inputs)
  File "/home/sraut/ext_main/cad_rlc/executorch/.venv/lib64/python3.11/site-packages/executorch/backends/cadence/aot/export_example.py", line 110, in export_and_run_model
    runtime.run_and_compare(
  File "/home/sraut/ext_main/cad_rlc/executorch/.venv/lib64/python3.11/site-packages/executorch/backends/cadence/runtime/runtime.py", line 140, in run_and_compare
    outputs = run(executorch_prog, inputs, ref_outputs, working_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sraut/ext_main/cad_rlc/executorch/.venv/lib64/python3.11/site-packages/executorch/backends/cadence/runtime/runtime.py", line 61, in run
    program = executorch_prog.executorch_program
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'executorch_program'

