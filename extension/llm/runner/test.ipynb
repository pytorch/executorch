{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d6107d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.extension.llm.runner import MultimodalRunner, GenerationConfig, make_image_input, make_text_input\n",
    "from executorch.kernels import quantized\n",
    "from transformers import AutoProcessor\n",
    "from executorch.extension.llm.custom_ops import custom_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c3dc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 20815.40it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "image_url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n",
    "            }]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": image_url},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"What are the things I should be cautious about when I visit here?\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "inputs = processor.apply_chat_template(conversation, add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8997de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "What are the things I should be cautious about when I visit here? \n"
     ]
    }
   ],
   "source": [
    "print(processor.apply_chat_template(conversation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06c66c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Volumes/larryliu/work/models/llava/tokenizer_config.json',\n",
       " '/Volumes/larryliu/work/models/llava/special_tokens_map.json',\n",
       " '/Volumes/larryliu/work/models/llava/chat_template.jinja',\n",
       " '/Volumes/larryliu/work/models/llava/tokenizer.model',\n",
       " '/Volumes/larryliu/work/models/llava/added_tokens.json',\n",
       " '/Volumes/larryliu/work/models/llava/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(\"/Volumes/larryliu/work/models/llava/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E tokenizers:hf_tokenizer.cpp:60] Error parsing json file: [json.exception.parse_error.101] parse error at line 2, column 1: syntax error while parsing value - invalid literal; last read: '<U+000A><U+000E>'\n",
      "E tokenizers:tiktoken.cpp:59] invalid tiktoken line: \n",
      "[llm_runner_helper.cpp:77] Loaded Sentencepiece tokenizer\n",
      "[llm_runner_helper.cpp:270] Reading metadata from model\n",
      "[llm_runner_helper.cpp:133] Metadata: use_sdpa_with_kv_cache = 1\n",
      "[llm_runner_helper.cpp:133] Metadata: use_kv_cache = 1\n",
      "[llm_runner_helper.cpp:131] Method get_max_context_len not found, using the default value 128\n",
      "[llm_runner_helper.cpp:133] Metadata: get_max_context_len = 128\n",
      "[llm_runner_helper.cpp:133] Metadata: get_max_seq_len = 2048\n",
      "[llm_runner_helper.cpp:131] Method enable_dynamic_shape not found, using the default value 0\n",
      "[llm_runner_helper.cpp:133] Metadata: enable_dynamic_shape = 0\n",
      "[llm_runner_helper.cpp:144] Setting kMaxContextLen to kMaxSeqLen value: 2048\n",
      "[multimodal_runner.cpp:88] RSS after loading model: 0.000000 MiB (0 if unsupported)\n",
      "[multimodal_runner.cpp:114] Prefilling input 0/3, type: text\n",
      "[util.h:125] second_input_sizes[0] = 2047\n",
      "[multimodal_runner.cpp:114] Prefilling input 1/3, type: image\n",
      "[multimodal_prefiller.cpp:87] Image tensor dim: 4, dtype: Float\n",
      "[util.h:125] second_input_sizes[0] = 2047\n",
      "[multimodal_runner.cpp:114] Prefilling input 2/3, type: text\n",
      "[util.h:125] second_input_sizes[0] = 2047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the things I should be cautious about when I visit here? ASSISTANT: 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[multimodal_runner.cpp:132] RSS after multimodal input processing: 0.000000 MiB (0 if unsupported)\n",
      "[multimodal_runner.cpp:144] Max new tokens resolved: 100, pos_ 634, max_context_len 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Weather conditions: Since the image shows a dock surrounded by a large body of water, it is important to be aware of the weather conditions. Unfavorable weather, such as strong winds, heavy rain, or storms, can make the dock slippery and increase the risk of accidents.\n",
      "\n",
      "2. Tides and water levels: Be aware of the tides and water levels, as they can affect the dock's stability and safety. Tides can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stats.h:108] \tPrompt Tokens: 634    Generated Tokens: 99\n",
      "[stats.h:114] \tModel Load Time:\t\t8.312000 (seconds)\n",
      "[stats.h:124] \tTotal inference time:\t\t57.655000 (seconds)\t\t Rate: \t1.717110 (tokens/second)\n",
      "[stats.h:132] \t\tPrompt evaluation:\t30.963000 (seconds)\t\t Rate: \t20.476052 (tokens/second)\n",
      "[stats.h:143] \t\tGenerated 99 tokens:\t26.692000 (seconds)\t\t Rate: \t3.708976 (tokens/second)\n",
      "[stats.h:151] \tTime to first generated token:\t30.963000 (seconds)\n",
      "[stats.h:158] \tSampling time over 733 tokens:\t0.006000 (seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchObserver {\"prompt_tokens\":634,\"generated_tokens\":99,\"model_load_start_ms\":1758244266102,\"model_load_end_ms\":1758244274414,\"inference_start_ms\":1758244274414,\"inference_end_ms\":1758244332069,\"prompt_eval_end_ms\":1758244305377,\"first_token_ms\":1758244305377,\"aggregate_sampling_time_ms\":6,\"SCALING_FACTOR_UNITS_PER_SECOND\":1000}\n"
     ]
    }
   ],
   "source": [
    "inputs_combined = [\n",
    "    make_text_input(\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: \"), \n",
    "    make_image_input(inputs[\"pixel_values\"]), \n",
    "    make_text_input(\"\\nWhat are the things I should be cautious about when I visit here? \"),\n",
    "]\n",
    "runner = MultimodalRunner(\"/Volumes/larryliu/work/models/llava/model.pte\", \"/Volumes/larryliu/work/models/llava/tokenizer.model\", None)\n",
    "config = GenerationConfig()\n",
    "config.max_new_tokens = 100\n",
    "runner.generate(inputs_combined, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22b8dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.extension.pybindings.portable_lib import _load_for_executorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bae6e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = _load_for_executorch(\"/Volumes/larryliu/work/models/llava/model.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a48cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MethodMeta(name='text_decoder', num_inputs=2, input_tensor_meta=['TensorInfo(sizes=[1, 2047, 4096], dtype=Float, is_memory_planned=False, nbytes=33538048)', 'TensorInfo(sizes=[2047], dtype=Long, is_memory_planned=False, nbytes=16376)'], num_outputs=1, output_tensor_meta=['TensorInfo(sizes=[1, 2047, 32064], dtype=Float, is_memory_planned=True, nbytes=262540032)'])\n"
     ]
    }
   ],
   "source": [
    "print(module.method_meta(\"text_decoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86217dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = inputs[\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95ef85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = module.run_method(\"vision_encoder\", [image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cf24682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(res[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8460349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.46s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlavaForConditionalGeneration\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ee3cf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llava-hf/llava-1.5-7b-hf'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20562094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"llava\" in model.config.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d07ca7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class VisionExportableModule(torch.nn.Module):\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def prepare_export_inputs(self):\n",
    "        # 1. Get export inputs\n",
    "        model_id = self.model.config.name_or_path\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        sample_conversation_with_image = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        processed_inputs = processor.apply_chat_template(\n",
    "            sample_conversation_with_image,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        if \"pixel_values\" not in processed_inputs:\n",
    "            raise ValueError(\n",
    "                f\"Unable to obtain sample audio encoder inputs for export for {model_id} - the processor did not return formatted inputs with the 'pixel_values' key: {processed_inputs}\"\n",
    "            )\n",
    "        export_inputs = processed_inputs[\"pixel_values\"]\n",
    "\n",
    "        # 2. Get export dynamic shapes\n",
    "        dynamic_shapes = None  # No batching for now.\n",
    "\n",
    "        return export_inputs, dynamic_shapes\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_features: torch.FloatTensor,\n",
    "    ):\n",
    "        image_embeds = self.model.get_image_features(input_features)\n",
    "        # Hack, assuming the text decoder will take a 3D tensor (batch_size, seq_len, hidden_size)\n",
    "        if \"llava\" in self.model.config.name_or_path:\n",
    "            # Llava returns a list of 2D tensors (seq_len, hidden_size)\n",
    "            return image_embeds[0].unsqueeze(0)\n",
    "        else:\n",
    "            return image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53a7a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision = VisionExportableModule(model.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac39bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vision.forward(image.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0a365b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576, 4096])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed2577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
