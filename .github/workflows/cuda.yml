# Test ExecuTorch CUDA Build Compatibility
# This workflow tests whether ExecuTorch can be successfully built with CUDA support
# across different CUDA versions (12.6, 12.8, 12.9) using the command:
#   ./install_executorch.sh
#
# Note: ExecuTorch automatically detects the system CUDA version using nvcc and
# installs the appropriate PyTorch wheel. No manual CUDA/PyTorch installation needed.

name: Test CUDA Builds

on:
  pull_request:
  push:
    branches:
      - main
      - release/*

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}-${{ github.event_name == 'workflow_dispatch' }}-${{ github.event_name == 'schedule' }}
  cancel-in-progress: false

jobs:
  test-cuda-builds:
    strategy:
      fail-fast: false
      matrix:
        cuda-version: ["12.6", "12.8", "13.0"]

    name: test-executorch-cuda-build-${{ matrix.cuda-version }}
    uses: pytorch/test-infra/.github/workflows/linux_job_v2.yml@main
    permissions:
      id-token: write
      contents: read
    with:
      timeout: 90
      runner: linux.g5.4xlarge.nvidia.gpu
      gpu-arch-type: cuda
      gpu-arch-version: ${{ matrix.cuda-version }}
      use-custom-docker-registry: false
      submodules: recursive
      ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
      script: |
        set -eux

        # Test ExecuTorch CUDA build - ExecuTorch will automatically detect CUDA version
        # and install the appropriate PyTorch wheel
        source .ci/scripts/test-cuda-build.sh "${{ matrix.cuda-version }}"

  # This job will fail if any of the CUDA versions fail
  check-all-cuda-builds:
    needs: test-cuda-builds
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Check if all CUDA builds succeeded
        run: |
          if [[ "${{ needs.test-cuda-builds.result }}" != "success" ]]; then
            echo "ERROR: One or more ExecuTorch CUDA builds failed!"
            echo "CUDA build results: ${{ needs.test-cuda-builds.result }}"
            exit 1
          else
            echo "SUCCESS: All ExecuTorch CUDA builds (12.6, 12.8, 12.9) completed successfully!"
          fi

  test-models-cuda:
    name: test-models-cuda
    uses: pytorch/test-infra/.github/workflows/linux_job_v2.yml@main
    permissions:
      id-token: write
      contents: read
    strategy:
      fail-fast: false
      matrix:
        model: [linear, add, add_mul, resnet18, conv1d]
    with:
      timeout: 90
      runner: linux.g5.4xlarge.nvidia.gpu
      gpu-arch-type: cuda
      gpu-arch-version: 12.6
      use-custom-docker-registry: false
      submodules: recursive
      ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
      script: |
        set -eux

        PYTHON_EXECUTABLE=python ./install_executorch.sh
        export LD_LIBRARY_PATH=/opt/conda/lib:$LD_LIBRARY_PATH
        PYTHON_EXECUTABLE=python source .ci/scripts/test_model.sh "${{ matrix.model }}" cmake cuda

  export-model-cuda-artifact:
    name: export-${{ matrix.model.name }}-cuda-${{ matrix.quant.name }}
    # Skip this job if the pull request is from a fork (HuggingFace secrets are not available)
    if: github.event.pull_request.head.repo.full_name == github.repository || github.event_name != 'pull_request'
    uses: pytorch/test-infra/.github/workflows/linux_job_v2.yml@main
    permissions:
      id-token: write
      contents: read
    secrets: inherit
    strategy:
      fail-fast: false
      matrix:
        model:
          - name: "voxtral"
            hf_model: "mistralai/Voxtral-Mini-3B-2507"
            task: "multimodal-text-to-text"
            max_seq_len: 1024
            extra_pip: "mistral-common librosa"
            preprocessor_feature_size: 128
            preprocessor_output: "voxtral_preprocessor.pte"
          - name: "whisper"
            hf_model: "openai/whisper-small"
            task: "automatic-speech-recognition"
            max_seq_len: ""
            extra_pip: "librosa"
            preprocessor_feature_size: 80
            preprocessor_output: "whisper_preprocessor.pte"
          - name: "gemma3"
            hf_model: "google/gemma-3-4b-it"
            task: "multimodal-text-to-text"
            max_seq_len: 64
            extra_pip: ""
            preprocessor_feature_size: ""
            preprocessor_output: ""
        quant:
          - name: "non-quantized"
            extra_args: ""
          - name: "quantized-int4-tile-packed"
            extra_args: "--qlinear 4w --qlinear_encoder 4w --qlinear_packing_format tile_packed_to_4d --qlinear_encoder_packing_format tile_packed_to_4d"
          - name: "quantized-int4-weight-only"
            # TODO: adding "--qlinear 4w" produces invalid results. Need further investigation.
            extra_args: "--qlinear_encoder 4w"
        exclude:
          # TODO: enable int4-weight-only on gemma3.
          - model: { name: "gemma3" }
            quant: { name: "quantized-int4-weight-only" }
    with:
      timeout: 90
      secrets-env: EXECUTORCH_HF_TOKEN
      runner: linux.g5.4xlarge.nvidia.gpu
      gpu-arch-type: cuda
      gpu-arch-version: 12.6
      use-custom-docker-registry: false
      submodules: recursive
      upload-artifact: ${{ matrix.model.name }}-cuda-${{ matrix.quant.name == 'non-quantized' && 'export' || matrix.quant.name }}
      ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
      script: |
        set -eux

        echo "::group::Setup ExecuTorch"
        ./install_executorch.sh
        echo "::endgroup::"

        echo "::group::Setup Huggingface"
        pip install -U "huggingface_hub[cli]<1.0" accelerate
        huggingface-cli login --token $SECRET_EXECUTORCH_HF_TOKEN
        OPTIMUM_ET_VERSION=$(cat .ci/docker/ci_commit_pins/optimum-executorch.txt)
        pip install git+https://github.com/huggingface/optimum-executorch.git@${OPTIMUM_ET_VERSION}
        if [ -n "${{ matrix.model.extra_pip }}" ]; then
          pip install ${{ matrix.model.extra_pip }}
        fi
        pip list
        echo "::endgroup::"

        echo "::group::Export ${{ matrix.model.name }} (${{ matrix.quant.name }})"
        EXTRA_ARGS="${{ matrix.quant.extra_args }}"
        MAX_SEQ_LEN_ARG=""
        if [ -n "${{ matrix.model.max_seq_len }}" ]; then
          MAX_SEQ_LEN_ARG="--max_seq_len ${{ matrix.model.max_seq_len }}"
        fi
        optimum-cli export executorch \
            --model "${{ matrix.model.hf_model }}" \
            --task "${{ matrix.model.task }}" \
            --recipe "cuda" \
            --dtype bfloat16 \
            --device cuda \
            ${MAX_SEQ_LEN_ARG} \
            ${EXTRA_ARGS} \
            --output_dir ./

        if [ -n "${{ matrix.model.preprocessor_output }}" ]; then
          python -m executorch.extension.audio.mel_spectrogram \
              --feature_size ${{ matrix.model.preprocessor_feature_size }} \
              --stack_output \
              --max_audio_len 300 \
              --output_file ${{ matrix.model.preprocessor_output }}
        fi

        test -f model.pte
        test -f aoti_cuda_blob.ptd
        if [ -n "${{ matrix.model.preprocessor_output }}" ]; then
          test -f ${{ matrix.model.preprocessor_output }}
        fi
        echo "::endgroup::"

        echo "::group::Store ${{ matrix.model.name }} Artifacts (${{ matrix.quant.name }})"
        mkdir -p "${RUNNER_ARTIFACT_DIR}"
        cp model.pte "${RUNNER_ARTIFACT_DIR}/"
        cp aoti_cuda_blob.ptd "${RUNNER_ARTIFACT_DIR}/"
        if [ -n "${{ matrix.model.preprocessor_output }}" ]; then
          cp ${{ matrix.model.preprocessor_output }} "${RUNNER_ARTIFACT_DIR}/"
        fi
        ls -al "${RUNNER_ARTIFACT_DIR}"
        echo "::endgroup::"

  benchmark-model-cuda:
    name: benchmark-${{ matrix.model }}-cuda
    needs: export-model-cuda-artifact
    uses: pytorch/test-infra/.github/workflows/linux_job_v2.yml@main
    permissions:
      id-token: write
      contents: read
    strategy:
      fail-fast: false
      matrix:
        model: [voxtral, gemma3]
    with:
      timeout: 90
      runner: linux.g5.4xlarge.nvidia.gpu
      gpu-arch-type: cuda
      gpu-arch-version: 12.6
      use-custom-docker-registry: false
      submodules: recursive
      download-artifact: ${{ matrix.model }}-cuda-export
      ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
      script: |
        set -eux

        echo "::group::Setup ExecuTorch Requirements"
        ./install_requirements.sh
        pip list
        echo "::endgroup::"

        echo "::group::Prepare ${{ matrix.model }} Artifacts"
        cp "${RUNNER_ARTIFACT_DIR}/model.pte" .
        cp "${RUNNER_ARTIFACT_DIR}/aoti_cuda_blob.ptd" .
        ls -al model.pte aoti_cuda_blob.ptd
        echo "::endgroup::"

        echo "::group::Build ${{ matrix.model }} Benchmark"
        cmake -DCMAKE_BUILD_TYPE=Release \
              -DEXECUTORCH_BUILD_CUDA=ON \
              -DEXECUTORCH_BUILD_EXTENSION_TENSOR=ON \
              -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON \
              -DEXECUTORCH_BUILD_EXTENSION_NAMED_DATA_MAP=ON \
              -DEXECUTORCH_BUILD_TESTS=ON \
              -Bcmake-out .
        cmake --build cmake-out -j$(( $(nproc) - 1 )) --target multimodal_benchmark
        echo "::endgroup::"

        echo "::group::Run ${{ matrix.model }} Benchmark"

        export LD_LIBRARY_PATH=/opt/conda/lib:$LD_LIBRARY_PATH
        cmake-out/backends/cuda/multimodal_benchmark ${{ matrix.model }} model.pte aoti_cuda_blob.ptd

        echo "::endgroup::"

  test-model-cuda-e2e:
    name: test-${{ matrix.model.name }}-cuda-e2e-${{ matrix.format.name }}
    needs: export-model-cuda-artifact
    uses: pytorch/test-infra/.github/workflows/linux_job_v2.yml@main
    permissions:
      id-token: write
      contents: read
    strategy:
      fail-fast: false
      matrix:
        model:
          - name: "voxtral"
            runner_target: "voxtral_runner"
            runner_path: "voxtral"
            expected_output: "poem"
            preprocessor: "voxtral_preprocessor.pte"
            tokenizer_url: "https://huggingface.co/mistralai/Voxtral-Mini-3B-2507/resolve/main"
            tokenizer_file: "tekken.json"
            audio_url: "https://github.com/voxserv/audio_quality_testing_samples/raw/refs/heads/master/testaudio/16000/test01_20s.wav"
            audio_file: "poem.wav"
          - name: "whisper"
            runner_target: "whisper_runner"
            runner_path: "whisper"
            expected_output: "Mr. Quilter"
            preprocessor: "whisper_preprocessor.pte"
            tokenizer_url: "https://huggingface.co/openai/whisper-small/resolve/main"
            audio_file: "output.wav"
          - name: "gemma3"
            runner_target: "gemma3_e2e_runner"
            runner_path: "gemma3"
            expected_output: "chip"
            preprocessor: ""
            tokenizer_url: "https://huggingface.co/google/gemma-3-4b-it/resolve/main"
            image_path: "docs/source/_static/img/et-logo.png"
        format:
          - name: "non-quantized"
          - name: "quantized-int4-tile-packed"
          - name: "quantized-int4-weight-only"
        exclude:
          # TODO: enable int4-weight-only on gemma3.
          - model: { name: "gemma3" }
            format: { name: "quantized-int4-weight-only" }
    with:
      timeout: 90
      runner: linux.g5.4xlarge.nvidia.gpu
      gpu-arch-type: cuda
      gpu-arch-version: 12.6
      use-custom-docker-registry: false
      submodules: recursive
      download-artifact: ${{ matrix.model.name }}-cuda-${{ matrix.format.name == 'non-quantized' && 'export' || matrix.format.name }}
      ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
      script: |
        set -eux

        echo "::group::Setup ExecuTorch Requirements"
        ./install_requirements.sh
        pip list
        echo "::endgroup::"

        echo "::group::Prepare ${{ matrix.model.name }} Artifacts (${{ matrix.format.name }})"
        cp "${RUNNER_ARTIFACT_DIR}/model.pte" .
        cp "${RUNNER_ARTIFACT_DIR}/aoti_cuda_blob.ptd" .

        # Copy preprocessor if present
        if [ -n "${{ matrix.model.preprocessor }}" ]; then
          cp "${RUNNER_ARTIFACT_DIR}/${{ matrix.model.preprocessor }}" .
        fi

        # Download tokenizer files
        TOKENIZER_URL="${{ matrix.model.tokenizer_url }}"
        if [ "${{ matrix.model.tokenizer_file }}" != "" ]; then
          curl -L $TOKENIZER_URL/${{ matrix.model.tokenizer_file }} -o ${{ matrix.model.tokenizer_file }}
        else
          curl -L $TOKENIZER_URL/tokenizer.json -o tokenizer.json
          curl -L $TOKENIZER_URL/tokenizer_config.json -o tokenizer_config.json
          curl -L $TOKENIZER_URL/special_tokens_map.json -o special_tokens_map.json
        fi

        # Download test files
        if [ "${{ matrix.model.audio_url }}" != "" ]; then
          curl -L ${{ matrix.model.audio_url }} -o ${{ matrix.model.audio_file }}
        elif [ "${{ matrix.model.audio_file }}" = "output.wav" ]; then
          python <<-EOF
          from datasets import load_dataset
          import soundfile as sf
          sample = load_dataset("distil-whisper/librispeech_long", "clean", split="validation")[0]["audio"]
          sf.write('output.wav', sample["array"][:sample["sampling_rate"]*30], sample["sampling_rate"])
          EOF
        fi

        ls -al
        echo "::endgroup::"

        echo "::group::Build ${{ matrix.model.name }} Runner"
        cmake --preset llm \
              -DEXECUTORCH_BUILD_CUDA=ON \
              -DCMAKE_INSTALL_PREFIX=cmake-out \
              -DCMAKE_BUILD_TYPE=Release \
              -Bcmake-out -S.
        cmake --build cmake-out -j$(( $(nproc) - 1 )) --target install --config Release

        cmake -DEXECUTORCH_BUILD_CUDA=ON \
              -DCMAKE_BUILD_TYPE=Release \
              -Sexamples/models/${{ matrix.model.runner_path }} \
              -Bcmake-out/examples/models/${{ matrix.model.runner_path }}/
        cmake --build cmake-out/examples/models/${{ matrix.model.runner_path }} --target ${{ matrix.model.runner_target }} --config Release
        echo "::endgroup::"

        echo "::group::Run ${{ matrix.model.name }} Runner (${{ matrix.format.name }})"
        set +e
        export LD_LIBRARY_PATH=/opt/conda/lib:$LD_LIBRARY_PATH

        # Build runner command with common arguments
        RUNNER_BIN="cmake-out/examples/models/${{ matrix.model.runner_path }}/${{ matrix.model.runner_target }}"
        RUNNER_ARGS="--model_path model.pte --data_path aoti_cuda_blob.ptd --temperature 0"

        # Add model-specific arguments
        case "${{ matrix.model.name }}" in
          voxtral)
            RUNNER_ARGS="$RUNNER_ARGS --tokenizer_path ${{ matrix.model.tokenizer_file }} --audio_path ${{ matrix.model.audio_file }} --processor_path ${{ matrix.model.preprocessor }}"
            ;;
          whisper)
            RUNNER_ARGS="$RUNNER_ARGS --tokenizer_path ./ --audio_path ${{ matrix.model.audio_file }} --processor_path ${{ matrix.model.preprocessor }}"
            ;;
          gemma3)
            RUNNER_ARGS="$RUNNER_ARGS --tokenizer_path ./ --image_path ${{ matrix.model.image_path }}"
            ;;
        esac

        OUTPUT=$($RUNNER_BIN $RUNNER_ARGS 2>&1)
        EXIT_CODE=$?
        set -e

        echo "$OUTPUT"

        if ! echo "$OUTPUT" | grep -iq "${{ matrix.model.expected_output }}"; then
          echo "Expected output '${{ matrix.model.expected_output }}' not found in output"
          exit 1
        fi

        if [ $EXIT_CODE -ne 0 ]; then
          echo "Unexpected exit code: $EXIT_CODE"
          exit $EXIT_CODE
        fi
        echo "::endgroup::"
