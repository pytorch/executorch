# Test ExecuTorch CUDA Windows Cross-Compilation Export
# This workflow tests model export targeting CUDA Windows using optimum-executorch.
# It runs on a Linux machine with CUDA and installs mingw + Windows CUDA SDK at runtime
# for Windows cross-compilation.

name: Test CUDA Windows Export

on:
  pull_request:
  push:
    branches:
      - main
      - release/*

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}-${{ github.event_name == 'workflow_dispatch' }}-${{ github.event_name == 'schedule' }}
  cancel-in-progress: false

jobs:
  export-model-cuda-windows-artifact:
    name: export-model-cuda-windows-artifact
    # Skip this job if the pull request is from a fork (HuggingFace secrets are not available)
    if: github.event.pull_request.head.repo.full_name == github.repository || github.event_name != 'pull_request'
    uses: pytorch/test-infra/.github/workflows/linux_job_v2.yml@main
    permissions:
      id-token: write
      contents: read
    secrets: inherit
    strategy:
      fail-fast: false
      matrix:
        model:
          - repo: "mistralai"
            name: "Voxtral-Mini-3B-2507"
          - repo: "openai"
            name: "whisper-small"
          - repo: "openai"
            name: "whisper-large-v3-turbo"
          - repo: "google"
            name: "gemma-3-4b-it"
        quant:
          - "non-quantized"
          - "quantized-int4-tile-packed"
          - "quantized-int4-weight-only"
        exclude:
          # TODO: enable int4-weight-only on gemma3.
          - model:
              repo: "google"
              name: "gemma-3-4b-it"
            quant: "quantized-int4-weight-only"
    with:
      timeout: 90
      secrets-env: EXECUTORCH_HF_TOKEN
      runner: linux.g5.4xlarge.nvidia.gpu
      gpu-arch-type: cuda
      gpu-arch-version: 12.8
      use-custom-docker-registry: false
      submodules: recursive
      upload-artifact: ${{ matrix.model.repo }}-${{ matrix.model.name }}-cuda-windows-${{ matrix.quant }}
      ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
      script: |
        set -eux

        echo "::group::Install Windows cross-compilation dependencies"
        # Install mingw-w64 cross-compiler
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends g++-mingw-w64-x86-64 mingw-w64-tools p7zip-full
        x86_64-w64-mingw32-g++ --version

        # Download and extract Windows CUDA toolkit
        # We need this for cross-compiling CUDA code for Windows
        # Note: CUDA 12.8 installer is versioned as 12.8.1 with driver 572.61
        CUDA_INSTALLER_VERSION="12.8.1"
        CUDA_DRIVER_VERSION="572.61"
        WINDOWS_CUDA_INSTALL_DIR="/tmp/cuda-windows"
        mkdir -p "${WINDOWS_CUDA_INSTALL_DIR}"

        CUDA_INSTALLER="cuda_${CUDA_INSTALLER_VERSION}_${CUDA_DRIVER_VERSION}_windows.exe"
        CUDA_URL="https://developer.download.nvidia.com/compute/cuda/${CUDA_INSTALLER_VERSION}/local_installers/${CUDA_INSTALLER}"

        echo "Downloading Windows CUDA toolkit from ${CUDA_URL}..."
        wget -q "${CUDA_URL}" -O "${WINDOWS_CUDA_INSTALL_DIR}/${CUDA_INSTALLER}"

        echo "Extracting Windows CUDA toolkit..."
        7z x "${WINDOWS_CUDA_INSTALL_DIR}/${CUDA_INSTALLER}" -o"${WINDOWS_CUDA_INSTALL_DIR}/extracted" -y

        # Clean up installer
        rm -f "${WINDOWS_CUDA_INSTALL_DIR}/${CUDA_INSTALLER}"

        # Set environment variable for Windows CUDA
        export WINDOWS_CUDA_HOME="${WINDOWS_CUDA_INSTALL_DIR}/extracted/cuda_cudart/cudart"
        echo "WINDOWS_CUDA_HOME=${WINDOWS_CUDA_HOME}"
        ls -la "${WINDOWS_CUDA_HOME}"
        echo "::endgroup::"

        echo "::group::Setup ExecuTorch"
        ./install_executorch.sh
        echo "::endgroup::"

        echo "::group::Setup Huggingface"
        pip install -U "huggingface_hub[cli]<1.0" accelerate
        huggingface-cli login --token $SECRET_EXECUTORCH_HF_TOKEN
        OPTIMUM_ET_VERSION=$(cat .ci/docker/ci_commit_pins/optimum-executorch.txt)
        pip install git+https://github.com/huggingface/optimum-executorch.git@${OPTIMUM_ET_VERSION}
        echo "::endgroup::"

        source .ci/scripts/export_model_artifact.sh cuda-windows "${{ matrix.model.repo }}/${{ matrix.model.name }}" "${{ matrix.quant }}" "${RUNNER_ARTIFACT_DIR}"
