name: trunk

on:
  push:
    branches:
      - main
      - release/*
    tags:
      - ciflow/trunk/*
  pull_request:
    paths:
      - .ci/docker/ci_commit_pins/pytorch.txt
      - .ci/scripts/**
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}-${{ github.event_name == 'workflow_dispatch' }}-${{ github.event_name == 'schedule' }}
  cancel-in-progress: true

jobs:
  test-huggingface-transformers-xnnpack:
    if: ${{ !github.event.pull_request.head.repo.fork }}
    name: test-huggingface-transformers-xnnpack
    uses: pytorch/test-infra/.github/workflows/linux_job_v2.yml@main
    permissions:
      id-token: write
      contents: read
    secrets: inherit
    strategy:
      fail-fast: false
      matrix:
        config:
          - gemma3-1b|xnnpack|--quantize
          - llama3.2-1b|xnnpack|--quantize
          - qwen3-0.6b|xnnpack|--quantize
          - qwen3-1.7b|xnnpack|--quantize
          - gemma3-1b|xnnpack|--quantize
          - phi4-mini|xnnpack|--quantize
          - smollm2-135m|xnnpack|--quantize
          - smollm3-3b|xnnpack|--quantize
      secrets-env: EXECUTORCH_HF_TOKEN
      runner: linux.24xlarge
      docker-image: ci-image:executorch-ubuntu-22.04-clang12
      submodules: 'recursive'
      ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
      timeout: 90
      script: |
        IFS='|' read -r MODEL RECIPE QUANTIZE <<< "${{ matrix.config }}"
        echo "::group::Test MODEL: $MODEL RECIPE: $RECIPE QUANTIZE: $QUANTIZE"

        # The generic Linux job chooses to use base env, not the one setup by the image
        CONDA_ENV=$(conda env list --json | jq -r ".envs | .[-1]")
        conda activate "${CONDA_ENV}"

        echo "::group::Setup ExecuTorch"
        PYTHON_EXECUTABLE=python bash .ci/scripts/setup-linux.sh --build-tool "cmake"
        echo "::endgroup::"

        echo "::group::Setup Huggingface"
        pip install -U "huggingface_hub[cli]" accelerate
        huggingface-cli login --token $SECRET_EXECUTORCH_HF_TOKEN
        OPTIMUM_ET_VERSION=$(cat .ci/docker/ci_commit_pins/optimum-executorch.txt)
        pip install git+https://github.com/huggingface/optimum-executorch.git@${OPTIMUM_ET_VERSION}
        echo "::endgroup::"

        echo "::group::Test MODEL: $MODEL RECIPE: $RECIPE QUANTIZE: $QUANTIZE"
        export OUTPUT_DIR="$(pwd)/${MODEL}_${RECIPE}_${QUANTIZE}"
        python .ci/scripts/test_huggingface_optimum_model.py --model "$MODEL" --recipe "$RECIPE" $QUANTIZE --model_dir "$OUTPUT_DIR"
        echo "::endgroup::"
